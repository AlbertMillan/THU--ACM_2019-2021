{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework-2: MLP for MNIST Classification\n",
    "\n",
    "### In this homework, you need to\n",
    "- #### implement SGD optimizer (`./optimizer.py`)\n",
    "- #### implement forward and backward for FCLayer (`layers/fc_layer.py`)\n",
    "- #### implement forward and backward for SigmoidLayer (`layers/sigmoid_layer.py`)\n",
    "- #### implement forward and backward for ReLULayer (`layers/relu_layer.py`)\n",
    "- #### implement EuclideanLossLayer (`criterion/euclidean_loss.py`)\n",
    "- #### implement SoftmaxCrossEntropyLossLayer (`criterion/softmax_cross_entropy.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from network import Network\n",
    "from solver import train, test\n",
    "from plot import plot_loss_and_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST Dataset\n",
    "We use tensorflow tools to load dataset for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_image(image):\n",
    "    # Normalize from [0, 255.] to [0., 1.0], and then subtract by the mean value\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.reshape(image, [784])\n",
    "    image = image / 255.0\n",
    "    image = image - tf.reduce_mean(image)\n",
    "    return image\n",
    "\n",
    "def decode_label(label):\n",
    "    # Encode label with one-hot encoding\n",
    "    return tf.one_hot(label, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train).map(decode_image)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train).map(decode_label)\n",
    "data_train = tf.data.Dataset.zip((x_train, y_train))\n",
    "\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test).map(decode_image)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test).map(decode_label)\n",
    "data_test = tf.data.Dataset.zip((x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyerparameters\n",
    "You can modify hyerparameters by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 20\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.001\n",
    "weight_decay = 0.1\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 40\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 0.003\n",
    "\n",
    "# (0.1 : 0.84) (0.001 : 0.89) (0.00001 : 0.88)\n",
    "weight_decay = 0.001\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MLP with Euclidean Loss\n",
    "In part-1, you need to train a MLP with **Euclidean Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **./optimizer.py** and **criterion/euclidean_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import EuclideanLossLayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = EuclideanLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 MLP with Euclidean Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/fc_layer.py** and **layers/sigmoid_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, SigmoidLayer\n",
    "\n",
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/millana/Desktop/Albert/COURSES/DL/homework-2/homework-2/homework2-mlp/solver.py:15: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "Epoch [0][40]\t Batch [0][550]\t Training Loss 0.2940\t Accuracy 0.0800\n",
      "Epoch [0][40]\t Batch [50][550]\t Training Loss 0.1068\t Accuracy 0.2961\n",
      "Epoch [0][40]\t Batch [100][550]\t Training Loss 0.0960\t Accuracy 0.4492\n",
      "Epoch [0][40]\t Batch [150][550]\t Training Loss 0.0908\t Accuracy 0.5251\n",
      "Epoch [0][40]\t Batch [200][550]\t Training Loss 0.0873\t Accuracy 0.5781\n",
      "Epoch [0][40]\t Batch [250][550]\t Training Loss 0.0847\t Accuracy 0.6146\n",
      "Epoch [0][40]\t Batch [300][550]\t Training Loss 0.0828\t Accuracy 0.6410\n",
      "Epoch [0][40]\t Batch [350][550]\t Training Loss 0.0814\t Accuracy 0.6607\n",
      "Epoch [0][40]\t Batch [400][550]\t Training Loss 0.0801\t Accuracy 0.6777\n",
      "Epoch [0][40]\t Batch [450][550]\t Training Loss 0.0791\t Accuracy 0.6908\n",
      "Epoch [0][40]\t Batch [500][550]\t Training Loss 0.0783\t Accuracy 0.7017\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0775\t Average training accuracy 0.7119\n",
      "Epoch [0]\t Average validation loss 0.0667\t Average validation accuracy 0.8638\n",
      "\n",
      "Epoch [1][40]\t Batch [0][550]\t Training Loss 0.0667\t Accuracy 0.8900\n",
      "Epoch [1][40]\t Batch [50][550]\t Training Loss 0.0685\t Accuracy 0.8251\n",
      "Epoch [1][40]\t Batch [100][550]\t Training Loss 0.0683\t Accuracy 0.8251\n",
      "Epoch [1][40]\t Batch [150][550]\t Training Loss 0.0688\t Accuracy 0.8198\n",
      "Epoch [1][40]\t Batch [200][550]\t Training Loss 0.0687\t Accuracy 0.8226\n",
      "Epoch [1][40]\t Batch [250][550]\t Training Loss 0.0685\t Accuracy 0.8222\n",
      "Epoch [1][40]\t Batch [300][550]\t Training Loss 0.0684\t Accuracy 0.8229\n",
      "Epoch [1][40]\t Batch [350][550]\t Training Loss 0.0684\t Accuracy 0.8224\n",
      "Epoch [1][40]\t Batch [400][550]\t Training Loss 0.0683\t Accuracy 0.8237\n",
      "Epoch [1][40]\t Batch [450][550]\t Training Loss 0.0682\t Accuracy 0.8238\n",
      "Epoch [1][40]\t Batch [500][550]\t Training Loss 0.0681\t Accuracy 0.8240\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0680\t Average training accuracy 0.8251\n",
      "Epoch [1]\t Average validation loss 0.0640\t Average validation accuracy 0.8822\n",
      "\n",
      "Epoch [2][40]\t Batch [0][550]\t Training Loss 0.0644\t Accuracy 0.8900\n",
      "Epoch [2][40]\t Batch [50][550]\t Training Loss 0.0659\t Accuracy 0.8449\n",
      "Epoch [2][40]\t Batch [100][550]\t Training Loss 0.0658\t Accuracy 0.8444\n",
      "Epoch [2][40]\t Batch [150][550]\t Training Loss 0.0664\t Accuracy 0.8369\n",
      "Epoch [2][40]\t Batch [200][550]\t Training Loss 0.0663\t Accuracy 0.8382\n",
      "Epoch [2][40]\t Batch [250][550]\t Training Loss 0.0663\t Accuracy 0.8381\n",
      "Epoch [2][40]\t Batch [300][550]\t Training Loss 0.0662\t Accuracy 0.8386\n",
      "Epoch [2][40]\t Batch [350][550]\t Training Loss 0.0662\t Accuracy 0.8374\n",
      "Epoch [2][40]\t Batch [400][550]\t Training Loss 0.0662\t Accuracy 0.8383\n",
      "Epoch [2][40]\t Batch [450][550]\t Training Loss 0.0661\t Accuracy 0.8379\n",
      "Epoch [2][40]\t Batch [500][550]\t Training Loss 0.0661\t Accuracy 0.8378\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0660\t Average training accuracy 0.8385\n",
      "Epoch [2]\t Average validation loss 0.0626\t Average validation accuracy 0.8866\n",
      "\n",
      "Epoch [3][40]\t Batch [0][550]\t Training Loss 0.0631\t Accuracy 0.9200\n",
      "Epoch [3][40]\t Batch [50][550]\t Training Loss 0.0644\t Accuracy 0.8533\n",
      "Epoch [3][40]\t Batch [100][550]\t Training Loss 0.0644\t Accuracy 0.8522\n",
      "Epoch [3][40]\t Batch [150][550]\t Training Loss 0.0649\t Accuracy 0.8441\n",
      "Epoch [3][40]\t Batch [200][550]\t Training Loss 0.0649\t Accuracy 0.8460\n",
      "Epoch [3][40]\t Batch [250][550]\t Training Loss 0.0648\t Accuracy 0.8460\n",
      "Epoch [3][40]\t Batch [300][550]\t Training Loss 0.0648\t Accuracy 0.8462\n",
      "Epoch [3][40]\t Batch [350][550]\t Training Loss 0.0648\t Accuracy 0.8450\n",
      "Epoch [3][40]\t Batch [400][550]\t Training Loss 0.0648\t Accuracy 0.8460\n",
      "Epoch [3][40]\t Batch [450][550]\t Training Loss 0.0648\t Accuracy 0.8458\n",
      "Epoch [3][40]\t Batch [500][550]\t Training Loss 0.0648\t Accuracy 0.8457\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0647\t Average training accuracy 0.8460\n",
      "Epoch [3]\t Average validation loss 0.0614\t Average validation accuracy 0.8908\n",
      "\n",
      "Epoch [4][40]\t Batch [0][550]\t Training Loss 0.0619\t Accuracy 0.9300\n",
      "Epoch [4][40]\t Batch [50][550]\t Training Loss 0.0632\t Accuracy 0.8614\n",
      "Epoch [4][40]\t Batch [100][550]\t Training Loss 0.0632\t Accuracy 0.8596\n",
      "Epoch [4][40]\t Batch [150][550]\t Training Loss 0.0637\t Accuracy 0.8511\n",
      "Epoch [4][40]\t Batch [200][550]\t Training Loss 0.0637\t Accuracy 0.8533\n",
      "Epoch [4][40]\t Batch [250][550]\t Training Loss 0.0636\t Accuracy 0.8533\n",
      "Epoch [4][40]\t Batch [300][550]\t Training Loss 0.0636\t Accuracy 0.8535\n",
      "Epoch [4][40]\t Batch [350][550]\t Training Loss 0.0636\t Accuracy 0.8524\n",
      "Epoch [4][40]\t Batch [400][550]\t Training Loss 0.0636\t Accuracy 0.8534\n",
      "Epoch [4][40]\t Batch [450][550]\t Training Loss 0.0636\t Accuracy 0.8530\n",
      "Epoch [4][40]\t Batch [500][550]\t Training Loss 0.0636\t Accuracy 0.8527\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0635\t Average training accuracy 0.8530\n",
      "Epoch [4]\t Average validation loss 0.0602\t Average validation accuracy 0.8942\n",
      "\n",
      "Epoch [5][40]\t Batch [0][550]\t Training Loss 0.0607\t Accuracy 0.9200\n",
      "Epoch [5][40]\t Batch [50][550]\t Training Loss 0.0620\t Accuracy 0.8673\n",
      "Epoch [5][40]\t Batch [100][550]\t Training Loss 0.0620\t Accuracy 0.8650\n",
      "Epoch [5][40]\t Batch [150][550]\t Training Loss 0.0626\t Accuracy 0.8570\n",
      "Epoch [5][40]\t Batch [200][550]\t Training Loss 0.0625\t Accuracy 0.8588\n",
      "Epoch [5][40]\t Batch [250][550]\t Training Loss 0.0625\t Accuracy 0.8592\n",
      "Epoch [5][40]\t Batch [300][550]\t Training Loss 0.0624\t Accuracy 0.8594\n",
      "Epoch [5][40]\t Batch [350][550]\t Training Loss 0.0625\t Accuracy 0.8583\n",
      "Epoch [5][40]\t Batch [400][550]\t Training Loss 0.0624\t Accuracy 0.8593\n",
      "Epoch [5][40]\t Batch [450][550]\t Training Loss 0.0624\t Accuracy 0.8591\n",
      "Epoch [5][40]\t Batch [500][550]\t Training Loss 0.0624\t Accuracy 0.8588\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0623\t Average training accuracy 0.8593\n",
      "Epoch [5]\t Average validation loss 0.0589\t Average validation accuracy 0.8990\n",
      "\n",
      "Epoch [6][40]\t Batch [0][550]\t Training Loss 0.0594\t Accuracy 0.9200\n",
      "Epoch [6][40]\t Batch [50][550]\t Training Loss 0.0608\t Accuracy 0.8716\n",
      "Epoch [6][40]\t Batch [100][550]\t Training Loss 0.0608\t Accuracy 0.8687\n",
      "Epoch [6][40]\t Batch [150][550]\t Training Loss 0.0614\t Accuracy 0.8617\n",
      "Epoch [6][40]\t Batch [200][550]\t Training Loss 0.0613\t Accuracy 0.8642\n",
      "Epoch [6][40]\t Batch [250][550]\t Training Loss 0.0612\t Accuracy 0.8645\n",
      "Epoch [6][40]\t Batch [300][550]\t Training Loss 0.0612\t Accuracy 0.8651\n",
      "Epoch [6][40]\t Batch [350][550]\t Training Loss 0.0612\t Accuracy 0.8644\n",
      "Epoch [6][40]\t Batch [400][550]\t Training Loss 0.0612\t Accuracy 0.8654\n",
      "Epoch [6][40]\t Batch [450][550]\t Training Loss 0.0612\t Accuracy 0.8653\n",
      "Epoch [6][40]\t Batch [500][550]\t Training Loss 0.0612\t Accuracy 0.8649\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0611\t Average training accuracy 0.8655\n",
      "Epoch [6]\t Average validation loss 0.0575\t Average validation accuracy 0.9046\n",
      "\n",
      "Epoch [7][40]\t Batch [0][550]\t Training Loss 0.0579\t Accuracy 0.9100\n",
      "Epoch [7][40]\t Batch [50][550]\t Training Loss 0.0595\t Accuracy 0.8769\n",
      "Epoch [7][40]\t Batch [100][550]\t Training Loss 0.0595\t Accuracy 0.8739\n",
      "Epoch [7][40]\t Batch [150][550]\t Training Loss 0.0601\t Accuracy 0.8679\n",
      "Epoch [7][40]\t Batch [200][550]\t Training Loss 0.0600\t Accuracy 0.8702\n",
      "Epoch [7][40]\t Batch [250][550]\t Training Loss 0.0599\t Accuracy 0.8708\n",
      "Epoch [7][40]\t Batch [300][550]\t Training Loss 0.0598\t Accuracy 0.8712\n",
      "Epoch [7][40]\t Batch [350][550]\t Training Loss 0.0599\t Accuracy 0.8704\n",
      "Epoch [7][40]\t Batch [400][550]\t Training Loss 0.0598\t Accuracy 0.8712\n",
      "Epoch [7][40]\t Batch [450][550]\t Training Loss 0.0598\t Accuracy 0.8713\n",
      "Epoch [7][40]\t Batch [500][550]\t Training Loss 0.0599\t Accuracy 0.8709\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0597\t Average training accuracy 0.8715\n",
      "Epoch [7]\t Average validation loss 0.0560\t Average validation accuracy 0.9080\n",
      "\n",
      "Epoch [8][40]\t Batch [0][550]\t Training Loss 0.0564\t Accuracy 0.9100\n",
      "Epoch [8][40]\t Batch [50][550]\t Training Loss 0.0581\t Accuracy 0.8818\n",
      "Epoch [8][40]\t Batch [100][550]\t Training Loss 0.0581\t Accuracy 0.8796\n",
      "Epoch [8][40]\t Batch [150][550]\t Training Loss 0.0587\t Accuracy 0.8739\n",
      "Epoch [8][40]\t Batch [200][550]\t Training Loss 0.0586\t Accuracy 0.8762\n",
      "Epoch [8][40]\t Batch [250][550]\t Training Loss 0.0586\t Accuracy 0.8766\n",
      "Epoch [8][40]\t Batch [300][550]\t Training Loss 0.0585\t Accuracy 0.8772\n",
      "Epoch [8][40]\t Batch [350][550]\t Training Loss 0.0585\t Accuracy 0.8766\n",
      "Epoch [8][40]\t Batch [400][550]\t Training Loss 0.0585\t Accuracy 0.8772\n",
      "Epoch [8][40]\t Batch [450][550]\t Training Loss 0.0585\t Accuracy 0.8773\n",
      "Epoch [8][40]\t Batch [500][550]\t Training Loss 0.0585\t Accuracy 0.8769\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0584\t Average training accuracy 0.8775\n",
      "Epoch [8]\t Average validation loss 0.0545\t Average validation accuracy 0.9122\n",
      "\n",
      "Epoch [9][40]\t Batch [0][550]\t Training Loss 0.0548\t Accuracy 0.9200\n",
      "Epoch [9][40]\t Batch [50][550]\t Training Loss 0.0568\t Accuracy 0.8892\n",
      "Epoch [9][40]\t Batch [100][550]\t Training Loss 0.0568\t Accuracy 0.8859\n",
      "Epoch [9][40]\t Batch [150][550]\t Training Loss 0.0573\t Accuracy 0.8807\n",
      "Epoch [9][40]\t Batch [200][550]\t Training Loss 0.0572\t Accuracy 0.8830\n",
      "Epoch [9][40]\t Batch [250][550]\t Training Loss 0.0572\t Accuracy 0.8833\n",
      "Epoch [9][40]\t Batch [300][550]\t Training Loss 0.0571\t Accuracy 0.8836\n",
      "Epoch [9][40]\t Batch [350][550]\t Training Loss 0.0571\t Accuracy 0.8826\n",
      "Epoch [9][40]\t Batch [400][550]\t Training Loss 0.0571\t Accuracy 0.8830\n",
      "Epoch [9][40]\t Batch [450][550]\t Training Loss 0.0571\t Accuracy 0.8834\n",
      "Epoch [9][40]\t Batch [500][550]\t Training Loss 0.0571\t Accuracy 0.8830\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0570\t Average training accuracy 0.8835\n",
      "Epoch [9]\t Average validation loss 0.0529\t Average validation accuracy 0.9172\n",
      "\n",
      "Epoch [10][40]\t Batch [0][550]\t Training Loss 0.0531\t Accuracy 0.9200\n",
      "Epoch [10][40]\t Batch [50][550]\t Training Loss 0.0554\t Accuracy 0.8971\n",
      "Epoch [10][40]\t Batch [100][550]\t Training Loss 0.0554\t Accuracy 0.8926\n",
      "Epoch [10][40]\t Batch [150][550]\t Training Loss 0.0560\t Accuracy 0.8880\n",
      "Epoch [10][40]\t Batch [200][550]\t Training Loss 0.0559\t Accuracy 0.8896\n",
      "Epoch [10][40]\t Batch [250][550]\t Training Loss 0.0558\t Accuracy 0.8898\n",
      "Epoch [10][40]\t Batch [300][550]\t Training Loss 0.0557\t Accuracy 0.8900\n",
      "Epoch [10][40]\t Batch [350][550]\t Training Loss 0.0558\t Accuracy 0.8889\n",
      "Epoch [10][40]\t Batch [400][550]\t Training Loss 0.0557\t Accuracy 0.8893\n",
      "Epoch [10][40]\t Batch [450][550]\t Training Loss 0.0557\t Accuracy 0.8896\n",
      "Epoch [10][40]\t Batch [500][550]\t Training Loss 0.0557\t Accuracy 0.8890\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0556\t Average training accuracy 0.8895\n",
      "Epoch [10]\t Average validation loss 0.0514\t Average validation accuracy 0.9214\n",
      "\n",
      "Epoch [11][40]\t Batch [0][550]\t Training Loss 0.0516\t Accuracy 0.9200\n",
      "Epoch [11][40]\t Batch [50][550]\t Training Loss 0.0541\t Accuracy 0.9014\n",
      "Epoch [11][40]\t Batch [100][550]\t Training Loss 0.0542\t Accuracy 0.8976\n",
      "Epoch [11][40]\t Batch [150][550]\t Training Loss 0.0547\t Accuracy 0.8934\n",
      "Epoch [11][40]\t Batch [200][550]\t Training Loss 0.0546\t Accuracy 0.8951\n",
      "Epoch [11][40]\t Batch [250][550]\t Training Loss 0.0545\t Accuracy 0.8953\n",
      "Epoch [11][40]\t Batch [300][550]\t Training Loss 0.0544\t Accuracy 0.8953\n",
      "Epoch [11][40]\t Batch [350][550]\t Training Loss 0.0545\t Accuracy 0.8943\n",
      "Epoch [11][40]\t Batch [400][550]\t Training Loss 0.0544\t Accuracy 0.8945\n",
      "Epoch [11][40]\t Batch [450][550]\t Training Loss 0.0544\t Accuracy 0.8947\n",
      "Epoch [11][40]\t Batch [500][550]\t Training Loss 0.0544\t Accuracy 0.8942\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0544\t Average training accuracy 0.8945\n",
      "Epoch [11]\t Average validation loss 0.0500\t Average validation accuracy 0.9238\n",
      "\n",
      "Epoch [12][40]\t Batch [0][550]\t Training Loss 0.0501\t Accuracy 0.9200\n",
      "Epoch [12][40]\t Batch [50][550]\t Training Loss 0.0528\t Accuracy 0.9047\n",
      "Epoch [12][40]\t Batch [100][550]\t Training Loss 0.0529\t Accuracy 0.9016\n",
      "Epoch [12][40]\t Batch [150][550]\t Training Loss 0.0535\t Accuracy 0.8977\n",
      "Epoch [12][40]\t Batch [200][550]\t Training Loss 0.0533\t Accuracy 0.8994\n",
      "Epoch [12][40]\t Batch [250][550]\t Training Loss 0.0533\t Accuracy 0.9001\n",
      "Epoch [12][40]\t Batch [300][550]\t Training Loss 0.0532\t Accuracy 0.9002\n",
      "Epoch [12][40]\t Batch [350][550]\t Training Loss 0.0533\t Accuracy 0.8993\n",
      "Epoch [12][40]\t Batch [400][550]\t Training Loss 0.0532\t Accuracy 0.8993\n",
      "Epoch [12][40]\t Batch [450][550]\t Training Loss 0.0532\t Accuracy 0.8997\n",
      "Epoch [12][40]\t Batch [500][550]\t Training Loss 0.0532\t Accuracy 0.8992\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0532\t Average training accuracy 0.8995\n",
      "Epoch [12]\t Average validation loss 0.0488\t Average validation accuracy 0.9272\n",
      "\n",
      "Epoch [13][40]\t Batch [0][550]\t Training Loss 0.0487\t Accuracy 0.9300\n",
      "Epoch [13][40]\t Batch [50][550]\t Training Loss 0.0516\t Accuracy 0.9090\n",
      "Epoch [13][40]\t Batch [100][550]\t Training Loss 0.0518\t Accuracy 0.9059\n",
      "Epoch [13][40]\t Batch [150][550]\t Training Loss 0.0523\t Accuracy 0.9023\n",
      "Epoch [13][40]\t Batch [200][550]\t Training Loss 0.0522\t Accuracy 0.9038\n",
      "Epoch [13][40]\t Batch [250][550]\t Training Loss 0.0522\t Accuracy 0.9041\n",
      "Epoch [13][40]\t Batch [300][550]\t Training Loss 0.0521\t Accuracy 0.9044\n",
      "Epoch [13][40]\t Batch [350][550]\t Training Loss 0.0522\t Accuracy 0.9034\n",
      "Epoch [13][40]\t Batch [400][550]\t Training Loss 0.0521\t Accuracy 0.9035\n",
      "Epoch [13][40]\t Batch [450][550]\t Training Loss 0.0521\t Accuracy 0.9038\n",
      "Epoch [13][40]\t Batch [500][550]\t Training Loss 0.0521\t Accuracy 0.9034\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0521\t Average training accuracy 0.9036\n",
      "Epoch [13]\t Average validation loss 0.0476\t Average validation accuracy 0.9308\n",
      "\n",
      "Epoch [14][40]\t Batch [0][550]\t Training Loss 0.0475\t Accuracy 0.9400\n",
      "Epoch [14][40]\t Batch [50][550]\t Training Loss 0.0506\t Accuracy 0.9145\n",
      "Epoch [14][40]\t Batch [100][550]\t Training Loss 0.0508\t Accuracy 0.9112\n",
      "Epoch [14][40]\t Batch [150][550]\t Training Loss 0.0513\t Accuracy 0.9068\n",
      "Epoch [14][40]\t Batch [200][550]\t Training Loss 0.0512\t Accuracy 0.9077\n",
      "Epoch [14][40]\t Batch [250][550]\t Training Loss 0.0512\t Accuracy 0.9080\n",
      "Epoch [14][40]\t Batch [300][550]\t Training Loss 0.0511\t Accuracy 0.9085\n",
      "Epoch [14][40]\t Batch [350][550]\t Training Loss 0.0511\t Accuracy 0.9077\n",
      "Epoch [14][40]\t Batch [400][550]\t Training Loss 0.0511\t Accuracy 0.9078\n",
      "Epoch [14][40]\t Batch [450][550]\t Training Loss 0.0511\t Accuracy 0.9080\n",
      "Epoch [14][40]\t Batch [500][550]\t Training Loss 0.0511\t Accuracy 0.9074\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0511\t Average training accuracy 0.9076\n",
      "Epoch [14]\t Average validation loss 0.0466\t Average validation accuracy 0.9332\n",
      "\n",
      "Epoch [15][40]\t Batch [0][550]\t Training Loss 0.0464\t Accuracy 0.9500\n",
      "Epoch [15][40]\t Batch [50][550]\t Training Loss 0.0496\t Accuracy 0.9202\n",
      "Epoch [15][40]\t Batch [100][550]\t Training Loss 0.0498\t Accuracy 0.9150\n",
      "Epoch [15][40]\t Batch [150][550]\t Training Loss 0.0503\t Accuracy 0.9102\n",
      "Epoch [15][40]\t Batch [200][550]\t Training Loss 0.0502\t Accuracy 0.9107\n",
      "Epoch [15][40]\t Batch [250][550]\t Training Loss 0.0502\t Accuracy 0.9113\n",
      "Epoch [15][40]\t Batch [300][550]\t Training Loss 0.0501\t Accuracy 0.9118\n",
      "Epoch [15][40]\t Batch [350][550]\t Training Loss 0.0502\t Accuracy 0.9111\n",
      "Epoch [15][40]\t Batch [400][550]\t Training Loss 0.0501\t Accuracy 0.9110\n",
      "Epoch [15][40]\t Batch [450][550]\t Training Loss 0.0501\t Accuracy 0.9113\n",
      "Epoch [15][40]\t Batch [500][550]\t Training Loss 0.0502\t Accuracy 0.9106\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0501\t Average training accuracy 0.9107\n",
      "Epoch [15]\t Average validation loss 0.0457\t Average validation accuracy 0.9358\n",
      "\n",
      "Epoch [16][40]\t Batch [0][550]\t Training Loss 0.0454\t Accuracy 0.9400\n",
      "Epoch [16][40]\t Batch [50][550]\t Training Loss 0.0487\t Accuracy 0.9239\n",
      "Epoch [16][40]\t Batch [100][550]\t Training Loss 0.0490\t Accuracy 0.9182\n",
      "Epoch [16][40]\t Batch [150][550]\t Training Loss 0.0495\t Accuracy 0.9136\n",
      "Epoch [16][40]\t Batch [200][550]\t Training Loss 0.0493\t Accuracy 0.9137\n",
      "Epoch [16][40]\t Batch [250][550]\t Training Loss 0.0494\t Accuracy 0.9144\n",
      "Epoch [16][40]\t Batch [300][550]\t Training Loss 0.0493\t Accuracy 0.9148\n",
      "Epoch [16][40]\t Batch [350][550]\t Training Loss 0.0493\t Accuracy 0.9141\n",
      "Epoch [16][40]\t Batch [400][550]\t Training Loss 0.0493\t Accuracy 0.9139\n",
      "Epoch [16][40]\t Batch [450][550]\t Training Loss 0.0493\t Accuracy 0.9142\n",
      "Epoch [16][40]\t Batch [500][550]\t Training Loss 0.0494\t Accuracy 0.9135\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0493\t Average training accuracy 0.9136\n",
      "Epoch [16]\t Average validation loss 0.0449\t Average validation accuracy 0.9384\n",
      "\n",
      "Epoch [17][40]\t Batch [0][550]\t Training Loss 0.0446\t Accuracy 0.9400\n",
      "Epoch [17][40]\t Batch [50][550]\t Training Loss 0.0478\t Accuracy 0.9267\n",
      "Epoch [17][40]\t Batch [100][550]\t Training Loss 0.0482\t Accuracy 0.9216\n",
      "Epoch [17][40]\t Batch [150][550]\t Training Loss 0.0487\t Accuracy 0.9172\n",
      "Epoch [17][40]\t Batch [200][550]\t Training Loss 0.0486\t Accuracy 0.9174\n",
      "Epoch [17][40]\t Batch [250][550]\t Training Loss 0.0486\t Accuracy 0.9179\n",
      "Epoch [17][40]\t Batch [300][550]\t Training Loss 0.0485\t Accuracy 0.9181\n",
      "Epoch [17][40]\t Batch [350][550]\t Training Loss 0.0486\t Accuracy 0.9172\n",
      "Epoch [17][40]\t Batch [400][550]\t Training Loss 0.0485\t Accuracy 0.9168\n",
      "Epoch [17][40]\t Batch [450][550]\t Training Loss 0.0485\t Accuracy 0.9169\n",
      "Epoch [17][40]\t Batch [500][550]\t Training Loss 0.0486\t Accuracy 0.9163\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0485\t Average training accuracy 0.9162\n",
      "Epoch [17]\t Average validation loss 0.0441\t Average validation accuracy 0.9402\n",
      "\n",
      "Epoch [18][40]\t Batch [0][550]\t Training Loss 0.0438\t Accuracy 0.9400\n",
      "Epoch [18][40]\t Batch [50][550]\t Training Loss 0.0471\t Accuracy 0.9280\n",
      "Epoch [18][40]\t Batch [100][550]\t Training Loss 0.0474\t Accuracy 0.9240\n",
      "Epoch [18][40]\t Batch [150][550]\t Training Loss 0.0480\t Accuracy 0.9198\n",
      "Epoch [18][40]\t Batch [200][550]\t Training Loss 0.0478\t Accuracy 0.9202\n",
      "Epoch [18][40]\t Batch [250][550]\t Training Loss 0.0479\t Accuracy 0.9206\n",
      "Epoch [18][40]\t Batch [300][550]\t Training Loss 0.0478\t Accuracy 0.9207\n",
      "Epoch [18][40]\t Batch [350][550]\t Training Loss 0.0479\t Accuracy 0.9199\n",
      "Epoch [18][40]\t Batch [400][550]\t Training Loss 0.0478\t Accuracy 0.9195\n",
      "Epoch [18][40]\t Batch [450][550]\t Training Loss 0.0478\t Accuracy 0.9196\n",
      "Epoch [18][40]\t Batch [500][550]\t Training Loss 0.0479\t Accuracy 0.9189\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0478\t Average training accuracy 0.9188\n",
      "Epoch [18]\t Average validation loss 0.0435\t Average validation accuracy 0.9420\n",
      "\n",
      "Epoch [19][40]\t Batch [0][550]\t Training Loss 0.0431\t Accuracy 0.9400\n",
      "Epoch [19][40]\t Batch [50][550]\t Training Loss 0.0464\t Accuracy 0.9294\n",
      "Epoch [19][40]\t Batch [100][550]\t Training Loss 0.0468\t Accuracy 0.9256\n",
      "Epoch [19][40]\t Batch [150][550]\t Training Loss 0.0473\t Accuracy 0.9214\n",
      "Epoch [19][40]\t Batch [200][550]\t Training Loss 0.0472\t Accuracy 0.9222\n",
      "Epoch [19][40]\t Batch [250][550]\t Training Loss 0.0472\t Accuracy 0.9227\n",
      "Epoch [19][40]\t Batch [300][550]\t Training Loss 0.0471\t Accuracy 0.9228\n",
      "Epoch [19][40]\t Batch [350][550]\t Training Loss 0.0472\t Accuracy 0.9222\n",
      "Epoch [19][40]\t Batch [400][550]\t Training Loss 0.0472\t Accuracy 0.9216\n",
      "Epoch [19][40]\t Batch [450][550]\t Training Loss 0.0472\t Accuracy 0.9218\n",
      "Epoch [19][40]\t Batch [500][550]\t Training Loss 0.0473\t Accuracy 0.9210\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0472\t Average training accuracy 0.9209\n",
      "Epoch [19]\t Average validation loss 0.0429\t Average validation accuracy 0.9434\n",
      "\n",
      "Epoch [20][40]\t Batch [0][550]\t Training Loss 0.0425\t Accuracy 0.9400\n",
      "Epoch [20][40]\t Batch [50][550]\t Training Loss 0.0458\t Accuracy 0.9308\n",
      "Epoch [20][40]\t Batch [100][550]\t Training Loss 0.0462\t Accuracy 0.9280\n",
      "Epoch [20][40]\t Batch [150][550]\t Training Loss 0.0467\t Accuracy 0.9238\n",
      "Epoch [20][40]\t Batch [200][550]\t Training Loss 0.0466\t Accuracy 0.9244\n",
      "Epoch [20][40]\t Batch [250][550]\t Training Loss 0.0466\t Accuracy 0.9249\n",
      "Epoch [20][40]\t Batch [300][550]\t Training Loss 0.0465\t Accuracy 0.9249\n",
      "Epoch [20][40]\t Batch [350][550]\t Training Loss 0.0466\t Accuracy 0.9242\n",
      "Epoch [20][40]\t Batch [400][550]\t Training Loss 0.0466\t Accuracy 0.9237\n",
      "Epoch [20][40]\t Batch [450][550]\t Training Loss 0.0466\t Accuracy 0.9238\n",
      "Epoch [20][40]\t Batch [500][550]\t Training Loss 0.0467\t Accuracy 0.9229\n",
      "\n",
      "Epoch [20]\t Average training loss 0.0466\t Average training accuracy 0.9227\n",
      "Epoch [20]\t Average validation loss 0.0423\t Average validation accuracy 0.9444\n",
      "\n",
      "Epoch [21][40]\t Batch [0][550]\t Training Loss 0.0419\t Accuracy 0.9400\n",
      "Epoch [21][40]\t Batch [50][550]\t Training Loss 0.0452\t Accuracy 0.9333\n",
      "Epoch [21][40]\t Batch [100][550]\t Training Loss 0.0456\t Accuracy 0.9299\n",
      "Epoch [21][40]\t Batch [150][550]\t Training Loss 0.0462\t Accuracy 0.9257\n",
      "Epoch [21][40]\t Batch [200][550]\t Training Loss 0.0460\t Accuracy 0.9262\n",
      "Epoch [21][40]\t Batch [250][550]\t Training Loss 0.0460\t Accuracy 0.9264\n",
      "Epoch [21][40]\t Batch [300][550]\t Training Loss 0.0460\t Accuracy 0.9262\n",
      "Epoch [21][40]\t Batch [350][550]\t Training Loss 0.0461\t Accuracy 0.9256\n",
      "Epoch [21][40]\t Batch [400][550]\t Training Loss 0.0460\t Accuracy 0.9253\n",
      "Epoch [21][40]\t Batch [450][550]\t Training Loss 0.0460\t Accuracy 0.9253\n",
      "Epoch [21][40]\t Batch [500][550]\t Training Loss 0.0461\t Accuracy 0.9244\n",
      "\n",
      "Epoch [21]\t Average training loss 0.0461\t Average training accuracy 0.9243\n",
      "Epoch [21]\t Average validation loss 0.0418\t Average validation accuracy 0.9448\n",
      "\n",
      "Epoch [22][40]\t Batch [0][550]\t Training Loss 0.0414\t Accuracy 0.9400\n",
      "Epoch [22][40]\t Batch [50][550]\t Training Loss 0.0447\t Accuracy 0.9345\n",
      "Epoch [22][40]\t Batch [100][550]\t Training Loss 0.0451\t Accuracy 0.9306\n",
      "Epoch [22][40]\t Batch [150][550]\t Training Loss 0.0456\t Accuracy 0.9270\n",
      "Epoch [22][40]\t Batch [200][550]\t Training Loss 0.0455\t Accuracy 0.9272\n",
      "Epoch [22][40]\t Batch [250][550]\t Training Loss 0.0455\t Accuracy 0.9273\n",
      "Epoch [22][40]\t Batch [300][550]\t Training Loss 0.0455\t Accuracy 0.9272\n",
      "Epoch [22][40]\t Batch [350][550]\t Training Loss 0.0455\t Accuracy 0.9268\n",
      "Epoch [22][40]\t Batch [400][550]\t Training Loss 0.0455\t Accuracy 0.9266\n",
      "Epoch [22][40]\t Batch [450][550]\t Training Loss 0.0455\t Accuracy 0.9265\n",
      "Epoch [22][40]\t Batch [500][550]\t Training Loss 0.0456\t Accuracy 0.9257\n",
      "\n",
      "Epoch [22]\t Average training loss 0.0456\t Average training accuracy 0.9257\n",
      "Epoch [22]\t Average validation loss 0.0414\t Average validation accuracy 0.9458\n",
      "\n",
      "Epoch [23][40]\t Batch [0][550]\t Training Loss 0.0409\t Accuracy 0.9400\n",
      "Epoch [23][40]\t Batch [50][550]\t Training Loss 0.0442\t Accuracy 0.9349\n",
      "Epoch [23][40]\t Batch [100][550]\t Training Loss 0.0446\t Accuracy 0.9313\n",
      "Epoch [23][40]\t Batch [150][550]\t Training Loss 0.0452\t Accuracy 0.9281\n",
      "Epoch [23][40]\t Batch [200][550]\t Training Loss 0.0450\t Accuracy 0.9285\n",
      "Epoch [23][40]\t Batch [250][550]\t Training Loss 0.0450\t Accuracy 0.9286\n",
      "Epoch [23][40]\t Batch [300][550]\t Training Loss 0.0450\t Accuracy 0.9285\n",
      "Epoch [23][40]\t Batch [350][550]\t Training Loss 0.0451\t Accuracy 0.9280\n",
      "Epoch [23][40]\t Batch [400][550]\t Training Loss 0.0451\t Accuracy 0.9279\n",
      "Epoch [23][40]\t Batch [450][550]\t Training Loss 0.0451\t Accuracy 0.9278\n",
      "Epoch [23][40]\t Batch [500][550]\t Training Loss 0.0452\t Accuracy 0.9269\n",
      "\n",
      "Epoch [23]\t Average training loss 0.0451\t Average training accuracy 0.9269\n",
      "Epoch [23]\t Average validation loss 0.0410\t Average validation accuracy 0.9466\n",
      "\n",
      "Epoch [24][40]\t Batch [0][550]\t Training Loss 0.0404\t Accuracy 0.9400\n",
      "Epoch [24][40]\t Batch [50][550]\t Training Loss 0.0437\t Accuracy 0.9363\n",
      "Epoch [24][40]\t Batch [100][550]\t Training Loss 0.0442\t Accuracy 0.9326\n",
      "Epoch [24][40]\t Batch [150][550]\t Training Loss 0.0447\t Accuracy 0.9294\n",
      "Epoch [24][40]\t Batch [200][550]\t Training Loss 0.0446\t Accuracy 0.9300\n",
      "Epoch [24][40]\t Batch [250][550]\t Training Loss 0.0446\t Accuracy 0.9304\n",
      "Epoch [24][40]\t Batch [300][550]\t Training Loss 0.0445\t Accuracy 0.9302\n",
      "Epoch [24][40]\t Batch [350][550]\t Training Loss 0.0446\t Accuracy 0.9297\n",
      "Epoch [24][40]\t Batch [400][550]\t Training Loss 0.0446\t Accuracy 0.9295\n",
      "Epoch [24][40]\t Batch [450][550]\t Training Loss 0.0446\t Accuracy 0.9294\n",
      "Epoch [24][40]\t Batch [500][550]\t Training Loss 0.0447\t Accuracy 0.9286\n",
      "\n",
      "Epoch [24]\t Average training loss 0.0447\t Average training accuracy 0.9285\n",
      "Epoch [24]\t Average validation loss 0.0406\t Average validation accuracy 0.9490\n",
      "\n",
      "Epoch [25][40]\t Batch [0][550]\t Training Loss 0.0400\t Accuracy 0.9400\n",
      "Epoch [25][40]\t Batch [50][550]\t Training Loss 0.0433\t Accuracy 0.9369\n",
      "Epoch [25][40]\t Batch [100][550]\t Training Loss 0.0437\t Accuracy 0.9333\n",
      "Epoch [25][40]\t Batch [150][550]\t Training Loss 0.0443\t Accuracy 0.9301\n",
      "Epoch [25][40]\t Batch [200][550]\t Training Loss 0.0442\t Accuracy 0.9310\n",
      "Epoch [25][40]\t Batch [250][550]\t Training Loss 0.0442\t Accuracy 0.9315\n",
      "Epoch [25][40]\t Batch [300][550]\t Training Loss 0.0441\t Accuracy 0.9313\n",
      "Epoch [25][40]\t Batch [350][550]\t Training Loss 0.0442\t Accuracy 0.9307\n",
      "Epoch [25][40]\t Batch [400][550]\t Training Loss 0.0442\t Accuracy 0.9305\n",
      "Epoch [25][40]\t Batch [450][550]\t Training Loss 0.0442\t Accuracy 0.9306\n",
      "Epoch [25][40]\t Batch [500][550]\t Training Loss 0.0443\t Accuracy 0.9297\n",
      "\n",
      "Epoch [25]\t Average training loss 0.0443\t Average training accuracy 0.9297\n",
      "Epoch [25]\t Average validation loss 0.0402\t Average validation accuracy 0.9494\n",
      "\n",
      "Epoch [26][40]\t Batch [0][550]\t Training Loss 0.0396\t Accuracy 0.9400\n",
      "Epoch [26][40]\t Batch [50][550]\t Training Loss 0.0429\t Accuracy 0.9378\n",
      "Epoch [26][40]\t Batch [100][550]\t Training Loss 0.0433\t Accuracy 0.9345\n",
      "Epoch [26][40]\t Batch [150][550]\t Training Loss 0.0439\t Accuracy 0.9314\n",
      "Epoch [26][40]\t Batch [200][550]\t Training Loss 0.0438\t Accuracy 0.9321\n",
      "Epoch [26][40]\t Batch [250][550]\t Training Loss 0.0438\t Accuracy 0.9325\n",
      "Epoch [26][40]\t Batch [300][550]\t Training Loss 0.0437\t Accuracy 0.9322\n",
      "Epoch [26][40]\t Batch [350][550]\t Training Loss 0.0438\t Accuracy 0.9315\n",
      "Epoch [26][40]\t Batch [400][550]\t Training Loss 0.0438\t Accuracy 0.9314\n",
      "Epoch [26][40]\t Batch [450][550]\t Training Loss 0.0438\t Accuracy 0.9315\n",
      "Epoch [26][40]\t Batch [500][550]\t Training Loss 0.0439\t Accuracy 0.9307\n",
      "\n",
      "Epoch [26]\t Average training loss 0.0439\t Average training accuracy 0.9307\n",
      "Epoch [26]\t Average validation loss 0.0398\t Average validation accuracy 0.9498\n",
      "\n",
      "Epoch [27][40]\t Batch [0][550]\t Training Loss 0.0393\t Accuracy 0.9400\n",
      "Epoch [27][40]\t Batch [50][550]\t Training Loss 0.0425\t Accuracy 0.9386\n",
      "Epoch [27][40]\t Batch [100][550]\t Training Loss 0.0430\t Accuracy 0.9351\n",
      "Epoch [27][40]\t Batch [150][550]\t Training Loss 0.0435\t Accuracy 0.9323\n",
      "Epoch [27][40]\t Batch [200][550]\t Training Loss 0.0434\t Accuracy 0.9330\n",
      "Epoch [27][40]\t Batch [250][550]\t Training Loss 0.0434\t Accuracy 0.9335\n",
      "Epoch [27][40]\t Batch [300][550]\t Training Loss 0.0434\t Accuracy 0.9332\n",
      "Epoch [27][40]\t Batch [350][550]\t Training Loss 0.0434\t Accuracy 0.9327\n",
      "Epoch [27][40]\t Batch [400][550]\t Training Loss 0.0435\t Accuracy 0.9325\n",
      "Epoch [27][40]\t Batch [450][550]\t Training Loss 0.0435\t Accuracy 0.9326\n",
      "Epoch [27][40]\t Batch [500][550]\t Training Loss 0.0435\t Accuracy 0.9318\n",
      "\n",
      "Epoch [27]\t Average training loss 0.0435\t Average training accuracy 0.9319\n",
      "Epoch [27]\t Average validation loss 0.0395\t Average validation accuracy 0.9506\n",
      "\n",
      "Epoch [28][40]\t Batch [0][550]\t Training Loss 0.0390\t Accuracy 0.9400\n",
      "Epoch [28][40]\t Batch [50][550]\t Training Loss 0.0421\t Accuracy 0.9392\n",
      "Epoch [28][40]\t Batch [100][550]\t Training Loss 0.0426\t Accuracy 0.9365\n",
      "Epoch [28][40]\t Batch [150][550]\t Training Loss 0.0432\t Accuracy 0.9336\n",
      "Epoch [28][40]\t Batch [200][550]\t Training Loss 0.0430\t Accuracy 0.9344\n",
      "Epoch [28][40]\t Batch [250][550]\t Training Loss 0.0430\t Accuracy 0.9347\n",
      "Epoch [28][40]\t Batch [300][550]\t Training Loss 0.0430\t Accuracy 0.9343\n",
      "Epoch [28][40]\t Batch [350][550]\t Training Loss 0.0431\t Accuracy 0.9338\n",
      "Epoch [28][40]\t Batch [400][550]\t Training Loss 0.0431\t Accuracy 0.9336\n",
      "Epoch [28][40]\t Batch [450][550]\t Training Loss 0.0431\t Accuracy 0.9336\n",
      "Epoch [28][40]\t Batch [500][550]\t Training Loss 0.0432\t Accuracy 0.9330\n",
      "\n",
      "Epoch [28]\t Average training loss 0.0432\t Average training accuracy 0.9331\n",
      "Epoch [28]\t Average validation loss 0.0392\t Average validation accuracy 0.9522\n",
      "\n",
      "Epoch [29][40]\t Batch [0][550]\t Training Loss 0.0386\t Accuracy 0.9400\n",
      "Epoch [29][40]\t Batch [50][550]\t Training Loss 0.0418\t Accuracy 0.9398\n",
      "Epoch [29][40]\t Batch [100][550]\t Training Loss 0.0423\t Accuracy 0.9370\n",
      "Epoch [29][40]\t Batch [150][550]\t Training Loss 0.0428\t Accuracy 0.9342\n",
      "Epoch [29][40]\t Batch [200][550]\t Training Loss 0.0427\t Accuracy 0.9350\n",
      "Epoch [29][40]\t Batch [250][550]\t Training Loss 0.0427\t Accuracy 0.9353\n",
      "Epoch [29][40]\t Batch [300][550]\t Training Loss 0.0427\t Accuracy 0.9350\n",
      "Epoch [29][40]\t Batch [350][550]\t Training Loss 0.0428\t Accuracy 0.9344\n",
      "Epoch [29][40]\t Batch [400][550]\t Training Loss 0.0428\t Accuracy 0.9344\n",
      "Epoch [29][40]\t Batch [450][550]\t Training Loss 0.0428\t Accuracy 0.9344\n",
      "Epoch [29][40]\t Batch [500][550]\t Training Loss 0.0429\t Accuracy 0.9339\n",
      "\n",
      "Epoch [29]\t Average training loss 0.0428\t Average training accuracy 0.9340\n",
      "Epoch [29]\t Average validation loss 0.0389\t Average validation accuracy 0.9526\n",
      "\n",
      "Epoch [30][40]\t Batch [0][550]\t Training Loss 0.0384\t Accuracy 0.9400\n",
      "Epoch [30][40]\t Batch [50][550]\t Training Loss 0.0414\t Accuracy 0.9404\n",
      "Epoch [30][40]\t Batch [100][550]\t Training Loss 0.0419\t Accuracy 0.9378\n",
      "Epoch [30][40]\t Batch [150][550]\t Training Loss 0.0425\t Accuracy 0.9351\n",
      "Epoch [30][40]\t Batch [200][550]\t Training Loss 0.0424\t Accuracy 0.9358\n",
      "Epoch [30][40]\t Batch [250][550]\t Training Loss 0.0424\t Accuracy 0.9361\n",
      "Epoch [30][40]\t Batch [300][550]\t Training Loss 0.0424\t Accuracy 0.9357\n",
      "Epoch [30][40]\t Batch [350][550]\t Training Loss 0.0424\t Accuracy 0.9352\n",
      "Epoch [30][40]\t Batch [400][550]\t Training Loss 0.0425\t Accuracy 0.9351\n",
      "Epoch [30][40]\t Batch [450][550]\t Training Loss 0.0425\t Accuracy 0.9352\n",
      "Epoch [30][40]\t Batch [500][550]\t Training Loss 0.0426\t Accuracy 0.9347\n",
      "\n",
      "Epoch [30]\t Average training loss 0.0425\t Average training accuracy 0.9347\n",
      "Epoch [30]\t Average validation loss 0.0387\t Average validation accuracy 0.9532\n",
      "\n",
      "Epoch [31][40]\t Batch [0][550]\t Training Loss 0.0381\t Accuracy 0.9400\n",
      "Epoch [31][40]\t Batch [50][550]\t Training Loss 0.0411\t Accuracy 0.9418\n",
      "Epoch [31][40]\t Batch [100][550]\t Training Loss 0.0416\t Accuracy 0.9387\n",
      "Epoch [31][40]\t Batch [150][550]\t Training Loss 0.0422\t Accuracy 0.9363\n",
      "Epoch [31][40]\t Batch [200][550]\t Training Loss 0.0421\t Accuracy 0.9370\n",
      "Epoch [31][40]\t Batch [250][550]\t Training Loss 0.0421\t Accuracy 0.9373\n",
      "Epoch [31][40]\t Batch [300][550]\t Training Loss 0.0421\t Accuracy 0.9368\n",
      "Epoch [31][40]\t Batch [350][550]\t Training Loss 0.0421\t Accuracy 0.9363\n",
      "Epoch [31][40]\t Batch [400][550]\t Training Loss 0.0422\t Accuracy 0.9362\n",
      "Epoch [31][40]\t Batch [450][550]\t Training Loss 0.0422\t Accuracy 0.9361\n",
      "Epoch [31][40]\t Batch [500][550]\t Training Loss 0.0423\t Accuracy 0.9356\n",
      "\n",
      "Epoch [31]\t Average training loss 0.0422\t Average training accuracy 0.9356\n",
      "Epoch [31]\t Average validation loss 0.0384\t Average validation accuracy 0.9536\n",
      "\n",
      "Epoch [32][40]\t Batch [0][550]\t Training Loss 0.0378\t Accuracy 0.9400\n",
      "Epoch [32][40]\t Batch [50][550]\t Training Loss 0.0409\t Accuracy 0.9416\n",
      "Epoch [32][40]\t Batch [100][550]\t Training Loss 0.0414\t Accuracy 0.9392\n",
      "Epoch [32][40]\t Batch [150][550]\t Training Loss 0.0419\t Accuracy 0.9368\n",
      "Epoch [32][40]\t Batch [200][550]\t Training Loss 0.0418\t Accuracy 0.9375\n",
      "Epoch [32][40]\t Batch [250][550]\t Training Loss 0.0418\t Accuracy 0.9379\n",
      "Epoch [32][40]\t Batch [300][550]\t Training Loss 0.0418\t Accuracy 0.9375\n",
      "Epoch [32][40]\t Batch [350][550]\t Training Loss 0.0419\t Accuracy 0.9371\n",
      "Epoch [32][40]\t Batch [400][550]\t Training Loss 0.0419\t Accuracy 0.9371\n",
      "Epoch [32][40]\t Batch [450][550]\t Training Loss 0.0419\t Accuracy 0.9371\n",
      "Epoch [32][40]\t Batch [500][550]\t Training Loss 0.0420\t Accuracy 0.9365\n",
      "\n",
      "Epoch [32]\t Average training loss 0.0420\t Average training accuracy 0.9366\n",
      "Epoch [32]\t Average validation loss 0.0382\t Average validation accuracy 0.9540\n",
      "\n",
      "Epoch [33][40]\t Batch [0][550]\t Training Loss 0.0376\t Accuracy 0.9400\n",
      "Epoch [33][40]\t Batch [50][550]\t Training Loss 0.0406\t Accuracy 0.9425\n",
      "Epoch [33][40]\t Batch [100][550]\t Training Loss 0.0411\t Accuracy 0.9400\n",
      "Epoch [33][40]\t Batch [150][550]\t Training Loss 0.0416\t Accuracy 0.9375\n",
      "Epoch [33][40]\t Batch [200][550]\t Training Loss 0.0415\t Accuracy 0.9382\n",
      "Epoch [33][40]\t Batch [250][550]\t Training Loss 0.0415\t Accuracy 0.9387\n",
      "Epoch [33][40]\t Batch [300][550]\t Training Loss 0.0415\t Accuracy 0.9382\n",
      "Epoch [33][40]\t Batch [350][550]\t Training Loss 0.0416\t Accuracy 0.9378\n",
      "Epoch [33][40]\t Batch [400][550]\t Training Loss 0.0416\t Accuracy 0.9378\n",
      "Epoch [33][40]\t Batch [450][550]\t Training Loss 0.0416\t Accuracy 0.9378\n",
      "Epoch [33][40]\t Batch [500][550]\t Training Loss 0.0417\t Accuracy 0.9372\n",
      "\n",
      "Epoch [33]\t Average training loss 0.0417\t Average training accuracy 0.9373\n",
      "Epoch [33]\t Average validation loss 0.0380\t Average validation accuracy 0.9548\n",
      "\n",
      "Epoch [34][40]\t Batch [0][550]\t Training Loss 0.0374\t Accuracy 0.9400\n",
      "Epoch [34][40]\t Batch [50][550]\t Training Loss 0.0403\t Accuracy 0.9429\n",
      "Epoch [34][40]\t Batch [100][550]\t Training Loss 0.0408\t Accuracy 0.9405\n",
      "Epoch [34][40]\t Batch [150][550]\t Training Loss 0.0414\t Accuracy 0.9383\n",
      "Epoch [34][40]\t Batch [200][550]\t Training Loss 0.0413\t Accuracy 0.9392\n",
      "Epoch [34][40]\t Batch [250][550]\t Training Loss 0.0413\t Accuracy 0.9396\n",
      "Epoch [34][40]\t Batch [300][550]\t Training Loss 0.0412\t Accuracy 0.9390\n",
      "Epoch [34][40]\t Batch [350][550]\t Training Loss 0.0413\t Accuracy 0.9387\n",
      "Epoch [34][40]\t Batch [400][550]\t Training Loss 0.0414\t Accuracy 0.9387\n",
      "Epoch [34][40]\t Batch [450][550]\t Training Loss 0.0414\t Accuracy 0.9388\n",
      "Epoch [34][40]\t Batch [500][550]\t Training Loss 0.0415\t Accuracy 0.9381\n",
      "\n",
      "Epoch [34]\t Average training loss 0.0414\t Average training accuracy 0.9381\n",
      "Epoch [34]\t Average validation loss 0.0378\t Average validation accuracy 0.9552\n",
      "\n",
      "Epoch [35][40]\t Batch [0][550]\t Training Loss 0.0371\t Accuracy 0.9400\n",
      "Epoch [35][40]\t Batch [50][550]\t Training Loss 0.0401\t Accuracy 0.9433\n",
      "Epoch [35][40]\t Batch [100][550]\t Training Loss 0.0406\t Accuracy 0.9411\n",
      "Epoch [35][40]\t Batch [150][550]\t Training Loss 0.0411\t Accuracy 0.9389\n",
      "Epoch [35][40]\t Batch [200][550]\t Training Loss 0.0410\t Accuracy 0.9398\n",
      "Epoch [35][40]\t Batch [250][550]\t Training Loss 0.0410\t Accuracy 0.9401\n",
      "Epoch [35][40]\t Batch [300][550]\t Training Loss 0.0410\t Accuracy 0.9394\n",
      "Epoch [35][40]\t Batch [350][550]\t Training Loss 0.0411\t Accuracy 0.9392\n",
      "Epoch [35][40]\t Batch [400][550]\t Training Loss 0.0411\t Accuracy 0.9392\n",
      "Epoch [35][40]\t Batch [450][550]\t Training Loss 0.0411\t Accuracy 0.9393\n",
      "Epoch [35][40]\t Batch [500][550]\t Training Loss 0.0412\t Accuracy 0.9387\n",
      "\n",
      "Epoch [35]\t Average training loss 0.0412\t Average training accuracy 0.9387\n",
      "Epoch [35]\t Average validation loss 0.0376\t Average validation accuracy 0.9554\n",
      "\n",
      "Epoch [36][40]\t Batch [0][550]\t Training Loss 0.0369\t Accuracy 0.9400\n",
      "Epoch [36][40]\t Batch [50][550]\t Training Loss 0.0398\t Accuracy 0.9441\n",
      "Epoch [36][40]\t Batch [100][550]\t Training Loss 0.0403\t Accuracy 0.9416\n",
      "Epoch [36][40]\t Batch [150][550]\t Training Loss 0.0409\t Accuracy 0.9394\n",
      "Epoch [36][40]\t Batch [200][550]\t Training Loss 0.0408\t Accuracy 0.9403\n",
      "Epoch [36][40]\t Batch [250][550]\t Training Loss 0.0408\t Accuracy 0.9405\n",
      "Epoch [36][40]\t Batch [300][550]\t Training Loss 0.0408\t Accuracy 0.9400\n",
      "Epoch [36][40]\t Batch [350][550]\t Training Loss 0.0409\t Accuracy 0.9397\n",
      "Epoch [36][40]\t Batch [400][550]\t Training Loss 0.0409\t Accuracy 0.9398\n",
      "Epoch [36][40]\t Batch [450][550]\t Training Loss 0.0409\t Accuracy 0.9399\n",
      "Epoch [36][40]\t Batch [500][550]\t Training Loss 0.0410\t Accuracy 0.9392\n",
      "\n",
      "Epoch [36]\t Average training loss 0.0410\t Average training accuracy 0.9392\n",
      "Epoch [36]\t Average validation loss 0.0374\t Average validation accuracy 0.9560\n",
      "\n",
      "Epoch [37][40]\t Batch [0][550]\t Training Loss 0.0367\t Accuracy 0.9400\n",
      "Epoch [37][40]\t Batch [50][550]\t Training Loss 0.0396\t Accuracy 0.9451\n",
      "Epoch [37][40]\t Batch [100][550]\t Training Loss 0.0401\t Accuracy 0.9424\n",
      "Epoch [37][40]\t Batch [150][550]\t Training Loss 0.0406\t Accuracy 0.9405\n",
      "Epoch [37][40]\t Batch [200][550]\t Training Loss 0.0406\t Accuracy 0.9412\n",
      "Epoch [37][40]\t Batch [250][550]\t Training Loss 0.0406\t Accuracy 0.9414\n",
      "Epoch [37][40]\t Batch [300][550]\t Training Loss 0.0405\t Accuracy 0.9408\n",
      "Epoch [37][40]\t Batch [350][550]\t Training Loss 0.0406\t Accuracy 0.9405\n",
      "Epoch [37][40]\t Batch [400][550]\t Training Loss 0.0407\t Accuracy 0.9405\n",
      "Epoch [37][40]\t Batch [450][550]\t Training Loss 0.0407\t Accuracy 0.9406\n",
      "Epoch [37][40]\t Batch [500][550]\t Training Loss 0.0408\t Accuracy 0.9398\n",
      "\n",
      "Epoch [37]\t Average training loss 0.0407\t Average training accuracy 0.9399\n",
      "Epoch [37]\t Average validation loss 0.0372\t Average validation accuracy 0.9560\n",
      "\n",
      "Epoch [38][40]\t Batch [0][550]\t Training Loss 0.0365\t Accuracy 0.9500\n",
      "Epoch [38][40]\t Batch [50][550]\t Training Loss 0.0394\t Accuracy 0.9453\n",
      "Epoch [38][40]\t Batch [100][550]\t Training Loss 0.0399\t Accuracy 0.9426\n",
      "Epoch [38][40]\t Batch [150][550]\t Training Loss 0.0404\t Accuracy 0.9409\n",
      "Epoch [38][40]\t Batch [200][550]\t Training Loss 0.0403\t Accuracy 0.9416\n",
      "Epoch [38][40]\t Batch [250][550]\t Training Loss 0.0403\t Accuracy 0.9417\n",
      "Epoch [38][40]\t Batch [300][550]\t Training Loss 0.0403\t Accuracy 0.9412\n",
      "Epoch [38][40]\t Batch [350][550]\t Training Loss 0.0404\t Accuracy 0.9409\n",
      "Epoch [38][40]\t Batch [400][550]\t Training Loss 0.0405\t Accuracy 0.9409\n",
      "Epoch [38][40]\t Batch [450][550]\t Training Loss 0.0405\t Accuracy 0.9409\n",
      "Epoch [38][40]\t Batch [500][550]\t Training Loss 0.0406\t Accuracy 0.9402\n",
      "\n",
      "Epoch [38]\t Average training loss 0.0405\t Average training accuracy 0.9402\n",
      "Epoch [38]\t Average validation loss 0.0370\t Average validation accuracy 0.9566\n",
      "\n",
      "Epoch [39][40]\t Batch [0][550]\t Training Loss 0.0364\t Accuracy 0.9500\n",
      "Epoch [39][40]\t Batch [50][550]\t Training Loss 0.0392\t Accuracy 0.9463\n",
      "Epoch [39][40]\t Batch [100][550]\t Training Loss 0.0397\t Accuracy 0.9433\n",
      "Epoch [39][40]\t Batch [150][550]\t Training Loss 0.0402\t Accuracy 0.9416\n",
      "Epoch [39][40]\t Batch [200][550]\t Training Loss 0.0401\t Accuracy 0.9422\n",
      "Epoch [39][40]\t Batch [250][550]\t Training Loss 0.0401\t Accuracy 0.9422\n",
      "Epoch [39][40]\t Batch [300][550]\t Training Loss 0.0401\t Accuracy 0.9418\n",
      "Epoch [39][40]\t Batch [350][550]\t Training Loss 0.0402\t Accuracy 0.9415\n",
      "Epoch [39][40]\t Batch [400][550]\t Training Loss 0.0403\t Accuracy 0.9413\n",
      "Epoch [39][40]\t Batch [450][550]\t Training Loss 0.0403\t Accuracy 0.9414\n",
      "Epoch [39][40]\t Batch [500][550]\t Training Loss 0.0404\t Accuracy 0.9408\n",
      "\n",
      "Epoch [39]\t Average training loss 0.0403\t Average training accuracy 0.9407\n",
      "Epoch [39]\t Average validation loss 0.0369\t Average validation accuracy 0.9570\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9403.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 MLP with Euclidean Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Euclidean loss function.\n",
    "\n",
    "### TODO\n",
    "Before executing the following code, you should complete **layers/relu_layer.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 40\n",
    "init_std = 0.01\n",
    "\n",
    "# (0.1 : 0.92) (0.001 : 0.94) (0.00001 : 0.93)\n",
    "learning_rate_SGD = 0.003\n",
    "weight_decay = 0.001\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import FCLayer, ReLULayer\n",
    "\n",
    "reluMLP = Network()\n",
    "# TODO build ReLUMLP with FCLayer and ReLULayer\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][40]\t Batch [0][550]\t Training Loss 0.3434\t Accuracy 0.0500\n",
      "Epoch [0][40]\t Batch [50][550]\t Training Loss 0.1402\t Accuracy 0.3427\n",
      "Epoch [0][40]\t Batch [100][550]\t Training Loss 0.1190\t Accuracy 0.4719\n",
      "Epoch [0][40]\t Batch [150][550]\t Training Loss 0.1084\t Accuracy 0.5443\n",
      "Epoch [0][40]\t Batch [200][550]\t Training Loss 0.1013\t Accuracy 0.5929\n",
      "Epoch [0][40]\t Batch [250][550]\t Training Loss 0.0961\t Accuracy 0.6318\n",
      "Epoch [0][40]\t Batch [300][550]\t Training Loss 0.0920\t Accuracy 0.6617\n",
      "Epoch [0][40]\t Batch [350][550]\t Training Loss 0.0888\t Accuracy 0.6838\n",
      "Epoch [0][40]\t Batch [400][550]\t Training Loss 0.0861\t Accuracy 0.7032\n",
      "Epoch [0][40]\t Batch [450][550]\t Training Loss 0.0838\t Accuracy 0.7194\n",
      "Epoch [0][40]\t Batch [500][550]\t Training Loss 0.0818\t Accuracy 0.7321\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0799\t Average training accuracy 0.7443\n",
      "Epoch [0]\t Average validation loss 0.0576\t Average validation accuracy 0.9012\n",
      "\n",
      "Epoch [1][40]\t Batch [0][550]\t Training Loss 0.0586\t Accuracy 0.9000\n",
      "Epoch [1][40]\t Batch [50][550]\t Training Loss 0.0589\t Accuracy 0.8792\n",
      "Epoch [1][40]\t Batch [100][550]\t Training Loss 0.0585\t Accuracy 0.8823\n",
      "Epoch [1][40]\t Batch [150][550]\t Training Loss 0.0583\t Accuracy 0.8832\n",
      "Epoch [1][40]\t Batch [200][550]\t Training Loss 0.0578\t Accuracy 0.8850\n",
      "Epoch [1][40]\t Batch [250][550]\t Training Loss 0.0574\t Accuracy 0.8875\n",
      "Epoch [1][40]\t Batch [300][550]\t Training Loss 0.0570\t Accuracy 0.8885\n",
      "Epoch [1][40]\t Batch [350][550]\t Training Loss 0.0566\t Accuracy 0.8891\n",
      "Epoch [1][40]\t Batch [400][550]\t Training Loss 0.0562\t Accuracy 0.8903\n",
      "Epoch [1][40]\t Batch [450][550]\t Training Loss 0.0559\t Accuracy 0.8913\n",
      "Epoch [1][40]\t Batch [500][550]\t Training Loss 0.0556\t Accuracy 0.8917\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0552\t Average training accuracy 0.8929\n",
      "Epoch [1]\t Average validation loss 0.0485\t Average validation accuracy 0.9300\n",
      "\n",
      "Epoch [2][40]\t Batch [0][550]\t Training Loss 0.0501\t Accuracy 0.9400\n",
      "Epoch [2][40]\t Batch [50][550]\t Training Loss 0.0500\t Accuracy 0.9159\n",
      "Epoch [2][40]\t Batch [100][550]\t Training Loss 0.0500\t Accuracy 0.9139\n",
      "Epoch [2][40]\t Batch [150][550]\t Training Loss 0.0502\t Accuracy 0.9128\n",
      "Epoch [2][40]\t Batch [200][550]\t Training Loss 0.0499\t Accuracy 0.9143\n",
      "Epoch [2][40]\t Batch [250][550]\t Training Loss 0.0498\t Accuracy 0.9143\n",
      "Epoch [2][40]\t Batch [300][550]\t Training Loss 0.0496\t Accuracy 0.9144\n",
      "Epoch [2][40]\t Batch [350][550]\t Training Loss 0.0495\t Accuracy 0.9145\n",
      "Epoch [2][40]\t Batch [400][550]\t Training Loss 0.0494\t Accuracy 0.9146\n",
      "Epoch [2][40]\t Batch [450][550]\t Training Loss 0.0492\t Accuracy 0.9147\n",
      "Epoch [2][40]\t Batch [500][550]\t Training Loss 0.0492\t Accuracy 0.9145\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0490\t Average training accuracy 0.9148\n",
      "Epoch [2]\t Average validation loss 0.0445\t Average validation accuracy 0.9410\n",
      "\n",
      "Epoch [3][40]\t Batch [0][550]\t Training Loss 0.0460\t Accuracy 0.9400\n",
      "Epoch [3][40]\t Batch [50][550]\t Training Loss 0.0458\t Accuracy 0.9280\n",
      "Epoch [3][40]\t Batch [100][550]\t Training Loss 0.0460\t Accuracy 0.9256\n",
      "Epoch [3][40]\t Batch [150][550]\t Training Loss 0.0463\t Accuracy 0.9252\n",
      "Epoch [3][40]\t Batch [200][550]\t Training Loss 0.0461\t Accuracy 0.9267\n",
      "Epoch [3][40]\t Batch [250][550]\t Training Loss 0.0460\t Accuracy 0.9265\n",
      "Epoch [3][40]\t Batch [300][550]\t Training Loss 0.0460\t Accuracy 0.9262\n",
      "Epoch [3][40]\t Batch [350][550]\t Training Loss 0.0460\t Accuracy 0.9260\n",
      "Epoch [3][40]\t Batch [400][550]\t Training Loss 0.0459\t Accuracy 0.9258\n",
      "Epoch [3][40]\t Batch [450][550]\t Training Loss 0.0459\t Accuracy 0.9257\n",
      "Epoch [3][40]\t Batch [500][550]\t Training Loss 0.0459\t Accuracy 0.9251\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0458\t Average training accuracy 0.9253\n",
      "Epoch [3]\t Average validation loss 0.0422\t Average validation accuracy 0.9466\n",
      "\n",
      "Epoch [4][40]\t Batch [0][550]\t Training Loss 0.0436\t Accuracy 0.9500\n",
      "Epoch [4][40]\t Batch [50][550]\t Training Loss 0.0433\t Accuracy 0.9361\n",
      "Epoch [4][40]\t Batch [100][550]\t Training Loss 0.0436\t Accuracy 0.9335\n",
      "Epoch [4][40]\t Batch [150][550]\t Training Loss 0.0439\t Accuracy 0.9327\n",
      "Epoch [4][40]\t Batch [200][550]\t Training Loss 0.0438\t Accuracy 0.9341\n",
      "Epoch [4][40]\t Batch [250][550]\t Training Loss 0.0438\t Accuracy 0.9338\n",
      "Epoch [4][40]\t Batch [300][550]\t Training Loss 0.0438\t Accuracy 0.9335\n",
      "Epoch [4][40]\t Batch [350][550]\t Training Loss 0.0438\t Accuracy 0.9333\n",
      "Epoch [4][40]\t Batch [400][550]\t Training Loss 0.0438\t Accuracy 0.9333\n",
      "Epoch [4][40]\t Batch [450][550]\t Training Loss 0.0437\t Accuracy 0.9333\n",
      "Epoch [4][40]\t Batch [500][550]\t Training Loss 0.0438\t Accuracy 0.9325\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0437\t Average training accuracy 0.9326\n",
      "Epoch [4]\t Average validation loss 0.0406\t Average validation accuracy 0.9510\n",
      "\n",
      "Epoch [5][40]\t Batch [0][550]\t Training Loss 0.0418\t Accuracy 0.9500\n",
      "Epoch [5][40]\t Batch [50][550]\t Training Loss 0.0416\t Accuracy 0.9418\n",
      "Epoch [5][40]\t Batch [100][550]\t Training Loss 0.0419\t Accuracy 0.9388\n",
      "Epoch [5][40]\t Batch [150][550]\t Training Loss 0.0423\t Accuracy 0.9377\n",
      "Epoch [5][40]\t Batch [200][550]\t Training Loss 0.0422\t Accuracy 0.9389\n",
      "Epoch [5][40]\t Batch [250][550]\t Training Loss 0.0421\t Accuracy 0.9388\n",
      "Epoch [5][40]\t Batch [300][550]\t Training Loss 0.0422\t Accuracy 0.9381\n",
      "Epoch [5][40]\t Batch [350][550]\t Training Loss 0.0422\t Accuracy 0.9381\n",
      "Epoch [5][40]\t Batch [400][550]\t Training Loss 0.0422\t Accuracy 0.9383\n",
      "Epoch [5][40]\t Batch [450][550]\t Training Loss 0.0422\t Accuracy 0.9384\n",
      "Epoch [5][40]\t Batch [500][550]\t Training Loss 0.0423\t Accuracy 0.9375\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0422\t Average training accuracy 0.9376\n",
      "Epoch [5]\t Average validation loss 0.0395\t Average validation accuracy 0.9538\n",
      "\n",
      "Epoch [6][40]\t Batch [0][550]\t Training Loss 0.0405\t Accuracy 0.9500\n",
      "Epoch [6][40]\t Batch [50][550]\t Training Loss 0.0403\t Accuracy 0.9449\n",
      "Epoch [6][40]\t Batch [100][550]\t Training Loss 0.0406\t Accuracy 0.9428\n",
      "Epoch [6][40]\t Batch [150][550]\t Training Loss 0.0410\t Accuracy 0.9418\n",
      "Epoch [6][40]\t Batch [200][550]\t Training Loss 0.0410\t Accuracy 0.9426\n",
      "Epoch [6][40]\t Batch [250][550]\t Training Loss 0.0409\t Accuracy 0.9427\n",
      "Epoch [6][40]\t Batch [300][550]\t Training Loss 0.0410\t Accuracy 0.9421\n",
      "Epoch [6][40]\t Batch [350][550]\t Training Loss 0.0410\t Accuracy 0.9422\n",
      "Epoch [6][40]\t Batch [400][550]\t Training Loss 0.0410\t Accuracy 0.9423\n",
      "Epoch [6][40]\t Batch [450][550]\t Training Loss 0.0410\t Accuracy 0.9424\n",
      "Epoch [6][40]\t Batch [500][550]\t Training Loss 0.0411\t Accuracy 0.9415\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0410\t Average training accuracy 0.9416\n",
      "Epoch [6]\t Average validation loss 0.0386\t Average validation accuracy 0.9550\n",
      "\n",
      "Epoch [7][40]\t Batch [0][550]\t Training Loss 0.0394\t Accuracy 0.9500\n",
      "Epoch [7][40]\t Batch [50][550]\t Training Loss 0.0393\t Accuracy 0.9486\n",
      "Epoch [7][40]\t Batch [100][550]\t Training Loss 0.0396\t Accuracy 0.9458\n",
      "Epoch [7][40]\t Batch [150][550]\t Training Loss 0.0400\t Accuracy 0.9452\n",
      "Epoch [7][40]\t Batch [200][550]\t Training Loss 0.0399\t Accuracy 0.9461\n",
      "Epoch [7][40]\t Batch [250][550]\t Training Loss 0.0398\t Accuracy 0.9463\n",
      "Epoch [7][40]\t Batch [300][550]\t Training Loss 0.0399\t Accuracy 0.9457\n",
      "Epoch [7][40]\t Batch [350][550]\t Training Loss 0.0400\t Accuracy 0.9456\n",
      "Epoch [7][40]\t Batch [400][550]\t Training Loss 0.0400\t Accuracy 0.9457\n",
      "Epoch [7][40]\t Batch [450][550]\t Training Loss 0.0400\t Accuracy 0.9459\n",
      "Epoch [7][40]\t Batch [500][550]\t Training Loss 0.0401\t Accuracy 0.9451\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0400\t Average training accuracy 0.9453\n",
      "Epoch [7]\t Average validation loss 0.0378\t Average validation accuracy 0.9562\n",
      "\n",
      "Epoch [8][40]\t Batch [0][550]\t Training Loss 0.0386\t Accuracy 0.9600\n",
      "Epoch [8][40]\t Batch [50][550]\t Training Loss 0.0384\t Accuracy 0.9524\n",
      "Epoch [8][40]\t Batch [100][550]\t Training Loss 0.0388\t Accuracy 0.9489\n",
      "Epoch [8][40]\t Batch [150][550]\t Training Loss 0.0392\t Accuracy 0.9479\n",
      "Epoch [8][40]\t Batch [200][550]\t Training Loss 0.0391\t Accuracy 0.9487\n",
      "Epoch [8][40]\t Batch [250][550]\t Training Loss 0.0390\t Accuracy 0.9488\n",
      "Epoch [8][40]\t Batch [300][550]\t Training Loss 0.0391\t Accuracy 0.9480\n",
      "Epoch [8][40]\t Batch [350][550]\t Training Loss 0.0391\t Accuracy 0.9479\n",
      "Epoch [8][40]\t Batch [400][550]\t Training Loss 0.0391\t Accuracy 0.9479\n",
      "Epoch [8][40]\t Batch [450][550]\t Training Loss 0.0391\t Accuracy 0.9482\n",
      "Epoch [8][40]\t Batch [500][550]\t Training Loss 0.0392\t Accuracy 0.9476\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0392\t Average training accuracy 0.9478\n",
      "Epoch [8]\t Average validation loss 0.0372\t Average validation accuracy 0.9580\n",
      "\n",
      "Epoch [9][40]\t Batch [0][550]\t Training Loss 0.0377\t Accuracy 0.9600\n",
      "Epoch [9][40]\t Batch [50][550]\t Training Loss 0.0377\t Accuracy 0.9545\n",
      "Epoch [9][40]\t Batch [100][550]\t Training Loss 0.0380\t Accuracy 0.9514\n",
      "Epoch [9][40]\t Batch [150][550]\t Training Loss 0.0384\t Accuracy 0.9504\n",
      "Epoch [9][40]\t Batch [200][550]\t Training Loss 0.0384\t Accuracy 0.9512\n",
      "Epoch [9][40]\t Batch [250][550]\t Training Loss 0.0382\t Accuracy 0.9514\n",
      "Epoch [9][40]\t Batch [300][550]\t Training Loss 0.0384\t Accuracy 0.9508\n",
      "Epoch [9][40]\t Batch [350][550]\t Training Loss 0.0384\t Accuracy 0.9507\n",
      "Epoch [9][40]\t Batch [400][550]\t Training Loss 0.0384\t Accuracy 0.9506\n",
      "Epoch [9][40]\t Batch [450][550]\t Training Loss 0.0384\t Accuracy 0.9509\n",
      "Epoch [9][40]\t Batch [500][550]\t Training Loss 0.0385\t Accuracy 0.9502\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0384\t Average training accuracy 0.9503\n",
      "Epoch [9]\t Average validation loss 0.0367\t Average validation accuracy 0.9592\n",
      "\n",
      "Epoch [10][40]\t Batch [0][550]\t Training Loss 0.0370\t Accuracy 0.9600\n",
      "Epoch [10][40]\t Batch [50][550]\t Training Loss 0.0371\t Accuracy 0.9561\n",
      "Epoch [10][40]\t Batch [100][550]\t Training Loss 0.0374\t Accuracy 0.9534\n",
      "Epoch [10][40]\t Batch [150][550]\t Training Loss 0.0378\t Accuracy 0.9523\n",
      "Epoch [10][40]\t Batch [200][550]\t Training Loss 0.0377\t Accuracy 0.9528\n",
      "Epoch [10][40]\t Batch [250][550]\t Training Loss 0.0376\t Accuracy 0.9531\n",
      "Epoch [10][40]\t Batch [300][550]\t Training Loss 0.0377\t Accuracy 0.9527\n",
      "Epoch [10][40]\t Batch [350][550]\t Training Loss 0.0377\t Accuracy 0.9528\n",
      "Epoch [10][40]\t Batch [400][550]\t Training Loss 0.0378\t Accuracy 0.9526\n",
      "Epoch [10][40]\t Batch [450][550]\t Training Loss 0.0378\t Accuracy 0.9527\n",
      "Epoch [10][40]\t Batch [500][550]\t Training Loss 0.0379\t Accuracy 0.9520\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0378\t Average training accuracy 0.9521\n",
      "Epoch [10]\t Average validation loss 0.0362\t Average validation accuracy 0.9608\n",
      "\n",
      "Epoch [11][40]\t Batch [0][550]\t Training Loss 0.0364\t Accuracy 0.9700\n",
      "Epoch [11][40]\t Batch [50][550]\t Training Loss 0.0365\t Accuracy 0.9578\n",
      "Epoch [11][40]\t Batch [100][550]\t Training Loss 0.0368\t Accuracy 0.9550\n",
      "Epoch [11][40]\t Batch [150][550]\t Training Loss 0.0372\t Accuracy 0.9540\n",
      "Epoch [11][40]\t Batch [200][550]\t Training Loss 0.0372\t Accuracy 0.9545\n",
      "Epoch [11][40]\t Batch [250][550]\t Training Loss 0.0370\t Accuracy 0.9549\n",
      "Epoch [11][40]\t Batch [300][550]\t Training Loss 0.0372\t Accuracy 0.9546\n",
      "Epoch [11][40]\t Batch [350][550]\t Training Loss 0.0372\t Accuracy 0.9547\n",
      "Epoch [11][40]\t Batch [400][550]\t Training Loss 0.0372\t Accuracy 0.9545\n",
      "Epoch [11][40]\t Batch [450][550]\t Training Loss 0.0372\t Accuracy 0.9547\n",
      "Epoch [11][40]\t Batch [500][550]\t Training Loss 0.0373\t Accuracy 0.9541\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0373\t Average training accuracy 0.9542\n",
      "Epoch [11]\t Average validation loss 0.0358\t Average validation accuracy 0.9626\n",
      "\n",
      "Epoch [12][40]\t Batch [0][550]\t Training Loss 0.0358\t Accuracy 0.9700\n",
      "Epoch [12][40]\t Batch [50][550]\t Training Loss 0.0360\t Accuracy 0.9584\n",
      "Epoch [12][40]\t Batch [100][550]\t Training Loss 0.0363\t Accuracy 0.9557\n",
      "Epoch [12][40]\t Batch [150][550]\t Training Loss 0.0367\t Accuracy 0.9547\n",
      "Epoch [12][40]\t Batch [200][550]\t Training Loss 0.0366\t Accuracy 0.9554\n",
      "Epoch [12][40]\t Batch [250][550]\t Training Loss 0.0365\t Accuracy 0.9559\n",
      "Epoch [12][40]\t Batch [300][550]\t Training Loss 0.0367\t Accuracy 0.9558\n",
      "Epoch [12][40]\t Batch [350][550]\t Training Loss 0.0367\t Accuracy 0.9560\n",
      "Epoch [12][40]\t Batch [400][550]\t Training Loss 0.0367\t Accuracy 0.9559\n",
      "Epoch [12][40]\t Batch [450][550]\t Training Loss 0.0367\t Accuracy 0.9562\n",
      "Epoch [12][40]\t Batch [500][550]\t Training Loss 0.0368\t Accuracy 0.9557\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0368\t Average training accuracy 0.9558\n",
      "Epoch [12]\t Average validation loss 0.0355\t Average validation accuracy 0.9644\n",
      "\n",
      "Epoch [13][40]\t Batch [0][550]\t Training Loss 0.0352\t Accuracy 0.9700\n",
      "Epoch [13][40]\t Batch [50][550]\t Training Loss 0.0355\t Accuracy 0.9602\n",
      "Epoch [13][40]\t Batch [100][550]\t Training Loss 0.0359\t Accuracy 0.9576\n",
      "Epoch [13][40]\t Batch [150][550]\t Training Loss 0.0363\t Accuracy 0.9567\n",
      "Epoch [13][40]\t Batch [200][550]\t Training Loss 0.0362\t Accuracy 0.9574\n",
      "Epoch [13][40]\t Batch [250][550]\t Training Loss 0.0361\t Accuracy 0.9578\n",
      "Epoch [13][40]\t Batch [300][550]\t Training Loss 0.0362\t Accuracy 0.9575\n",
      "Epoch [13][40]\t Batch [350][550]\t Training Loss 0.0362\t Accuracy 0.9576\n",
      "Epoch [13][40]\t Batch [400][550]\t Training Loss 0.0362\t Accuracy 0.9575\n",
      "Epoch [13][40]\t Batch [450][550]\t Training Loss 0.0362\t Accuracy 0.9577\n",
      "Epoch [13][40]\t Batch [500][550]\t Training Loss 0.0364\t Accuracy 0.9572\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0363\t Average training accuracy 0.9574\n",
      "Epoch [13]\t Average validation loss 0.0352\t Average validation accuracy 0.9644\n",
      "\n",
      "Epoch [14][40]\t Batch [0][550]\t Training Loss 0.0348\t Accuracy 0.9700\n",
      "Epoch [14][40]\t Batch [50][550]\t Training Loss 0.0351\t Accuracy 0.9606\n",
      "Epoch [14][40]\t Batch [100][550]\t Training Loss 0.0355\t Accuracy 0.9584\n",
      "Epoch [14][40]\t Batch [150][550]\t Training Loss 0.0358\t Accuracy 0.9577\n",
      "Epoch [14][40]\t Batch [200][550]\t Training Loss 0.0358\t Accuracy 0.9585\n",
      "Epoch [14][40]\t Batch [250][550]\t Training Loss 0.0356\t Accuracy 0.9589\n",
      "Epoch [14][40]\t Batch [300][550]\t Training Loss 0.0358\t Accuracy 0.9586\n",
      "Epoch [14][40]\t Batch [350][550]\t Training Loss 0.0358\t Accuracy 0.9588\n",
      "Epoch [14][40]\t Batch [400][550]\t Training Loss 0.0358\t Accuracy 0.9586\n",
      "Epoch [14][40]\t Batch [450][550]\t Training Loss 0.0358\t Accuracy 0.9588\n",
      "Epoch [14][40]\t Batch [500][550]\t Training Loss 0.0359\t Accuracy 0.9583\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0359\t Average training accuracy 0.9585\n",
      "Epoch [14]\t Average validation loss 0.0349\t Average validation accuracy 0.9646\n",
      "\n",
      "Epoch [15][40]\t Batch [0][550]\t Training Loss 0.0345\t Accuracy 0.9700\n",
      "Epoch [15][40]\t Batch [50][550]\t Training Loss 0.0347\t Accuracy 0.9618\n",
      "Epoch [15][40]\t Batch [100][550]\t Training Loss 0.0351\t Accuracy 0.9595\n",
      "Epoch [15][40]\t Batch [150][550]\t Training Loss 0.0354\t Accuracy 0.9590\n",
      "Epoch [15][40]\t Batch [200][550]\t Training Loss 0.0354\t Accuracy 0.9597\n",
      "Epoch [15][40]\t Batch [250][550]\t Training Loss 0.0353\t Accuracy 0.9603\n",
      "Epoch [15][40]\t Batch [300][550]\t Training Loss 0.0354\t Accuracy 0.9599\n",
      "Epoch [15][40]\t Batch [350][550]\t Training Loss 0.0354\t Accuracy 0.9601\n",
      "Epoch [15][40]\t Batch [400][550]\t Training Loss 0.0354\t Accuracy 0.9600\n",
      "Epoch [15][40]\t Batch [450][550]\t Training Loss 0.0354\t Accuracy 0.9601\n",
      "Epoch [15][40]\t Batch [500][550]\t Training Loss 0.0356\t Accuracy 0.9595\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0355\t Average training accuracy 0.9597\n",
      "Epoch [15]\t Average validation loss 0.0346\t Average validation accuracy 0.9654\n",
      "\n",
      "Epoch [16][40]\t Batch [0][550]\t Training Loss 0.0341\t Accuracy 0.9700\n",
      "Epoch [16][40]\t Batch [50][550]\t Training Loss 0.0344\t Accuracy 0.9625\n",
      "Epoch [16][40]\t Batch [100][550]\t Training Loss 0.0347\t Accuracy 0.9604\n",
      "Epoch [16][40]\t Batch [150][550]\t Training Loss 0.0351\t Accuracy 0.9599\n",
      "Epoch [16][40]\t Batch [200][550]\t Training Loss 0.0350\t Accuracy 0.9605\n",
      "Epoch [16][40]\t Batch [250][550]\t Training Loss 0.0349\t Accuracy 0.9611\n",
      "Epoch [16][40]\t Batch [300][550]\t Training Loss 0.0351\t Accuracy 0.9608\n",
      "Epoch [16][40]\t Batch [350][550]\t Training Loss 0.0350\t Accuracy 0.9611\n",
      "Epoch [16][40]\t Batch [400][550]\t Training Loss 0.0351\t Accuracy 0.9610\n",
      "Epoch [16][40]\t Batch [450][550]\t Training Loss 0.0351\t Accuracy 0.9611\n",
      "Epoch [16][40]\t Batch [500][550]\t Training Loss 0.0352\t Accuracy 0.9604\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0352\t Average training accuracy 0.9606\n",
      "Epoch [16]\t Average validation loss 0.0343\t Average validation accuracy 0.9666\n",
      "\n",
      "Epoch [17][40]\t Batch [0][550]\t Training Loss 0.0338\t Accuracy 0.9700\n",
      "Epoch [17][40]\t Batch [50][550]\t Training Loss 0.0341\t Accuracy 0.9635\n",
      "Epoch [17][40]\t Batch [100][550]\t Training Loss 0.0344\t Accuracy 0.9617\n",
      "Epoch [17][40]\t Batch [150][550]\t Training Loss 0.0348\t Accuracy 0.9612\n",
      "Epoch [17][40]\t Batch [200][550]\t Training Loss 0.0347\t Accuracy 0.9617\n",
      "Epoch [17][40]\t Batch [250][550]\t Training Loss 0.0346\t Accuracy 0.9622\n",
      "Epoch [17][40]\t Batch [300][550]\t Training Loss 0.0347\t Accuracy 0.9618\n",
      "Epoch [17][40]\t Batch [350][550]\t Training Loss 0.0347\t Accuracy 0.9620\n",
      "Epoch [17][40]\t Batch [400][550]\t Training Loss 0.0347\t Accuracy 0.9619\n",
      "Epoch [17][40]\t Batch [450][550]\t Training Loss 0.0348\t Accuracy 0.9620\n",
      "Epoch [17][40]\t Batch [500][550]\t Training Loss 0.0349\t Accuracy 0.9614\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0348\t Average training accuracy 0.9615\n",
      "Epoch [17]\t Average validation loss 0.0341\t Average validation accuracy 0.9676\n",
      "\n",
      "Epoch [18][40]\t Batch [0][550]\t Training Loss 0.0335\t Accuracy 0.9700\n",
      "Epoch [18][40]\t Batch [50][550]\t Training Loss 0.0338\t Accuracy 0.9641\n",
      "Epoch [18][40]\t Batch [100][550]\t Training Loss 0.0341\t Accuracy 0.9627\n",
      "Epoch [18][40]\t Batch [150][550]\t Training Loss 0.0344\t Accuracy 0.9625\n",
      "Epoch [18][40]\t Batch [200][550]\t Training Loss 0.0344\t Accuracy 0.9629\n",
      "Epoch [18][40]\t Batch [250][550]\t Training Loss 0.0343\t Accuracy 0.9633\n",
      "Epoch [18][40]\t Batch [300][550]\t Training Loss 0.0344\t Accuracy 0.9629\n",
      "Epoch [18][40]\t Batch [350][550]\t Training Loss 0.0344\t Accuracy 0.9632\n",
      "Epoch [18][40]\t Batch [400][550]\t Training Loss 0.0344\t Accuracy 0.9632\n",
      "Epoch [18][40]\t Batch [450][550]\t Training Loss 0.0344\t Accuracy 0.9632\n",
      "Epoch [18][40]\t Batch [500][550]\t Training Loss 0.0346\t Accuracy 0.9626\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0345\t Average training accuracy 0.9628\n",
      "Epoch [18]\t Average validation loss 0.0339\t Average validation accuracy 0.9684\n",
      "\n",
      "Epoch [19][40]\t Batch [0][550]\t Training Loss 0.0331\t Accuracy 0.9700\n",
      "Epoch [19][40]\t Batch [50][550]\t Training Loss 0.0335\t Accuracy 0.9653\n",
      "Epoch [19][40]\t Batch [100][550]\t Training Loss 0.0338\t Accuracy 0.9635\n",
      "Epoch [19][40]\t Batch [150][550]\t Training Loss 0.0341\t Accuracy 0.9633\n",
      "Epoch [19][40]\t Batch [200][550]\t Training Loss 0.0341\t Accuracy 0.9637\n",
      "Epoch [19][40]\t Batch [250][550]\t Training Loss 0.0340\t Accuracy 0.9643\n",
      "Epoch [19][40]\t Batch [300][550]\t Training Loss 0.0341\t Accuracy 0.9639\n",
      "Epoch [19][40]\t Batch [350][550]\t Training Loss 0.0341\t Accuracy 0.9642\n",
      "Epoch [19][40]\t Batch [400][550]\t Training Loss 0.0341\t Accuracy 0.9642\n",
      "Epoch [19][40]\t Batch [450][550]\t Training Loss 0.0342\t Accuracy 0.9643\n",
      "Epoch [19][40]\t Batch [500][550]\t Training Loss 0.0343\t Accuracy 0.9637\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0342\t Average training accuracy 0.9638\n",
      "Epoch [19]\t Average validation loss 0.0337\t Average validation accuracy 0.9684\n",
      "\n",
      "Epoch [20][40]\t Batch [0][550]\t Training Loss 0.0327\t Accuracy 0.9700\n",
      "Epoch [20][40]\t Batch [50][550]\t Training Loss 0.0332\t Accuracy 0.9651\n",
      "Epoch [20][40]\t Batch [100][550]\t Training Loss 0.0336\t Accuracy 0.9643\n",
      "Epoch [20][40]\t Batch [150][550]\t Training Loss 0.0339\t Accuracy 0.9639\n",
      "Epoch [20][40]\t Batch [200][550]\t Training Loss 0.0338\t Accuracy 0.9644\n",
      "Epoch [20][40]\t Batch [250][550]\t Training Loss 0.0337\t Accuracy 0.9649\n",
      "Epoch [20][40]\t Batch [300][550]\t Training Loss 0.0338\t Accuracy 0.9646\n",
      "Epoch [20][40]\t Batch [350][550]\t Training Loss 0.0338\t Accuracy 0.9649\n",
      "Epoch [20][40]\t Batch [400][550]\t Training Loss 0.0339\t Accuracy 0.9649\n",
      "Epoch [20][40]\t Batch [450][550]\t Training Loss 0.0339\t Accuracy 0.9649\n",
      "Epoch [20][40]\t Batch [500][550]\t Training Loss 0.0340\t Accuracy 0.9643\n",
      "\n",
      "Epoch [20]\t Average training loss 0.0340\t Average training accuracy 0.9645\n",
      "Epoch [20]\t Average validation loss 0.0335\t Average validation accuracy 0.9686\n",
      "\n",
      "Epoch [21][40]\t Batch [0][550]\t Training Loss 0.0324\t Accuracy 0.9700\n",
      "Epoch [21][40]\t Batch [50][550]\t Training Loss 0.0330\t Accuracy 0.9653\n",
      "Epoch [21][40]\t Batch [100][550]\t Training Loss 0.0333\t Accuracy 0.9650\n",
      "Epoch [21][40]\t Batch [150][550]\t Training Loss 0.0336\t Accuracy 0.9646\n",
      "Epoch [21][40]\t Batch [200][550]\t Training Loss 0.0336\t Accuracy 0.9651\n",
      "Epoch [21][40]\t Batch [250][550]\t Training Loss 0.0334\t Accuracy 0.9657\n",
      "Epoch [21][40]\t Batch [300][550]\t Training Loss 0.0336\t Accuracy 0.9654\n",
      "Epoch [21][40]\t Batch [350][550]\t Training Loss 0.0336\t Accuracy 0.9657\n",
      "Epoch [21][40]\t Batch [400][550]\t Training Loss 0.0336\t Accuracy 0.9657\n",
      "Epoch [21][40]\t Batch [450][550]\t Training Loss 0.0336\t Accuracy 0.9657\n",
      "Epoch [21][40]\t Batch [500][550]\t Training Loss 0.0337\t Accuracy 0.9652\n",
      "\n",
      "Epoch [21]\t Average training loss 0.0337\t Average training accuracy 0.9653\n",
      "Epoch [21]\t Average validation loss 0.0333\t Average validation accuracy 0.9686\n",
      "\n",
      "Epoch [22][40]\t Batch [0][550]\t Training Loss 0.0321\t Accuracy 0.9700\n",
      "Epoch [22][40]\t Batch [50][550]\t Training Loss 0.0327\t Accuracy 0.9661\n",
      "Epoch [22][40]\t Batch [100][550]\t Training Loss 0.0331\t Accuracy 0.9657\n",
      "Epoch [22][40]\t Batch [150][550]\t Training Loss 0.0334\t Accuracy 0.9654\n",
      "Epoch [22][40]\t Batch [200][550]\t Training Loss 0.0333\t Accuracy 0.9658\n",
      "Epoch [22][40]\t Batch [250][550]\t Training Loss 0.0332\t Accuracy 0.9665\n",
      "Epoch [22][40]\t Batch [300][550]\t Training Loss 0.0333\t Accuracy 0.9662\n",
      "Epoch [22][40]\t Batch [350][550]\t Training Loss 0.0333\t Accuracy 0.9665\n",
      "Epoch [22][40]\t Batch [400][550]\t Training Loss 0.0333\t Accuracy 0.9665\n",
      "Epoch [22][40]\t Batch [450][550]\t Training Loss 0.0334\t Accuracy 0.9664\n",
      "Epoch [22][40]\t Batch [500][550]\t Training Loss 0.0335\t Accuracy 0.9659\n",
      "\n",
      "Epoch [22]\t Average training loss 0.0335\t Average training accuracy 0.9660\n",
      "Epoch [22]\t Average validation loss 0.0331\t Average validation accuracy 0.9692\n",
      "\n",
      "Epoch [23][40]\t Batch [0][550]\t Training Loss 0.0319\t Accuracy 0.9700\n",
      "Epoch [23][40]\t Batch [50][550]\t Training Loss 0.0325\t Accuracy 0.9665\n",
      "Epoch [23][40]\t Batch [100][550]\t Training Loss 0.0328\t Accuracy 0.9662\n",
      "Epoch [23][40]\t Batch [150][550]\t Training Loss 0.0331\t Accuracy 0.9661\n",
      "Epoch [23][40]\t Batch [200][550]\t Training Loss 0.0331\t Accuracy 0.9665\n",
      "Epoch [23][40]\t Batch [250][550]\t Training Loss 0.0329\t Accuracy 0.9673\n",
      "Epoch [23][40]\t Batch [300][550]\t Training Loss 0.0331\t Accuracy 0.9671\n",
      "Epoch [23][40]\t Batch [350][550]\t Training Loss 0.0331\t Accuracy 0.9674\n",
      "Epoch [23][40]\t Batch [400][550]\t Training Loss 0.0331\t Accuracy 0.9672\n",
      "Epoch [23][40]\t Batch [450][550]\t Training Loss 0.0331\t Accuracy 0.9671\n",
      "Epoch [23][40]\t Batch [500][550]\t Training Loss 0.0333\t Accuracy 0.9666\n",
      "\n",
      "Epoch [23]\t Average training loss 0.0332\t Average training accuracy 0.9667\n",
      "Epoch [23]\t Average validation loss 0.0330\t Average validation accuracy 0.9694\n",
      "\n",
      "Epoch [24][40]\t Batch [0][550]\t Training Loss 0.0316\t Accuracy 0.9700\n",
      "Epoch [24][40]\t Batch [50][550]\t Training Loss 0.0323\t Accuracy 0.9673\n",
      "Epoch [24][40]\t Batch [100][550]\t Training Loss 0.0326\t Accuracy 0.9666\n",
      "Epoch [24][40]\t Batch [150][550]\t Training Loss 0.0329\t Accuracy 0.9668\n",
      "Epoch [24][40]\t Batch [200][550]\t Training Loss 0.0329\t Accuracy 0.9672\n",
      "Epoch [24][40]\t Batch [250][550]\t Training Loss 0.0327\t Accuracy 0.9678\n",
      "Epoch [24][40]\t Batch [300][550]\t Training Loss 0.0329\t Accuracy 0.9676\n",
      "Epoch [24][40]\t Batch [350][550]\t Training Loss 0.0329\t Accuracy 0.9679\n",
      "Epoch [24][40]\t Batch [400][550]\t Training Loss 0.0329\t Accuracy 0.9678\n",
      "Epoch [24][40]\t Batch [450][550]\t Training Loss 0.0329\t Accuracy 0.9676\n",
      "Epoch [24][40]\t Batch [500][550]\t Training Loss 0.0330\t Accuracy 0.9671\n",
      "\n",
      "Epoch [24]\t Average training loss 0.0330\t Average training accuracy 0.9672\n",
      "Epoch [24]\t Average validation loss 0.0328\t Average validation accuracy 0.9700\n",
      "\n",
      "Epoch [25][40]\t Batch [0][550]\t Training Loss 0.0314\t Accuracy 0.9700\n",
      "Epoch [25][40]\t Batch [50][550]\t Training Loss 0.0321\t Accuracy 0.9680\n",
      "Epoch [25][40]\t Batch [100][550]\t Training Loss 0.0324\t Accuracy 0.9677\n",
      "Epoch [25][40]\t Batch [150][550]\t Training Loss 0.0327\t Accuracy 0.9678\n",
      "Epoch [25][40]\t Batch [200][550]\t Training Loss 0.0327\t Accuracy 0.9680\n",
      "Epoch [25][40]\t Batch [250][550]\t Training Loss 0.0325\t Accuracy 0.9684\n",
      "Epoch [25][40]\t Batch [300][550]\t Training Loss 0.0327\t Accuracy 0.9682\n",
      "Epoch [25][40]\t Batch [350][550]\t Training Loss 0.0326\t Accuracy 0.9684\n",
      "Epoch [25][40]\t Batch [400][550]\t Training Loss 0.0327\t Accuracy 0.9683\n",
      "Epoch [25][40]\t Batch [450][550]\t Training Loss 0.0327\t Accuracy 0.9682\n",
      "Epoch [25][40]\t Batch [500][550]\t Training Loss 0.0328\t Accuracy 0.9677\n",
      "\n",
      "Epoch [25]\t Average training loss 0.0328\t Average training accuracy 0.9678\n",
      "Epoch [25]\t Average validation loss 0.0327\t Average validation accuracy 0.9704\n",
      "\n",
      "Epoch [26][40]\t Batch [0][550]\t Training Loss 0.0312\t Accuracy 0.9700\n",
      "Epoch [26][40]\t Batch [50][550]\t Training Loss 0.0319\t Accuracy 0.9692\n",
      "Epoch [26][40]\t Batch [100][550]\t Training Loss 0.0322\t Accuracy 0.9681\n",
      "Epoch [26][40]\t Batch [150][550]\t Training Loss 0.0325\t Accuracy 0.9683\n",
      "Epoch [26][40]\t Batch [200][550]\t Training Loss 0.0325\t Accuracy 0.9686\n",
      "Epoch [26][40]\t Batch [250][550]\t Training Loss 0.0323\t Accuracy 0.9690\n",
      "Epoch [26][40]\t Batch [300][550]\t Training Loss 0.0325\t Accuracy 0.9687\n",
      "Epoch [26][40]\t Batch [350][550]\t Training Loss 0.0324\t Accuracy 0.9689\n",
      "Epoch [26][40]\t Batch [400][550]\t Training Loss 0.0325\t Accuracy 0.9688\n",
      "Epoch [26][40]\t Batch [450][550]\t Training Loss 0.0325\t Accuracy 0.9686\n",
      "Epoch [26][40]\t Batch [500][550]\t Training Loss 0.0326\t Accuracy 0.9682\n",
      "\n",
      "Epoch [26]\t Average training loss 0.0326\t Average training accuracy 0.9682\n",
      "Epoch [26]\t Average validation loss 0.0325\t Average validation accuracy 0.9698\n",
      "\n",
      "Epoch [27][40]\t Batch [0][550]\t Training Loss 0.0310\t Accuracy 0.9700\n",
      "Epoch [27][40]\t Batch [50][550]\t Training Loss 0.0317\t Accuracy 0.9692\n",
      "Epoch [27][40]\t Batch [100][550]\t Training Loss 0.0320\t Accuracy 0.9688\n",
      "Epoch [27][40]\t Batch [150][550]\t Training Loss 0.0323\t Accuracy 0.9691\n",
      "Epoch [27][40]\t Batch [200][550]\t Training Loss 0.0323\t Accuracy 0.9695\n",
      "Epoch [27][40]\t Batch [250][550]\t Training Loss 0.0321\t Accuracy 0.9698\n",
      "Epoch [27][40]\t Batch [300][550]\t Training Loss 0.0323\t Accuracy 0.9695\n",
      "Epoch [27][40]\t Batch [350][550]\t Training Loss 0.0322\t Accuracy 0.9695\n",
      "Epoch [27][40]\t Batch [400][550]\t Training Loss 0.0323\t Accuracy 0.9695\n",
      "Epoch [27][40]\t Batch [450][550]\t Training Loss 0.0323\t Accuracy 0.9693\n",
      "Epoch [27][40]\t Batch [500][550]\t Training Loss 0.0324\t Accuracy 0.9688\n",
      "\n",
      "Epoch [27]\t Average training loss 0.0324\t Average training accuracy 0.9688\n",
      "Epoch [27]\t Average validation loss 0.0324\t Average validation accuracy 0.9708\n",
      "\n",
      "Epoch [28][40]\t Batch [0][550]\t Training Loss 0.0308\t Accuracy 0.9700\n",
      "Epoch [28][40]\t Batch [50][550]\t Training Loss 0.0315\t Accuracy 0.9694\n",
      "Epoch [28][40]\t Batch [100][550]\t Training Loss 0.0318\t Accuracy 0.9694\n",
      "Epoch [28][40]\t Batch [150][550]\t Training Loss 0.0321\t Accuracy 0.9697\n",
      "Epoch [28][40]\t Batch [200][550]\t Training Loss 0.0321\t Accuracy 0.9699\n",
      "Epoch [28][40]\t Batch [250][550]\t Training Loss 0.0319\t Accuracy 0.9702\n",
      "Epoch [28][40]\t Batch [300][550]\t Training Loss 0.0321\t Accuracy 0.9700\n",
      "Epoch [28][40]\t Batch [350][550]\t Training Loss 0.0320\t Accuracy 0.9701\n",
      "Epoch [28][40]\t Batch [400][550]\t Training Loss 0.0321\t Accuracy 0.9700\n",
      "Epoch [28][40]\t Batch [450][550]\t Training Loss 0.0321\t Accuracy 0.9698\n",
      "Epoch [28][40]\t Batch [500][550]\t Training Loss 0.0322\t Accuracy 0.9693\n",
      "\n",
      "Epoch [28]\t Average training loss 0.0322\t Average training accuracy 0.9692\n",
      "Epoch [28]\t Average validation loss 0.0323\t Average validation accuracy 0.9710\n",
      "\n",
      "Epoch [29][40]\t Batch [0][550]\t Training Loss 0.0306\t Accuracy 0.9700\n",
      "Epoch [29][40]\t Batch [50][550]\t Training Loss 0.0313\t Accuracy 0.9692\n",
      "Epoch [29][40]\t Batch [100][550]\t Training Loss 0.0316\t Accuracy 0.9695\n",
      "Epoch [29][40]\t Batch [150][550]\t Training Loss 0.0319\t Accuracy 0.9699\n",
      "Epoch [29][40]\t Batch [200][550]\t Training Loss 0.0319\t Accuracy 0.9703\n",
      "Epoch [29][40]\t Batch [250][550]\t Training Loss 0.0317\t Accuracy 0.9706\n",
      "Epoch [29][40]\t Batch [300][550]\t Training Loss 0.0319\t Accuracy 0.9704\n",
      "Epoch [29][40]\t Batch [350][550]\t Training Loss 0.0319\t Accuracy 0.9704\n",
      "Epoch [29][40]\t Batch [400][550]\t Training Loss 0.0319\t Accuracy 0.9703\n",
      "Epoch [29][40]\t Batch [450][550]\t Training Loss 0.0320\t Accuracy 0.9701\n",
      "Epoch [29][40]\t Batch [500][550]\t Training Loss 0.0321\t Accuracy 0.9696\n",
      "\n",
      "Epoch [29]\t Average training loss 0.0320\t Average training accuracy 0.9697\n",
      "Epoch [29]\t Average validation loss 0.0322\t Average validation accuracy 0.9708\n",
      "\n",
      "Epoch [30][40]\t Batch [0][550]\t Training Loss 0.0304\t Accuracy 0.9700\n",
      "Epoch [30][40]\t Batch [50][550]\t Training Loss 0.0312\t Accuracy 0.9696\n",
      "Epoch [30][40]\t Batch [100][550]\t Training Loss 0.0315\t Accuracy 0.9698\n",
      "Epoch [30][40]\t Batch [150][550]\t Training Loss 0.0317\t Accuracy 0.9703\n",
      "Epoch [30][40]\t Batch [200][550]\t Training Loss 0.0317\t Accuracy 0.9708\n",
      "Epoch [30][40]\t Batch [250][550]\t Training Loss 0.0316\t Accuracy 0.9710\n",
      "Epoch [30][40]\t Batch [300][550]\t Training Loss 0.0317\t Accuracy 0.9709\n",
      "Epoch [30][40]\t Batch [350][550]\t Training Loss 0.0317\t Accuracy 0.9709\n",
      "Epoch [30][40]\t Batch [400][550]\t Training Loss 0.0317\t Accuracy 0.9708\n",
      "Epoch [30][40]\t Batch [450][550]\t Training Loss 0.0318\t Accuracy 0.9706\n",
      "Epoch [30][40]\t Batch [500][550]\t Training Loss 0.0319\t Accuracy 0.9701\n",
      "\n",
      "Epoch [30]\t Average training loss 0.0318\t Average training accuracy 0.9701\n",
      "Epoch [30]\t Average validation loss 0.0321\t Average validation accuracy 0.9716\n",
      "\n",
      "Epoch [31][40]\t Batch [0][550]\t Training Loss 0.0302\t Accuracy 0.9700\n",
      "Epoch [31][40]\t Batch [50][550]\t Training Loss 0.0310\t Accuracy 0.9702\n",
      "Epoch [31][40]\t Batch [100][550]\t Training Loss 0.0313\t Accuracy 0.9707\n",
      "Epoch [31][40]\t Batch [150][550]\t Training Loss 0.0316\t Accuracy 0.9710\n",
      "Epoch [31][40]\t Batch [200][550]\t Training Loss 0.0316\t Accuracy 0.9713\n",
      "Epoch [31][40]\t Batch [250][550]\t Training Loss 0.0314\t Accuracy 0.9717\n",
      "Epoch [31][40]\t Batch [300][550]\t Training Loss 0.0315\t Accuracy 0.9715\n",
      "Epoch [31][40]\t Batch [350][550]\t Training Loss 0.0315\t Accuracy 0.9715\n",
      "Epoch [31][40]\t Batch [400][550]\t Training Loss 0.0316\t Accuracy 0.9713\n",
      "Epoch [31][40]\t Batch [450][550]\t Training Loss 0.0316\t Accuracy 0.9710\n",
      "Epoch [31][40]\t Batch [500][550]\t Training Loss 0.0317\t Accuracy 0.9706\n",
      "\n",
      "Epoch [31]\t Average training loss 0.0317\t Average training accuracy 0.9706\n",
      "Epoch [31]\t Average validation loss 0.0320\t Average validation accuracy 0.9722\n",
      "\n",
      "Epoch [32][40]\t Batch [0][550]\t Training Loss 0.0301\t Accuracy 0.9700\n",
      "Epoch [32][40]\t Batch [50][550]\t Training Loss 0.0309\t Accuracy 0.9708\n",
      "Epoch [32][40]\t Batch [100][550]\t Training Loss 0.0312\t Accuracy 0.9709\n",
      "Epoch [32][40]\t Batch [150][550]\t Training Loss 0.0314\t Accuracy 0.9713\n",
      "Epoch [32][40]\t Batch [200][550]\t Training Loss 0.0314\t Accuracy 0.9716\n",
      "Epoch [32][40]\t Batch [250][550]\t Training Loss 0.0312\t Accuracy 0.9720\n",
      "Epoch [32][40]\t Batch [300][550]\t Training Loss 0.0314\t Accuracy 0.9719\n",
      "Epoch [32][40]\t Batch [350][550]\t Training Loss 0.0314\t Accuracy 0.9720\n",
      "Epoch [32][40]\t Batch [400][550]\t Training Loss 0.0314\t Accuracy 0.9717\n",
      "Epoch [32][40]\t Batch [450][550]\t Training Loss 0.0314\t Accuracy 0.9714\n",
      "Epoch [32][40]\t Batch [500][550]\t Training Loss 0.0316\t Accuracy 0.9710\n",
      "\n",
      "Epoch [32]\t Average training loss 0.0315\t Average training accuracy 0.9711\n",
      "Epoch [32]\t Average validation loss 0.0319\t Average validation accuracy 0.9718\n",
      "\n",
      "Epoch [33][40]\t Batch [0][550]\t Training Loss 0.0300\t Accuracy 0.9700\n",
      "Epoch [33][40]\t Batch [50][550]\t Training Loss 0.0307\t Accuracy 0.9712\n",
      "Epoch [33][40]\t Batch [100][550]\t Training Loss 0.0310\t Accuracy 0.9714\n",
      "Epoch [33][40]\t Batch [150][550]\t Training Loss 0.0313\t Accuracy 0.9719\n",
      "Epoch [33][40]\t Batch [200][550]\t Training Loss 0.0312\t Accuracy 0.9722\n",
      "Epoch [33][40]\t Batch [250][550]\t Training Loss 0.0311\t Accuracy 0.9725\n",
      "Epoch [33][40]\t Batch [300][550]\t Training Loss 0.0312\t Accuracy 0.9724\n",
      "Epoch [33][40]\t Batch [350][550]\t Training Loss 0.0312\t Accuracy 0.9724\n",
      "Epoch [33][40]\t Batch [400][550]\t Training Loss 0.0313\t Accuracy 0.9721\n",
      "Epoch [33][40]\t Batch [450][550]\t Training Loss 0.0313\t Accuracy 0.9719\n",
      "Epoch [33][40]\t Batch [500][550]\t Training Loss 0.0314\t Accuracy 0.9715\n",
      "\n",
      "Epoch [33]\t Average training loss 0.0314\t Average training accuracy 0.9715\n",
      "Epoch [33]\t Average validation loss 0.0318\t Average validation accuracy 0.9722\n",
      "\n",
      "Epoch [34][40]\t Batch [0][550]\t Training Loss 0.0299\t Accuracy 0.9700\n",
      "Epoch [34][40]\t Batch [50][550]\t Training Loss 0.0306\t Accuracy 0.9714\n",
      "Epoch [34][40]\t Batch [100][550]\t Training Loss 0.0309\t Accuracy 0.9721\n",
      "Epoch [34][40]\t Batch [150][550]\t Training Loss 0.0311\t Accuracy 0.9724\n",
      "Epoch [34][40]\t Batch [200][550]\t Training Loss 0.0311\t Accuracy 0.9726\n",
      "Epoch [34][40]\t Batch [250][550]\t Training Loss 0.0309\t Accuracy 0.9729\n",
      "Epoch [34][40]\t Batch [300][550]\t Training Loss 0.0311\t Accuracy 0.9728\n",
      "Epoch [34][40]\t Batch [350][550]\t Training Loss 0.0310\t Accuracy 0.9728\n",
      "Epoch [34][40]\t Batch [400][550]\t Training Loss 0.0311\t Accuracy 0.9724\n",
      "Epoch [34][40]\t Batch [450][550]\t Training Loss 0.0311\t Accuracy 0.9722\n",
      "Epoch [34][40]\t Batch [500][550]\t Training Loss 0.0313\t Accuracy 0.9718\n",
      "\n",
      "Epoch [34]\t Average training loss 0.0312\t Average training accuracy 0.9717\n",
      "Epoch [34]\t Average validation loss 0.0317\t Average validation accuracy 0.9718\n",
      "\n",
      "Epoch [35][40]\t Batch [0][550]\t Training Loss 0.0298\t Accuracy 0.9700\n",
      "Epoch [35][40]\t Batch [50][550]\t Training Loss 0.0304\t Accuracy 0.9716\n",
      "Epoch [35][40]\t Batch [100][550]\t Training Loss 0.0307\t Accuracy 0.9725\n",
      "Epoch [35][40]\t Batch [150][550]\t Training Loss 0.0310\t Accuracy 0.9728\n",
      "Epoch [35][40]\t Batch [200][550]\t Training Loss 0.0310\t Accuracy 0.9729\n",
      "Epoch [35][40]\t Batch [250][550]\t Training Loss 0.0308\t Accuracy 0.9734\n",
      "Epoch [35][40]\t Batch [300][550]\t Training Loss 0.0309\t Accuracy 0.9732\n",
      "Epoch [35][40]\t Batch [350][550]\t Training Loss 0.0309\t Accuracy 0.9732\n",
      "Epoch [35][40]\t Batch [400][550]\t Training Loss 0.0310\t Accuracy 0.9729\n",
      "Epoch [35][40]\t Batch [450][550]\t Training Loss 0.0310\t Accuracy 0.9726\n",
      "Epoch [35][40]\t Batch [500][550]\t Training Loss 0.0311\t Accuracy 0.9722\n",
      "\n",
      "Epoch [35]\t Average training loss 0.0311\t Average training accuracy 0.9721\n",
      "Epoch [35]\t Average validation loss 0.0316\t Average validation accuracy 0.9720\n",
      "\n",
      "Epoch [36][40]\t Batch [0][550]\t Training Loss 0.0297\t Accuracy 0.9700\n",
      "Epoch [36][40]\t Batch [50][550]\t Training Loss 0.0303\t Accuracy 0.9718\n",
      "Epoch [36][40]\t Batch [100][550]\t Training Loss 0.0306\t Accuracy 0.9728\n",
      "Epoch [36][40]\t Batch [150][550]\t Training Loss 0.0308\t Accuracy 0.9730\n",
      "Epoch [36][40]\t Batch [200][550]\t Training Loss 0.0308\t Accuracy 0.9732\n",
      "Epoch [36][40]\t Batch [250][550]\t Training Loss 0.0306\t Accuracy 0.9736\n",
      "Epoch [36][40]\t Batch [300][550]\t Training Loss 0.0308\t Accuracy 0.9734\n",
      "Epoch [36][40]\t Batch [350][550]\t Training Loss 0.0308\t Accuracy 0.9735\n",
      "Epoch [36][40]\t Batch [400][550]\t Training Loss 0.0308\t Accuracy 0.9732\n",
      "Epoch [36][40]\t Batch [450][550]\t Training Loss 0.0309\t Accuracy 0.9729\n",
      "Epoch [36][40]\t Batch [500][550]\t Training Loss 0.0310\t Accuracy 0.9725\n",
      "\n",
      "Epoch [36]\t Average training loss 0.0309\t Average training accuracy 0.9724\n",
      "Epoch [36]\t Average validation loss 0.0315\t Average validation accuracy 0.9718\n",
      "\n",
      "Epoch [37][40]\t Batch [0][550]\t Training Loss 0.0296\t Accuracy 0.9700\n",
      "Epoch [37][40]\t Batch [50][550]\t Training Loss 0.0302\t Accuracy 0.9727\n",
      "Epoch [37][40]\t Batch [100][550]\t Training Loss 0.0305\t Accuracy 0.9735\n",
      "Epoch [37][40]\t Batch [150][550]\t Training Loss 0.0307\t Accuracy 0.9736\n",
      "Epoch [37][40]\t Batch [200][550]\t Training Loss 0.0307\t Accuracy 0.9736\n",
      "Epoch [37][40]\t Batch [250][550]\t Training Loss 0.0305\t Accuracy 0.9741\n",
      "Epoch [37][40]\t Batch [300][550]\t Training Loss 0.0306\t Accuracy 0.9739\n",
      "Epoch [37][40]\t Batch [350][550]\t Training Loss 0.0306\t Accuracy 0.9740\n",
      "Epoch [37][40]\t Batch [400][550]\t Training Loss 0.0307\t Accuracy 0.9737\n",
      "Epoch [37][40]\t Batch [450][550]\t Training Loss 0.0307\t Accuracy 0.9734\n",
      "Epoch [37][40]\t Batch [500][550]\t Training Loss 0.0308\t Accuracy 0.9729\n",
      "\n",
      "Epoch [37]\t Average training loss 0.0308\t Average training accuracy 0.9729\n",
      "Epoch [37]\t Average validation loss 0.0314\t Average validation accuracy 0.9722\n",
      "\n",
      "Epoch [38][40]\t Batch [0][550]\t Training Loss 0.0295\t Accuracy 0.9700\n",
      "Epoch [38][40]\t Batch [50][550]\t Training Loss 0.0301\t Accuracy 0.9725\n",
      "Epoch [38][40]\t Batch [100][550]\t Training Loss 0.0303\t Accuracy 0.9735\n",
      "Epoch [38][40]\t Batch [150][550]\t Training Loss 0.0305\t Accuracy 0.9736\n",
      "Epoch [38][40]\t Batch [200][550]\t Training Loss 0.0306\t Accuracy 0.9737\n",
      "Epoch [38][40]\t Batch [250][550]\t Training Loss 0.0304\t Accuracy 0.9743\n",
      "Epoch [38][40]\t Batch [300][550]\t Training Loss 0.0305\t Accuracy 0.9741\n",
      "Epoch [38][40]\t Batch [350][550]\t Training Loss 0.0305\t Accuracy 0.9742\n",
      "Epoch [38][40]\t Batch [400][550]\t Training Loss 0.0306\t Accuracy 0.9740\n",
      "Epoch [38][40]\t Batch [450][550]\t Training Loss 0.0306\t Accuracy 0.9738\n",
      "Epoch [38][40]\t Batch [500][550]\t Training Loss 0.0307\t Accuracy 0.9733\n",
      "\n",
      "Epoch [38]\t Average training loss 0.0307\t Average training accuracy 0.9732\n",
      "Epoch [38]\t Average validation loss 0.0313\t Average validation accuracy 0.9716\n",
      "\n",
      "Epoch [39][40]\t Batch [0][550]\t Training Loss 0.0295\t Accuracy 0.9700\n",
      "Epoch [39][40]\t Batch [50][550]\t Training Loss 0.0300\t Accuracy 0.9731\n",
      "Epoch [39][40]\t Batch [100][550]\t Training Loss 0.0302\t Accuracy 0.9742\n",
      "Epoch [39][40]\t Batch [150][550]\t Training Loss 0.0304\t Accuracy 0.9743\n",
      "Epoch [39][40]\t Batch [200][550]\t Training Loss 0.0304\t Accuracy 0.9743\n",
      "Epoch [39][40]\t Batch [250][550]\t Training Loss 0.0303\t Accuracy 0.9748\n",
      "Epoch [39][40]\t Batch [300][550]\t Training Loss 0.0304\t Accuracy 0.9746\n",
      "Epoch [39][40]\t Batch [350][550]\t Training Loss 0.0304\t Accuracy 0.9747\n",
      "Epoch [39][40]\t Batch [400][550]\t Training Loss 0.0304\t Accuracy 0.9745\n",
      "Epoch [39][40]\t Batch [450][550]\t Training Loss 0.0305\t Accuracy 0.9743\n",
      "Epoch [39][40]\t Batch [500][550]\t Training Loss 0.0306\t Accuracy 0.9737\n",
      "\n",
      "Epoch [39]\t Average training loss 0.0305\t Average training accuracy 0.9736\n",
      "Epoch [39]\t Average validation loss 0.0313\t Average validation accuracy 0.9720\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9666.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLP with Softmax Cross-Entropy Loss\n",
    "In part-2, you need to train a MLP with **Softmax Cross-Entropy Loss**.  \n",
    "**Sigmoid Activation Function** and **ReLU Activation Function** will be used respectively again.\n",
    "### TODO\n",
    "Before executing the following code, you should complete **criterion/softmax_cross_entropy_loss.py**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 40\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 5e-3\n",
    "\n",
    "# (0.31 : 0.1) (0.001 : 0.87) (0.001 : 0.88)\n",
    "weight_decay = 1e-3\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer\n",
    "from layers import FCLayer, SigmoidLayer, ReLULayer\n",
    "from optimizer import SGD\n",
    "\n",
    "criterion = SoftmaxCrossEntropyLossLayer()\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MLP with Softmax Cross-Entropy Loss and Sigmoid Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using Sigmoid activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoidMLP = Network()\n",
    "# Build MLP with FCLayer and SigmoidLayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "sigmoidMLP.add(FCLayer(784, 128))\n",
    "sigmoidMLP.add(SigmoidLayer())\n",
    "sigmoidMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][40]\t Batch [0][550]\t Training Loss 2.9016\t Accuracy 0.1100\n",
      "Epoch [0][40]\t Batch [50][550]\t Training Loss 2.5049\t Accuracy 0.1184\n",
      "Epoch [0][40]\t Batch [100][550]\t Training Loss 2.3759\t Accuracy 0.1455\n",
      "Epoch [0][40]\t Batch [150][550]\t Training Loss 2.3081\t Accuracy 0.1877\n",
      "Epoch [0][40]\t Batch [200][550]\t Training Loss 2.2588\t Accuracy 0.2285\n",
      "Epoch [0][40]\t Batch [250][550]\t Training Loss 2.2194\t Accuracy 0.2641\n",
      "Epoch [0][40]\t Batch [300][550]\t Training Loss 2.1828\t Accuracy 0.2989\n",
      "Epoch [0][40]\t Batch [350][550]\t Training Loss 2.1527\t Accuracy 0.3254\n",
      "Epoch [0][40]\t Batch [400][550]\t Training Loss 2.1219\t Accuracy 0.3522\n",
      "Epoch [0][40]\t Batch [450][550]\t Training Loss 2.0943\t Accuracy 0.3757\n",
      "Epoch [0][40]\t Batch [500][550]\t Training Loss 2.0670\t Accuracy 0.3972\n",
      "\n",
      "Epoch [0]\t Average training loss 2.0413\t Average training accuracy 0.4175\n",
      "Epoch [0]\t Average validation loss 1.7310\t Average validation accuracy 0.6664\n",
      "\n",
      "Epoch [1][40]\t Batch [0][550]\t Training Loss 1.7420\t Accuracy 0.6300\n",
      "Epoch [1][40]\t Batch [50][550]\t Training Loss 1.7271\t Accuracy 0.6612\n",
      "Epoch [1][40]\t Batch [100][550]\t Training Loss 1.7080\t Accuracy 0.6630\n",
      "Epoch [1][40]\t Batch [150][550]\t Training Loss 1.6921\t Accuracy 0.6626\n",
      "Epoch [1][40]\t Batch [200][550]\t Training Loss 1.6757\t Accuracy 0.6684\n",
      "Epoch [1][40]\t Batch [250][550]\t Training Loss 1.6581\t Accuracy 0.6752\n",
      "Epoch [1][40]\t Batch [300][550]\t Training Loss 1.6393\t Accuracy 0.6839\n",
      "Epoch [1][40]\t Batch [350][550]\t Training Loss 1.6270\t Accuracy 0.6895\n",
      "Epoch [1][40]\t Batch [400][550]\t Training Loss 1.6098\t Accuracy 0.6951\n",
      "Epoch [1][40]\t Batch [450][550]\t Training Loss 1.5953\t Accuracy 0.7001\n",
      "Epoch [1][40]\t Batch [500][550]\t Training Loss 1.5797\t Accuracy 0.7043\n",
      "\n",
      "Epoch [1]\t Average training loss 1.5643\t Average training accuracy 0.7089\n",
      "Epoch [1]\t Average validation loss 1.3502\t Average validation accuracy 0.8034\n",
      "\n",
      "Epoch [2][40]\t Batch [0][550]\t Training Loss 1.3621\t Accuracy 0.7600\n",
      "Epoch [2][40]\t Batch [50][550]\t Training Loss 1.3682\t Accuracy 0.7710\n",
      "Epoch [2][40]\t Batch [100][550]\t Training Loss 1.3574\t Accuracy 0.7716\n",
      "Epoch [2][40]\t Batch [150][550]\t Training Loss 1.3506\t Accuracy 0.7665\n",
      "Epoch [2][40]\t Batch [200][550]\t Training Loss 1.3417\t Accuracy 0.7691\n",
      "Epoch [2][40]\t Batch [250][550]\t Training Loss 1.3302\t Accuracy 0.7710\n",
      "Epoch [2][40]\t Batch [300][550]\t Training Loss 1.3178\t Accuracy 0.7746\n",
      "Epoch [2][40]\t Batch [350][550]\t Training Loss 1.3126\t Accuracy 0.7759\n",
      "Epoch [2][40]\t Batch [400][550]\t Training Loss 1.3013\t Accuracy 0.7784\n",
      "Epoch [2][40]\t Batch [450][550]\t Training Loss 1.2925\t Accuracy 0.7807\n",
      "Epoch [2][40]\t Batch [500][550]\t Training Loss 1.2825\t Accuracy 0.7820\n",
      "\n",
      "Epoch [2]\t Average training loss 1.2722\t Average training accuracy 0.7840\n",
      "Epoch [2]\t Average validation loss 1.1034\t Average validation accuracy 0.8466\n",
      "\n",
      "Epoch [3][40]\t Batch [0][550]\t Training Loss 1.1212\t Accuracy 0.7900\n",
      "Epoch [3][40]\t Batch [50][550]\t Training Loss 1.1359\t Accuracy 0.8084\n",
      "Epoch [3][40]\t Batch [100][550]\t Training Loss 1.1305\t Accuracy 0.8084\n",
      "Epoch [3][40]\t Batch [150][550]\t Training Loss 1.1292\t Accuracy 0.8041\n",
      "Epoch [3][40]\t Batch [200][550]\t Training Loss 1.1245\t Accuracy 0.8061\n",
      "Epoch [3][40]\t Batch [250][550]\t Training Loss 1.1166\t Accuracy 0.8059\n",
      "Epoch [3][40]\t Batch [300][550]\t Training Loss 1.1081\t Accuracy 0.8084\n",
      "Epoch [3][40]\t Batch [350][550]\t Training Loss 1.1069\t Accuracy 0.8083\n",
      "Epoch [3][40]\t Batch [400][550]\t Training Loss 1.0990\t Accuracy 0.8102\n",
      "Epoch [3][40]\t Batch [450][550]\t Training Loss 1.0937\t Accuracy 0.8111\n",
      "Epoch [3][40]\t Batch [500][550]\t Training Loss 1.0871\t Accuracy 0.8116\n",
      "\n",
      "Epoch [3]\t Average training loss 1.0798\t Average training accuracy 0.8125\n",
      "Epoch [3]\t Average validation loss 0.9378\t Average validation accuracy 0.8654\n",
      "\n",
      "Epoch [4][40]\t Batch [0][550]\t Training Loss 0.9625\t Accuracy 0.8400\n",
      "Epoch [4][40]\t Batch [50][550]\t Training Loss 0.9796\t Accuracy 0.8275\n",
      "Epoch [4][40]\t Batch [100][550]\t Training Loss 0.9778\t Accuracy 0.8276\n",
      "Epoch [4][40]\t Batch [150][550]\t Training Loss 0.9800\t Accuracy 0.8226\n",
      "Epoch [4][40]\t Batch [200][550]\t Training Loss 0.9775\t Accuracy 0.8244\n",
      "Epoch [4][40]\t Batch [250][550]\t Training Loss 0.9716\t Accuracy 0.8242\n",
      "Epoch [4][40]\t Batch [300][550]\t Training Loss 0.9657\t Accuracy 0.8260\n",
      "Epoch [4][40]\t Batch [350][550]\t Training Loss 0.9667\t Accuracy 0.8256\n",
      "Epoch [4][40]\t Batch [400][550]\t Training Loss 0.9610\t Accuracy 0.8271\n",
      "Epoch [4][40]\t Batch [450][550]\t Training Loss 0.9577\t Accuracy 0.8276\n",
      "Epoch [4][40]\t Batch [500][550]\t Training Loss 0.9532\t Accuracy 0.8276\n",
      "\n",
      "Epoch [4]\t Average training loss 0.9478\t Average training accuracy 0.8281\n",
      "Epoch [4]\t Average validation loss 0.8221\t Average validation accuracy 0.8750\n",
      "\n",
      "Epoch [5][40]\t Batch [0][550]\t Training Loss 0.8529\t Accuracy 0.8400\n",
      "Epoch [5][40]\t Batch [50][550]\t Training Loss 0.8699\t Accuracy 0.8400\n",
      "Epoch [5][40]\t Batch [100][550]\t Training Loss 0.8703\t Accuracy 0.8400\n",
      "Epoch [5][40]\t Batch [150][550]\t Training Loss 0.8748\t Accuracy 0.8350\n",
      "Epoch [5][40]\t Batch [200][550]\t Training Loss 0.8735\t Accuracy 0.8364\n",
      "Epoch [5][40]\t Batch [250][550]\t Training Loss 0.8690\t Accuracy 0.8362\n",
      "Epoch [5][40]\t Batch [300][550]\t Training Loss 0.8647\t Accuracy 0.8374\n",
      "Epoch [5][40]\t Batch [350][550]\t Training Loss 0.8670\t Accuracy 0.8364\n",
      "Epoch [5][40]\t Batch [400][550]\t Training Loss 0.8627\t Accuracy 0.8375\n",
      "Epoch [5][40]\t Batch [450][550]\t Training Loss 0.8607\t Accuracy 0.8376\n",
      "Epoch [5][40]\t Batch [500][550]\t Training Loss 0.8575\t Accuracy 0.8374\n",
      "\n",
      "Epoch [5]\t Average training loss 0.8533\t Average training accuracy 0.8378\n",
      "Epoch [5]\t Average validation loss 0.7378\t Average validation accuracy 0.8798\n",
      "\n",
      "Epoch [6][40]\t Batch [0][550]\t Training Loss 0.7739\t Accuracy 0.8600\n",
      "Epoch [6][40]\t Batch [50][550]\t Training Loss 0.7895\t Accuracy 0.8484\n",
      "Epoch [6][40]\t Batch [100][550]\t Training Loss 0.7915\t Accuracy 0.8481\n",
      "Epoch [6][40]\t Batch [150][550]\t Training Loss 0.7975\t Accuracy 0.8434\n",
      "Epoch [6][40]\t Batch [200][550]\t Training Loss 0.7969\t Accuracy 0.8446\n",
      "Epoch [6][40]\t Batch [250][550]\t Training Loss 0.7933\t Accuracy 0.8447\n",
      "Epoch [6][40]\t Batch [300][550]\t Training Loss 0.7900\t Accuracy 0.8455\n",
      "Epoch [6][40]\t Batch [350][550]\t Training Loss 0.7933\t Accuracy 0.8445\n",
      "Epoch [6][40]\t Batch [400][550]\t Training Loss 0.7898\t Accuracy 0.8454\n",
      "Epoch [6][40]\t Batch [450][550]\t Training Loss 0.7886\t Accuracy 0.8453\n",
      "Epoch [6][40]\t Batch [500][550]\t Training Loss 0.7864\t Accuracy 0.8449\n",
      "\n",
      "Epoch [6]\t Average training loss 0.7830\t Average training accuracy 0.8452\n",
      "Epoch [6]\t Average validation loss 0.6742\t Average validation accuracy 0.8848\n",
      "\n",
      "Epoch [7][40]\t Batch [0][550]\t Training Loss 0.7145\t Accuracy 0.8900\n",
      "Epoch [7][40]\t Batch [50][550]\t Training Loss 0.7284\t Accuracy 0.8549\n",
      "Epoch [7][40]\t Batch [100][550]\t Training Loss 0.7315\t Accuracy 0.8543\n",
      "Epoch [7][40]\t Batch [150][550]\t Training Loss 0.7387\t Accuracy 0.8494\n",
      "Epoch [7][40]\t Batch [200][550]\t Training Loss 0.7385\t Accuracy 0.8502\n",
      "Epoch [7][40]\t Batch [250][550]\t Training Loss 0.7355\t Accuracy 0.8506\n",
      "Epoch [7][40]\t Batch [300][550]\t Training Loss 0.7329\t Accuracy 0.8515\n",
      "Epoch [7][40]\t Batch [350][550]\t Training Loss 0.7367\t Accuracy 0.8505\n",
      "Epoch [7][40]\t Batch [400][550]\t Training Loss 0.7338\t Accuracy 0.8513\n",
      "Epoch [7][40]\t Batch [450][550]\t Training Loss 0.7332\t Accuracy 0.8509\n",
      "Epoch [7][40]\t Batch [500][550]\t Training Loss 0.7316\t Accuracy 0.8503\n",
      "\n",
      "Epoch [7]\t Average training loss 0.7288\t Average training accuracy 0.8507\n",
      "Epoch [7]\t Average validation loss 0.6248\t Average validation accuracy 0.8886\n",
      "\n",
      "Epoch [8][40]\t Batch [0][550]\t Training Loss 0.6683\t Accuracy 0.8900\n",
      "Epoch [8][40]\t Batch [50][550]\t Training Loss 0.6805\t Accuracy 0.8602\n",
      "Epoch [8][40]\t Batch [100][550]\t Training Loss 0.6845\t Accuracy 0.8600\n",
      "Epoch [8][40]\t Batch [150][550]\t Training Loss 0.6925\t Accuracy 0.8551\n",
      "Epoch [8][40]\t Batch [200][550]\t Training Loss 0.6925\t Accuracy 0.8555\n",
      "Epoch [8][40]\t Batch [250][550]\t Training Loss 0.6899\t Accuracy 0.8557\n",
      "Epoch [8][40]\t Batch [300][550]\t Training Loss 0.6880\t Accuracy 0.8566\n",
      "Epoch [8][40]\t Batch [350][550]\t Training Loss 0.6921\t Accuracy 0.8556\n",
      "Epoch [8][40]\t Batch [400][550]\t Training Loss 0.6896\t Accuracy 0.8563\n",
      "Epoch [8][40]\t Batch [450][550]\t Training Loss 0.6894\t Accuracy 0.8557\n",
      "Epoch [8][40]\t Batch [500][550]\t Training Loss 0.6883\t Accuracy 0.8549\n",
      "\n",
      "Epoch [8]\t Average training loss 0.6860\t Average training accuracy 0.8553\n",
      "Epoch [8]\t Average validation loss 0.5853\t Average validation accuracy 0.8928\n",
      "\n",
      "Epoch [9][40]\t Batch [0][550]\t Training Loss 0.6314\t Accuracy 0.9000\n",
      "Epoch [9][40]\t Batch [50][550]\t Training Loss 0.6420\t Accuracy 0.8653\n",
      "Epoch [9][40]\t Batch [100][550]\t Training Loss 0.6467\t Accuracy 0.8650\n",
      "Epoch [9][40]\t Batch [150][550]\t Training Loss 0.6554\t Accuracy 0.8597\n",
      "Epoch [9][40]\t Batch [200][550]\t Training Loss 0.6555\t Accuracy 0.8606\n",
      "Epoch [9][40]\t Batch [250][550]\t Training Loss 0.6532\t Accuracy 0.8609\n",
      "Epoch [9][40]\t Batch [300][550]\t Training Loss 0.6517\t Accuracy 0.8615\n",
      "Epoch [9][40]\t Batch [350][550]\t Training Loss 0.6560\t Accuracy 0.8602\n",
      "Epoch [9][40]\t Batch [400][550]\t Training Loss 0.6539\t Accuracy 0.8610\n",
      "Epoch [9][40]\t Batch [450][550]\t Training Loss 0.6539\t Accuracy 0.8604\n",
      "Epoch [9][40]\t Batch [500][550]\t Training Loss 0.6532\t Accuracy 0.8595\n",
      "\n",
      "Epoch [9]\t Average training loss 0.6512\t Average training accuracy 0.8597\n",
      "Epoch [9]\t Average validation loss 0.5532\t Average validation accuracy 0.8942\n",
      "\n",
      "Epoch [10][40]\t Batch [0][550]\t Training Loss 0.6011\t Accuracy 0.9000\n",
      "Epoch [10][40]\t Batch [50][550]\t Training Loss 0.6103\t Accuracy 0.8698\n",
      "Epoch [10][40]\t Batch [100][550]\t Training Loss 0.6156\t Accuracy 0.8688\n",
      "Epoch [10][40]\t Batch [150][550]\t Training Loss 0.6248\t Accuracy 0.8635\n",
      "Epoch [10][40]\t Batch [200][550]\t Training Loss 0.6250\t Accuracy 0.8642\n",
      "Epoch [10][40]\t Batch [250][550]\t Training Loss 0.6229\t Accuracy 0.8641\n",
      "Epoch [10][40]\t Batch [300][550]\t Training Loss 0.6217\t Accuracy 0.8645\n",
      "Epoch [10][40]\t Batch [350][550]\t Training Loss 0.6262\t Accuracy 0.8634\n",
      "Epoch [10][40]\t Batch [400][550]\t Training Loss 0.6243\t Accuracy 0.8640\n",
      "Epoch [10][40]\t Batch [450][550]\t Training Loss 0.6246\t Accuracy 0.8633\n",
      "Epoch [10][40]\t Batch [500][550]\t Training Loss 0.6242\t Accuracy 0.8626\n",
      "\n",
      "Epoch [10]\t Average training loss 0.6224\t Average training accuracy 0.8627\n",
      "Epoch [10]\t Average validation loss 0.5265\t Average validation accuracy 0.8958\n",
      "\n",
      "Epoch [11][40]\t Batch [0][550]\t Training Loss 0.5758\t Accuracy 0.9000\n",
      "Epoch [11][40]\t Batch [50][550]\t Training Loss 0.5839\t Accuracy 0.8729\n",
      "Epoch [11][40]\t Batch [100][550]\t Training Loss 0.5896\t Accuracy 0.8723\n",
      "Epoch [11][40]\t Batch [150][550]\t Training Loss 0.5993\t Accuracy 0.8669\n",
      "Epoch [11][40]\t Batch [200][550]\t Training Loss 0.5994\t Accuracy 0.8680\n",
      "Epoch [11][40]\t Batch [250][550]\t Training Loss 0.5976\t Accuracy 0.8681\n",
      "Epoch [11][40]\t Batch [300][550]\t Training Loss 0.5966\t Accuracy 0.8682\n",
      "Epoch [11][40]\t Batch [350][550]\t Training Loss 0.6012\t Accuracy 0.8669\n",
      "Epoch [11][40]\t Batch [400][550]\t Training Loss 0.5995\t Accuracy 0.8674\n",
      "Epoch [11][40]\t Batch [450][550]\t Training Loss 0.5999\t Accuracy 0.8668\n",
      "Epoch [11][40]\t Batch [500][550]\t Training Loss 0.5997\t Accuracy 0.8660\n",
      "\n",
      "Epoch [11]\t Average training loss 0.5982\t Average training accuracy 0.8660\n",
      "Epoch [11]\t Average validation loss 0.5040\t Average validation accuracy 0.8976\n",
      "\n",
      "Epoch [12][40]\t Batch [0][550]\t Training Loss 0.5543\t Accuracy 0.9000\n",
      "Epoch [12][40]\t Batch [50][550]\t Training Loss 0.5614\t Accuracy 0.8759\n",
      "Epoch [12][40]\t Batch [100][550]\t Training Loss 0.5675\t Accuracy 0.8749\n",
      "Epoch [12][40]\t Batch [150][550]\t Training Loss 0.5776\t Accuracy 0.8693\n",
      "Epoch [12][40]\t Batch [200][550]\t Training Loss 0.5777\t Accuracy 0.8705\n",
      "Epoch [12][40]\t Batch [250][550]\t Training Loss 0.5760\t Accuracy 0.8705\n",
      "Epoch [12][40]\t Batch [300][550]\t Training Loss 0.5752\t Accuracy 0.8708\n",
      "Epoch [12][40]\t Batch [350][550]\t Training Loss 0.5798\t Accuracy 0.8692\n",
      "Epoch [12][40]\t Batch [400][550]\t Training Loss 0.5784\t Accuracy 0.8697\n",
      "Epoch [12][40]\t Batch [450][550]\t Training Loss 0.5789\t Accuracy 0.8691\n",
      "Epoch [12][40]\t Batch [500][550]\t Training Loss 0.5789\t Accuracy 0.8683\n",
      "\n",
      "Epoch [12]\t Average training loss 0.5776\t Average training accuracy 0.8682\n",
      "Epoch [12]\t Average validation loss 0.4849\t Average validation accuracy 0.9014\n",
      "\n",
      "Epoch [13][40]\t Batch [0][550]\t Training Loss 0.5357\t Accuracy 0.9100\n",
      "Epoch [13][40]\t Batch [50][550]\t Training Loss 0.5420\t Accuracy 0.8780\n",
      "Epoch [13][40]\t Batch [100][550]\t Training Loss 0.5486\t Accuracy 0.8766\n",
      "Epoch [13][40]\t Batch [150][550]\t Training Loss 0.5589\t Accuracy 0.8711\n",
      "Epoch [13][40]\t Batch [200][550]\t Training Loss 0.5590\t Accuracy 0.8723\n",
      "Epoch [13][40]\t Batch [250][550]\t Training Loss 0.5574\t Accuracy 0.8724\n",
      "Epoch [13][40]\t Batch [300][550]\t Training Loss 0.5568\t Accuracy 0.8728\n",
      "Epoch [13][40]\t Batch [350][550]\t Training Loss 0.5615\t Accuracy 0.8712\n",
      "Epoch [13][40]\t Batch [400][550]\t Training Loss 0.5601\t Accuracy 0.8718\n",
      "Epoch [13][40]\t Batch [450][550]\t Training Loss 0.5608\t Accuracy 0.8712\n",
      "Epoch [13][40]\t Batch [500][550]\t Training Loss 0.5609\t Accuracy 0.8703\n",
      "\n",
      "Epoch [13]\t Average training loss 0.5598\t Average training accuracy 0.8702\n",
      "Epoch [13]\t Average validation loss 0.4683\t Average validation accuracy 0.9034\n",
      "\n",
      "Epoch [14][40]\t Batch [0][550]\t Training Loss 0.5195\t Accuracy 0.9100\n",
      "Epoch [14][40]\t Batch [50][550]\t Training Loss 0.5252\t Accuracy 0.8812\n",
      "Epoch [14][40]\t Batch [100][550]\t Training Loss 0.5321\t Accuracy 0.8792\n",
      "Epoch [14][40]\t Batch [150][550]\t Training Loss 0.5427\t Accuracy 0.8736\n",
      "Epoch [14][40]\t Batch [200][550]\t Training Loss 0.5427\t Accuracy 0.8748\n",
      "Epoch [14][40]\t Batch [250][550]\t Training Loss 0.5412\t Accuracy 0.8746\n",
      "Epoch [14][40]\t Batch [300][550]\t Training Loss 0.5408\t Accuracy 0.8749\n",
      "Epoch [14][40]\t Batch [350][550]\t Training Loss 0.5455\t Accuracy 0.8733\n",
      "Epoch [14][40]\t Batch [400][550]\t Training Loss 0.5442\t Accuracy 0.8739\n",
      "Epoch [14][40]\t Batch [450][550]\t Training Loss 0.5449\t Accuracy 0.8733\n",
      "Epoch [14][40]\t Batch [500][550]\t Training Loss 0.5452\t Accuracy 0.8724\n",
      "\n",
      "Epoch [14]\t Average training loss 0.5442\t Average training accuracy 0.8723\n",
      "Epoch [14]\t Average validation loss 0.4539\t Average validation accuracy 0.9044\n",
      "\n",
      "Epoch [15][40]\t Batch [0][550]\t Training Loss 0.5053\t Accuracy 0.9100\n",
      "Epoch [15][40]\t Batch [50][550]\t Training Loss 0.5105\t Accuracy 0.8827\n",
      "Epoch [15][40]\t Batch [100][550]\t Training Loss 0.5176\t Accuracy 0.8810\n",
      "Epoch [15][40]\t Batch [150][550]\t Training Loss 0.5284\t Accuracy 0.8757\n",
      "Epoch [15][40]\t Batch [200][550]\t Training Loss 0.5284\t Accuracy 0.8768\n",
      "Epoch [15][40]\t Batch [250][550]\t Training Loss 0.5270\t Accuracy 0.8767\n",
      "Epoch [15][40]\t Batch [300][550]\t Training Loss 0.5267\t Accuracy 0.8768\n",
      "Epoch [15][40]\t Batch [350][550]\t Training Loss 0.5314\t Accuracy 0.8751\n",
      "Epoch [15][40]\t Batch [400][550]\t Training Loss 0.5302\t Accuracy 0.8756\n",
      "Epoch [15][40]\t Batch [450][550]\t Training Loss 0.5310\t Accuracy 0.8751\n",
      "Epoch [15][40]\t Batch [500][550]\t Training Loss 0.5314\t Accuracy 0.8741\n",
      "\n",
      "Epoch [15]\t Average training loss 0.5305\t Average training accuracy 0.8741\n",
      "Epoch [15]\t Average validation loss 0.4412\t Average validation accuracy 0.9052\n",
      "\n",
      "Epoch [16][40]\t Batch [0][550]\t Training Loss 0.4926\t Accuracy 0.9100\n",
      "Epoch [16][40]\t Batch [50][550]\t Training Loss 0.4974\t Accuracy 0.8845\n",
      "Epoch [16][40]\t Batch [100][550]\t Training Loss 0.5049\t Accuracy 0.8832\n",
      "Epoch [16][40]\t Batch [150][550]\t Training Loss 0.5158\t Accuracy 0.8777\n",
      "Epoch [16][40]\t Batch [200][550]\t Training Loss 0.5158\t Accuracy 0.8790\n",
      "Epoch [16][40]\t Batch [250][550]\t Training Loss 0.5144\t Accuracy 0.8786\n",
      "Epoch [16][40]\t Batch [300][550]\t Training Loss 0.5142\t Accuracy 0.8787\n",
      "Epoch [16][40]\t Batch [350][550]\t Training Loss 0.5189\t Accuracy 0.8770\n",
      "Epoch [16][40]\t Batch [400][550]\t Training Loss 0.5178\t Accuracy 0.8773\n",
      "Epoch [16][40]\t Batch [450][550]\t Training Loss 0.5186\t Accuracy 0.8768\n",
      "Epoch [16][40]\t Batch [500][550]\t Training Loss 0.5191\t Accuracy 0.8758\n",
      "\n",
      "Epoch [16]\t Average training loss 0.5183\t Average training accuracy 0.8758\n",
      "Epoch [16]\t Average validation loss 0.4300\t Average validation accuracy 0.9060\n",
      "\n",
      "Epoch [17][40]\t Batch [0][550]\t Training Loss 0.4812\t Accuracy 0.9200\n",
      "Epoch [17][40]\t Batch [50][550]\t Training Loss 0.4858\t Accuracy 0.8865\n",
      "Epoch [17][40]\t Batch [100][550]\t Training Loss 0.4935\t Accuracy 0.8848\n",
      "Epoch [17][40]\t Batch [150][550]\t Training Loss 0.5046\t Accuracy 0.8793\n",
      "Epoch [17][40]\t Batch [200][550]\t Training Loss 0.5045\t Accuracy 0.8803\n",
      "Epoch [17][40]\t Batch [250][550]\t Training Loss 0.5032\t Accuracy 0.8801\n",
      "Epoch [17][40]\t Batch [300][550]\t Training Loss 0.5031\t Accuracy 0.8801\n",
      "Epoch [17][40]\t Batch [350][550]\t Training Loss 0.5077\t Accuracy 0.8785\n",
      "Epoch [17][40]\t Batch [400][550]\t Training Loss 0.5068\t Accuracy 0.8788\n",
      "Epoch [17][40]\t Batch [450][550]\t Training Loss 0.5076\t Accuracy 0.8784\n",
      "Epoch [17][40]\t Batch [500][550]\t Training Loss 0.5081\t Accuracy 0.8775\n",
      "\n",
      "Epoch [17]\t Average training loss 0.5075\t Average training accuracy 0.8775\n",
      "Epoch [17]\t Average validation loss 0.4200\t Average validation accuracy 0.9080\n",
      "\n",
      "Epoch [18][40]\t Batch [0][550]\t Training Loss 0.4710\t Accuracy 0.9200\n",
      "Epoch [18][40]\t Batch [50][550]\t Training Loss 0.4753\t Accuracy 0.8876\n",
      "Epoch [18][40]\t Batch [100][550]\t Training Loss 0.4833\t Accuracy 0.8860\n",
      "Epoch [18][40]\t Batch [150][550]\t Training Loss 0.4945\t Accuracy 0.8811\n",
      "Epoch [18][40]\t Batch [200][550]\t Training Loss 0.4943\t Accuracy 0.8819\n",
      "Epoch [18][40]\t Batch [250][550]\t Training Loss 0.4931\t Accuracy 0.8817\n",
      "Epoch [18][40]\t Batch [300][550]\t Training Loss 0.4931\t Accuracy 0.8817\n",
      "Epoch [18][40]\t Batch [350][550]\t Training Loss 0.4977\t Accuracy 0.8799\n",
      "Epoch [18][40]\t Batch [400][550]\t Training Loss 0.4968\t Accuracy 0.8803\n",
      "Epoch [18][40]\t Batch [450][550]\t Training Loss 0.4977\t Accuracy 0.8798\n",
      "Epoch [18][40]\t Batch [500][550]\t Training Loss 0.4983\t Accuracy 0.8790\n",
      "\n",
      "Epoch [18]\t Average training loss 0.4977\t Average training accuracy 0.8791\n",
      "Epoch [18]\t Average validation loss 0.4111\t Average validation accuracy 0.9088\n",
      "\n",
      "Epoch [19][40]\t Batch [0][550]\t Training Loss 0.4617\t Accuracy 0.9200\n",
      "Epoch [19][40]\t Batch [50][550]\t Training Loss 0.4659\t Accuracy 0.8890\n",
      "Epoch [19][40]\t Batch [100][550]\t Training Loss 0.4740\t Accuracy 0.8864\n",
      "Epoch [19][40]\t Batch [150][550]\t Training Loss 0.4854\t Accuracy 0.8819\n",
      "Epoch [19][40]\t Batch [200][550]\t Training Loss 0.4852\t Accuracy 0.8829\n",
      "Epoch [19][40]\t Batch [250][550]\t Training Loss 0.4840\t Accuracy 0.8827\n",
      "Epoch [19][40]\t Batch [300][550]\t Training Loss 0.4840\t Accuracy 0.8826\n",
      "Epoch [19][40]\t Batch [350][550]\t Training Loss 0.4886\t Accuracy 0.8807\n",
      "Epoch [19][40]\t Batch [400][550]\t Training Loss 0.4878\t Accuracy 0.8811\n",
      "Epoch [19][40]\t Batch [450][550]\t Training Loss 0.4887\t Accuracy 0.8807\n",
      "Epoch [19][40]\t Batch [500][550]\t Training Loss 0.4894\t Accuracy 0.8799\n",
      "\n",
      "Epoch [19]\t Average training loss 0.4889\t Average training accuracy 0.8800\n",
      "Epoch [19]\t Average validation loss 0.4030\t Average validation accuracy 0.9102\n",
      "\n",
      "Epoch [20][40]\t Batch [0][550]\t Training Loss 0.4532\t Accuracy 0.9300\n",
      "Epoch [20][40]\t Batch [50][550]\t Training Loss 0.4574\t Accuracy 0.8910\n",
      "Epoch [20][40]\t Batch [100][550]\t Training Loss 0.4657\t Accuracy 0.8878\n",
      "Epoch [20][40]\t Batch [150][550]\t Training Loss 0.4772\t Accuracy 0.8830\n",
      "Epoch [20][40]\t Batch [200][550]\t Training Loss 0.4768\t Accuracy 0.8840\n",
      "Epoch [20][40]\t Batch [250][550]\t Training Loss 0.4757\t Accuracy 0.8839\n",
      "Epoch [20][40]\t Batch [300][550]\t Training Loss 0.4758\t Accuracy 0.8837\n",
      "Epoch [20][40]\t Batch [350][550]\t Training Loss 0.4804\t Accuracy 0.8819\n",
      "Epoch [20][40]\t Batch [400][550]\t Training Loss 0.4796\t Accuracy 0.8822\n",
      "Epoch [20][40]\t Batch [450][550]\t Training Loss 0.4805\t Accuracy 0.8817\n",
      "Epoch [20][40]\t Batch [500][550]\t Training Loss 0.4812\t Accuracy 0.8809\n",
      "\n",
      "Epoch [20]\t Average training loss 0.4808\t Average training accuracy 0.8811\n",
      "Epoch [20]\t Average validation loss 0.3957\t Average validation accuracy 0.9108\n",
      "\n",
      "Epoch [21][40]\t Batch [0][550]\t Training Loss 0.4454\t Accuracy 0.9300\n",
      "Epoch [21][40]\t Batch [50][550]\t Training Loss 0.4496\t Accuracy 0.8918\n",
      "Epoch [21][40]\t Batch [100][550]\t Training Loss 0.4581\t Accuracy 0.8889\n",
      "Epoch [21][40]\t Batch [150][550]\t Training Loss 0.4697\t Accuracy 0.8842\n",
      "Epoch [21][40]\t Batch [200][550]\t Training Loss 0.4693\t Accuracy 0.8851\n",
      "Epoch [21][40]\t Batch [250][550]\t Training Loss 0.4682\t Accuracy 0.8849\n",
      "Epoch [21][40]\t Batch [300][550]\t Training Loss 0.4683\t Accuracy 0.8848\n",
      "Epoch [21][40]\t Batch [350][550]\t Training Loss 0.4729\t Accuracy 0.8831\n",
      "Epoch [21][40]\t Batch [400][550]\t Training Loss 0.4721\t Accuracy 0.8835\n",
      "Epoch [21][40]\t Batch [450][550]\t Training Loss 0.4731\t Accuracy 0.8830\n",
      "Epoch [21][40]\t Batch [500][550]\t Training Loss 0.4738\t Accuracy 0.8821\n",
      "\n",
      "Epoch [21]\t Average training loss 0.4735\t Average training accuracy 0.8822\n",
      "Epoch [21]\t Average validation loss 0.3890\t Average validation accuracy 0.9122\n",
      "\n",
      "Epoch [22][40]\t Batch [0][550]\t Training Loss 0.4383\t Accuracy 0.9300\n",
      "Epoch [22][40]\t Batch [50][550]\t Training Loss 0.4424\t Accuracy 0.8925\n",
      "Epoch [22][40]\t Batch [100][550]\t Training Loss 0.4511\t Accuracy 0.8896\n",
      "Epoch [22][40]\t Batch [150][550]\t Training Loss 0.4628\t Accuracy 0.8849\n",
      "Epoch [22][40]\t Batch [200][550]\t Training Loss 0.4623\t Accuracy 0.8859\n",
      "Epoch [22][40]\t Batch [250][550]\t Training Loss 0.4613\t Accuracy 0.8857\n",
      "Epoch [22][40]\t Batch [300][550]\t Training Loss 0.4615\t Accuracy 0.8855\n",
      "Epoch [22][40]\t Batch [350][550]\t Training Loss 0.4660\t Accuracy 0.8838\n",
      "Epoch [22][40]\t Batch [400][550]\t Training Loss 0.4653\t Accuracy 0.8843\n",
      "Epoch [22][40]\t Batch [450][550]\t Training Loss 0.4663\t Accuracy 0.8840\n",
      "Epoch [22][40]\t Batch [500][550]\t Training Loss 0.4671\t Accuracy 0.8832\n",
      "\n",
      "Epoch [22]\t Average training loss 0.4668\t Average training accuracy 0.8832\n",
      "Epoch [22]\t Average validation loss 0.3829\t Average validation accuracy 0.9128\n",
      "\n",
      "Epoch [23][40]\t Batch [0][550]\t Training Loss 0.4317\t Accuracy 0.9300\n",
      "Epoch [23][40]\t Batch [50][550]\t Training Loss 0.4359\t Accuracy 0.8935\n",
      "Epoch [23][40]\t Batch [100][550]\t Training Loss 0.4447\t Accuracy 0.8904\n",
      "Epoch [23][40]\t Batch [150][550]\t Training Loss 0.4565\t Accuracy 0.8858\n",
      "Epoch [23][40]\t Batch [200][550]\t Training Loss 0.4559\t Accuracy 0.8868\n",
      "Epoch [23][40]\t Batch [250][550]\t Training Loss 0.4549\t Accuracy 0.8868\n",
      "Epoch [23][40]\t Batch [300][550]\t Training Loss 0.4551\t Accuracy 0.8865\n",
      "Epoch [23][40]\t Batch [350][550]\t Training Loss 0.4597\t Accuracy 0.8848\n",
      "Epoch [23][40]\t Batch [400][550]\t Training Loss 0.4590\t Accuracy 0.8852\n",
      "Epoch [23][40]\t Batch [450][550]\t Training Loss 0.4600\t Accuracy 0.8849\n",
      "Epoch [23][40]\t Batch [500][550]\t Training Loss 0.4608\t Accuracy 0.8841\n",
      "\n",
      "Epoch [23]\t Average training loss 0.4606\t Average training accuracy 0.8841\n",
      "Epoch [23]\t Average validation loss 0.3773\t Average validation accuracy 0.9134\n",
      "\n",
      "Epoch [24][40]\t Batch [0][550]\t Training Loss 0.4255\t Accuracy 0.9300\n",
      "Epoch [24][40]\t Batch [50][550]\t Training Loss 0.4298\t Accuracy 0.8945\n",
      "Epoch [24][40]\t Batch [100][550]\t Training Loss 0.4388\t Accuracy 0.8910\n",
      "Epoch [24][40]\t Batch [150][550]\t Training Loss 0.4506\t Accuracy 0.8868\n",
      "Epoch [24][40]\t Batch [200][550]\t Training Loss 0.4500\t Accuracy 0.8877\n",
      "Epoch [24][40]\t Batch [250][550]\t Training Loss 0.4491\t Accuracy 0.8878\n",
      "Epoch [24][40]\t Batch [300][550]\t Training Loss 0.4493\t Accuracy 0.8876\n",
      "Epoch [24][40]\t Batch [350][550]\t Training Loss 0.4538\t Accuracy 0.8859\n",
      "Epoch [24][40]\t Batch [400][550]\t Training Loss 0.4532\t Accuracy 0.8862\n",
      "Epoch [24][40]\t Batch [450][550]\t Training Loss 0.4542\t Accuracy 0.8858\n",
      "Epoch [24][40]\t Batch [500][550]\t Training Loss 0.4550\t Accuracy 0.8850\n",
      "\n",
      "Epoch [24]\t Average training loss 0.4548\t Average training accuracy 0.8851\n",
      "Epoch [24]\t Average validation loss 0.3722\t Average validation accuracy 0.9138\n",
      "\n",
      "Epoch [25][40]\t Batch [0][550]\t Training Loss 0.4198\t Accuracy 0.9400\n",
      "Epoch [25][40]\t Batch [50][550]\t Training Loss 0.4243\t Accuracy 0.8955\n",
      "Epoch [25][40]\t Batch [100][550]\t Training Loss 0.4334\t Accuracy 0.8923\n",
      "Epoch [25][40]\t Batch [150][550]\t Training Loss 0.4452\t Accuracy 0.8879\n",
      "Epoch [25][40]\t Batch [200][550]\t Training Loss 0.4446\t Accuracy 0.8889\n",
      "Epoch [25][40]\t Batch [250][550]\t Training Loss 0.4437\t Accuracy 0.8891\n",
      "Epoch [25][40]\t Batch [300][550]\t Training Loss 0.4439\t Accuracy 0.8887\n",
      "Epoch [25][40]\t Batch [350][550]\t Training Loss 0.4484\t Accuracy 0.8870\n",
      "Epoch [25][40]\t Batch [400][550]\t Training Loss 0.4478\t Accuracy 0.8872\n",
      "Epoch [25][40]\t Batch [450][550]\t Training Loss 0.4488\t Accuracy 0.8868\n",
      "Epoch [25][40]\t Batch [500][550]\t Training Loss 0.4497\t Accuracy 0.8860\n",
      "\n",
      "Epoch [25]\t Average training loss 0.4495\t Average training accuracy 0.8861\n",
      "Epoch [25]\t Average validation loss 0.3674\t Average validation accuracy 0.9140\n",
      "\n",
      "Epoch [26][40]\t Batch [0][550]\t Training Loss 0.4145\t Accuracy 0.9400\n",
      "Epoch [26][40]\t Batch [50][550]\t Training Loss 0.4191\t Accuracy 0.8965\n",
      "Epoch [26][40]\t Batch [100][550]\t Training Loss 0.4283\t Accuracy 0.8932\n",
      "Epoch [26][40]\t Batch [150][550]\t Training Loss 0.4402\t Accuracy 0.8885\n",
      "Epoch [26][40]\t Batch [200][550]\t Training Loss 0.4395\t Accuracy 0.8897\n",
      "Epoch [26][40]\t Batch [250][550]\t Training Loss 0.4386\t Accuracy 0.8900\n",
      "Epoch [26][40]\t Batch [300][550]\t Training Loss 0.4389\t Accuracy 0.8895\n",
      "Epoch [26][40]\t Batch [350][550]\t Training Loss 0.4434\t Accuracy 0.8880\n",
      "Epoch [26][40]\t Batch [400][550]\t Training Loss 0.4428\t Accuracy 0.8882\n",
      "Epoch [26][40]\t Batch [450][550]\t Training Loss 0.4438\t Accuracy 0.8877\n",
      "Epoch [26][40]\t Batch [500][550]\t Training Loss 0.4447\t Accuracy 0.8869\n",
      "\n",
      "Epoch [26]\t Average training loss 0.4446\t Average training accuracy 0.8870\n",
      "Epoch [26]\t Average validation loss 0.3630\t Average validation accuracy 0.9148\n",
      "\n",
      "Epoch [27][40]\t Batch [0][550]\t Training Loss 0.4095\t Accuracy 0.9400\n",
      "Epoch [27][40]\t Batch [50][550]\t Training Loss 0.4142\t Accuracy 0.8976\n",
      "Epoch [27][40]\t Batch [100][550]\t Training Loss 0.4236\t Accuracy 0.8943\n",
      "Epoch [27][40]\t Batch [150][550]\t Training Loss 0.4355\t Accuracy 0.8895\n",
      "Epoch [27][40]\t Batch [200][550]\t Training Loss 0.4348\t Accuracy 0.8907\n",
      "Epoch [27][40]\t Batch [250][550]\t Training Loss 0.4339\t Accuracy 0.8910\n",
      "Epoch [27][40]\t Batch [300][550]\t Training Loss 0.4342\t Accuracy 0.8905\n",
      "Epoch [27][40]\t Batch [350][550]\t Training Loss 0.4387\t Accuracy 0.8889\n",
      "Epoch [27][40]\t Batch [400][550]\t Training Loss 0.4381\t Accuracy 0.8891\n",
      "Epoch [27][40]\t Batch [450][550]\t Training Loss 0.4391\t Accuracy 0.8886\n",
      "Epoch [27][40]\t Batch [500][550]\t Training Loss 0.4401\t Accuracy 0.8878\n",
      "\n",
      "Epoch [27]\t Average training loss 0.4400\t Average training accuracy 0.8879\n",
      "Epoch [27]\t Average validation loss 0.3590\t Average validation accuracy 0.9160\n",
      "\n",
      "Epoch [28][40]\t Batch [0][550]\t Training Loss 0.4048\t Accuracy 0.9400\n",
      "Epoch [28][40]\t Batch [50][550]\t Training Loss 0.4097\t Accuracy 0.8984\n",
      "Epoch [28][40]\t Batch [100][550]\t Training Loss 0.4192\t Accuracy 0.8950\n",
      "Epoch [28][40]\t Batch [150][550]\t Training Loss 0.4312\t Accuracy 0.8901\n",
      "Epoch [28][40]\t Batch [200][550]\t Training Loss 0.4304\t Accuracy 0.8912\n",
      "Epoch [28][40]\t Batch [250][550]\t Training Loss 0.4295\t Accuracy 0.8916\n",
      "Epoch [28][40]\t Batch [300][550]\t Training Loss 0.4299\t Accuracy 0.8912\n",
      "Epoch [28][40]\t Batch [350][550]\t Training Loss 0.4343\t Accuracy 0.8896\n",
      "Epoch [28][40]\t Batch [400][550]\t Training Loss 0.4337\t Accuracy 0.8897\n",
      "Epoch [28][40]\t Batch [450][550]\t Training Loss 0.4348\t Accuracy 0.8891\n",
      "Epoch [28][40]\t Batch [500][550]\t Training Loss 0.4357\t Accuracy 0.8884\n",
      "\n",
      "Epoch [28]\t Average training loss 0.4357\t Average training accuracy 0.8884\n",
      "Epoch [28]\t Average validation loss 0.3552\t Average validation accuracy 0.9162\n",
      "\n",
      "Epoch [29][40]\t Batch [0][550]\t Training Loss 0.4004\t Accuracy 0.9400\n",
      "Epoch [29][40]\t Batch [50][550]\t Training Loss 0.4055\t Accuracy 0.8990\n",
      "Epoch [29][40]\t Batch [100][550]\t Training Loss 0.4151\t Accuracy 0.8954\n",
      "Epoch [29][40]\t Batch [150][550]\t Training Loss 0.4271\t Accuracy 0.8906\n",
      "Epoch [29][40]\t Batch [200][550]\t Training Loss 0.4263\t Accuracy 0.8916\n",
      "Epoch [29][40]\t Batch [250][550]\t Training Loss 0.4254\t Accuracy 0.8920\n",
      "Epoch [29][40]\t Batch [300][550]\t Training Loss 0.4258\t Accuracy 0.8916\n",
      "Epoch [29][40]\t Batch [350][550]\t Training Loss 0.4302\t Accuracy 0.8901\n",
      "Epoch [29][40]\t Batch [400][550]\t Training Loss 0.4297\t Accuracy 0.8902\n",
      "Epoch [29][40]\t Batch [450][550]\t Training Loss 0.4307\t Accuracy 0.8896\n",
      "Epoch [29][40]\t Batch [500][550]\t Training Loss 0.4317\t Accuracy 0.8889\n",
      "\n",
      "Epoch [29]\t Average training loss 0.4317\t Average training accuracy 0.8890\n",
      "Epoch [29]\t Average validation loss 0.3516\t Average validation accuracy 0.9170\n",
      "\n",
      "Epoch [30][40]\t Batch [0][550]\t Training Loss 0.3962\t Accuracy 0.9400\n",
      "Epoch [30][40]\t Batch [50][550]\t Training Loss 0.4016\t Accuracy 0.8992\n",
      "Epoch [30][40]\t Batch [100][550]\t Training Loss 0.4112\t Accuracy 0.8961\n",
      "Epoch [30][40]\t Batch [150][550]\t Training Loss 0.4233\t Accuracy 0.8913\n",
      "Epoch [30][40]\t Batch [200][550]\t Training Loss 0.4224\t Accuracy 0.8922\n",
      "Epoch [30][40]\t Batch [250][550]\t Training Loss 0.4216\t Accuracy 0.8928\n",
      "Epoch [30][40]\t Batch [300][550]\t Training Loss 0.4220\t Accuracy 0.8922\n",
      "Epoch [30][40]\t Batch [350][550]\t Training Loss 0.4263\t Accuracy 0.8908\n",
      "Epoch [30][40]\t Batch [400][550]\t Training Loss 0.4258\t Accuracy 0.8908\n",
      "Epoch [30][40]\t Batch [450][550]\t Training Loss 0.4269\t Accuracy 0.8903\n",
      "Epoch [30][40]\t Batch [500][550]\t Training Loss 0.4279\t Accuracy 0.8897\n",
      "\n",
      "Epoch [30]\t Average training loss 0.4279\t Average training accuracy 0.8897\n",
      "Epoch [30]\t Average validation loss 0.3483\t Average validation accuracy 0.9172\n",
      "\n",
      "Epoch [31][40]\t Batch [0][550]\t Training Loss 0.3923\t Accuracy 0.9400\n",
      "Epoch [31][40]\t Batch [50][550]\t Training Loss 0.3979\t Accuracy 0.8998\n",
      "Epoch [31][40]\t Batch [100][550]\t Training Loss 0.4076\t Accuracy 0.8968\n",
      "Epoch [31][40]\t Batch [150][550]\t Training Loss 0.4197\t Accuracy 0.8919\n",
      "Epoch [31][40]\t Batch [200][550]\t Training Loss 0.4188\t Accuracy 0.8930\n",
      "Epoch [31][40]\t Batch [250][550]\t Training Loss 0.4180\t Accuracy 0.8935\n",
      "Epoch [31][40]\t Batch [300][550]\t Training Loss 0.4184\t Accuracy 0.8929\n",
      "Epoch [31][40]\t Batch [350][550]\t Training Loss 0.4227\t Accuracy 0.8915\n",
      "Epoch [31][40]\t Batch [400][550]\t Training Loss 0.4222\t Accuracy 0.8915\n",
      "Epoch [31][40]\t Batch [450][550]\t Training Loss 0.4233\t Accuracy 0.8909\n",
      "Epoch [31][40]\t Batch [500][550]\t Training Loss 0.4243\t Accuracy 0.8903\n",
      "\n",
      "Epoch [31]\t Average training loss 0.4243\t Average training accuracy 0.8903\n",
      "Epoch [31]\t Average validation loss 0.3452\t Average validation accuracy 0.9180\n",
      "\n",
      "Epoch [32][40]\t Batch [0][550]\t Training Loss 0.3886\t Accuracy 0.9400\n",
      "Epoch [32][40]\t Batch [50][550]\t Training Loss 0.3944\t Accuracy 0.9014\n",
      "Epoch [32][40]\t Batch [100][550]\t Training Loss 0.4042\t Accuracy 0.8977\n",
      "Epoch [32][40]\t Batch [150][550]\t Training Loss 0.4163\t Accuracy 0.8928\n",
      "Epoch [32][40]\t Batch [200][550]\t Training Loss 0.4153\t Accuracy 0.8938\n",
      "Epoch [32][40]\t Batch [250][550]\t Training Loss 0.4146\t Accuracy 0.8942\n",
      "Epoch [32][40]\t Batch [300][550]\t Training Loss 0.4150\t Accuracy 0.8936\n",
      "Epoch [32][40]\t Batch [350][550]\t Training Loss 0.4193\t Accuracy 0.8921\n",
      "Epoch [32][40]\t Batch [400][550]\t Training Loss 0.4188\t Accuracy 0.8920\n",
      "Epoch [32][40]\t Batch [450][550]\t Training Loss 0.4199\t Accuracy 0.8916\n",
      "Epoch [32][40]\t Batch [500][550]\t Training Loss 0.4209\t Accuracy 0.8910\n",
      "\n",
      "Epoch [32]\t Average training loss 0.4210\t Average training accuracy 0.8910\n",
      "Epoch [32]\t Average validation loss 0.3422\t Average validation accuracy 0.9184\n",
      "\n",
      "Epoch [33][40]\t Batch [0][550]\t Training Loss 0.3851\t Accuracy 0.9400\n",
      "Epoch [33][40]\t Batch [50][550]\t Training Loss 0.3911\t Accuracy 0.9018\n",
      "Epoch [33][40]\t Batch [100][550]\t Training Loss 0.4010\t Accuracy 0.8980\n",
      "Epoch [33][40]\t Batch [150][550]\t Training Loss 0.4131\t Accuracy 0.8932\n",
      "Epoch [33][40]\t Batch [200][550]\t Training Loss 0.4121\t Accuracy 0.8940\n",
      "Epoch [33][40]\t Batch [250][550]\t Training Loss 0.4114\t Accuracy 0.8946\n",
      "Epoch [33][40]\t Batch [300][550]\t Training Loss 0.4118\t Accuracy 0.8941\n",
      "Epoch [33][40]\t Batch [350][550]\t Training Loss 0.4161\t Accuracy 0.8926\n",
      "Epoch [33][40]\t Batch [400][550]\t Training Loss 0.4156\t Accuracy 0.8925\n",
      "Epoch [33][40]\t Batch [450][550]\t Training Loss 0.4167\t Accuracy 0.8920\n",
      "Epoch [33][40]\t Batch [500][550]\t Training Loss 0.4177\t Accuracy 0.8915\n",
      "\n",
      "Epoch [33]\t Average training loss 0.4178\t Average training accuracy 0.8914\n",
      "Epoch [33]\t Average validation loss 0.3395\t Average validation accuracy 0.9188\n",
      "\n",
      "Epoch [34][40]\t Batch [0][550]\t Training Loss 0.3817\t Accuracy 0.9400\n",
      "Epoch [34][40]\t Batch [50][550]\t Training Loss 0.3880\t Accuracy 0.9020\n",
      "Epoch [34][40]\t Batch [100][550]\t Training Loss 0.3979\t Accuracy 0.8983\n",
      "Epoch [34][40]\t Batch [150][550]\t Training Loss 0.4101\t Accuracy 0.8936\n",
      "Epoch [34][40]\t Batch [200][550]\t Training Loss 0.4091\t Accuracy 0.8944\n",
      "Epoch [34][40]\t Batch [250][550]\t Training Loss 0.4083\t Accuracy 0.8951\n",
      "Epoch [34][40]\t Batch [300][550]\t Training Loss 0.4088\t Accuracy 0.8946\n",
      "Epoch [34][40]\t Batch [350][550]\t Training Loss 0.4130\t Accuracy 0.8930\n",
      "Epoch [34][40]\t Batch [400][550]\t Training Loss 0.4126\t Accuracy 0.8931\n",
      "Epoch [34][40]\t Batch [450][550]\t Training Loss 0.4137\t Accuracy 0.8927\n",
      "Epoch [34][40]\t Batch [500][550]\t Training Loss 0.4147\t Accuracy 0.8922\n",
      "\n",
      "Epoch [34]\t Average training loss 0.4148\t Average training accuracy 0.8921\n",
      "Epoch [34]\t Average validation loss 0.3369\t Average validation accuracy 0.9196\n",
      "\n",
      "Epoch [35][40]\t Batch [0][550]\t Training Loss 0.3785\t Accuracy 0.9400\n",
      "Epoch [35][40]\t Batch [50][550]\t Training Loss 0.3850\t Accuracy 0.9029\n",
      "Epoch [35][40]\t Batch [100][550]\t Training Loss 0.3951\t Accuracy 0.8992\n",
      "Epoch [35][40]\t Batch [150][550]\t Training Loss 0.4073\t Accuracy 0.8942\n",
      "Epoch [35][40]\t Batch [200][550]\t Training Loss 0.4062\t Accuracy 0.8950\n",
      "Epoch [35][40]\t Batch [250][550]\t Training Loss 0.4054\t Accuracy 0.8955\n",
      "Epoch [35][40]\t Batch [300][550]\t Training Loss 0.4059\t Accuracy 0.8950\n",
      "Epoch [35][40]\t Batch [350][550]\t Training Loss 0.4101\t Accuracy 0.8935\n",
      "Epoch [35][40]\t Batch [400][550]\t Training Loss 0.4097\t Accuracy 0.8936\n",
      "Epoch [35][40]\t Batch [450][550]\t Training Loss 0.4108\t Accuracy 0.8933\n",
      "Epoch [35][40]\t Batch [500][550]\t Training Loss 0.4118\t Accuracy 0.8927\n",
      "\n",
      "Epoch [35]\t Average training loss 0.4120\t Average training accuracy 0.8925\n",
      "Epoch [35]\t Average validation loss 0.3344\t Average validation accuracy 0.9200\n",
      "\n",
      "Epoch [36][40]\t Batch [0][550]\t Training Loss 0.3755\t Accuracy 0.9400\n",
      "Epoch [36][40]\t Batch [50][550]\t Training Loss 0.3822\t Accuracy 0.9037\n",
      "Epoch [36][40]\t Batch [100][550]\t Training Loss 0.3923\t Accuracy 0.8999\n",
      "Epoch [36][40]\t Batch [150][550]\t Training Loss 0.4046\t Accuracy 0.8948\n",
      "Epoch [36][40]\t Batch [200][550]\t Training Loss 0.4034\t Accuracy 0.8956\n",
      "Epoch [36][40]\t Batch [250][550]\t Training Loss 0.4027\t Accuracy 0.8961\n",
      "Epoch [36][40]\t Batch [300][550]\t Training Loss 0.4032\t Accuracy 0.8956\n",
      "Epoch [36][40]\t Batch [350][550]\t Training Loss 0.4074\t Accuracy 0.8941\n",
      "Epoch [36][40]\t Batch [400][550]\t Training Loss 0.4070\t Accuracy 0.8943\n",
      "Epoch [36][40]\t Batch [450][550]\t Training Loss 0.4081\t Accuracy 0.8940\n",
      "Epoch [36][40]\t Batch [500][550]\t Training Loss 0.4091\t Accuracy 0.8935\n",
      "\n",
      "Epoch [36]\t Average training loss 0.4093\t Average training accuracy 0.8933\n",
      "Epoch [36]\t Average validation loss 0.3321\t Average validation accuracy 0.9206\n",
      "\n",
      "Epoch [37][40]\t Batch [0][550]\t Training Loss 0.3726\t Accuracy 0.9400\n",
      "Epoch [37][40]\t Batch [50][550]\t Training Loss 0.3796\t Accuracy 0.9045\n",
      "Epoch [37][40]\t Batch [100][550]\t Training Loss 0.3898\t Accuracy 0.9005\n",
      "Epoch [37][40]\t Batch [150][550]\t Training Loss 0.4020\t Accuracy 0.8954\n",
      "Epoch [37][40]\t Batch [200][550]\t Training Loss 0.4008\t Accuracy 0.8962\n",
      "Epoch [37][40]\t Batch [250][550]\t Training Loss 0.4001\t Accuracy 0.8967\n",
      "Epoch [37][40]\t Batch [300][550]\t Training Loss 0.4006\t Accuracy 0.8962\n",
      "Epoch [37][40]\t Batch [350][550]\t Training Loss 0.4048\t Accuracy 0.8946\n",
      "Epoch [37][40]\t Batch [400][550]\t Training Loss 0.4044\t Accuracy 0.8948\n",
      "Epoch [37][40]\t Batch [450][550]\t Training Loss 0.4055\t Accuracy 0.8945\n",
      "Epoch [37][40]\t Batch [500][550]\t Training Loss 0.4066\t Accuracy 0.8940\n",
      "\n",
      "Epoch [37]\t Average training loss 0.4067\t Average training accuracy 0.8938\n",
      "Epoch [37]\t Average validation loss 0.3299\t Average validation accuracy 0.9206\n",
      "\n",
      "Epoch [38][40]\t Batch [0][550]\t Training Loss 0.3698\t Accuracy 0.9400\n",
      "Epoch [38][40]\t Batch [50][550]\t Training Loss 0.3770\t Accuracy 0.9049\n",
      "Epoch [38][40]\t Batch [100][550]\t Training Loss 0.3873\t Accuracy 0.9012\n",
      "Epoch [38][40]\t Batch [150][550]\t Training Loss 0.3996\t Accuracy 0.8960\n",
      "Epoch [38][40]\t Batch [200][550]\t Training Loss 0.3983\t Accuracy 0.8970\n",
      "Epoch [38][40]\t Batch [250][550]\t Training Loss 0.3977\t Accuracy 0.8974\n",
      "Epoch [38][40]\t Batch [300][550]\t Training Loss 0.3981\t Accuracy 0.8968\n",
      "Epoch [38][40]\t Batch [350][550]\t Training Loss 0.4023\t Accuracy 0.8952\n",
      "Epoch [38][40]\t Batch [400][550]\t Training Loss 0.4019\t Accuracy 0.8953\n",
      "Epoch [38][40]\t Batch [450][550]\t Training Loss 0.4030\t Accuracy 0.8950\n",
      "Epoch [38][40]\t Batch [500][550]\t Training Loss 0.4041\t Accuracy 0.8945\n",
      "\n",
      "Epoch [38]\t Average training loss 0.4043\t Average training accuracy 0.8943\n",
      "Epoch [38]\t Average validation loss 0.3278\t Average validation accuracy 0.9210\n",
      "\n",
      "Epoch [39][40]\t Batch [0][550]\t Training Loss 0.3672\t Accuracy 0.9400\n",
      "Epoch [39][40]\t Batch [50][550]\t Training Loss 0.3746\t Accuracy 0.9051\n",
      "Epoch [39][40]\t Batch [100][550]\t Training Loss 0.3850\t Accuracy 0.9016\n",
      "Epoch [39][40]\t Batch [150][550]\t Training Loss 0.3972\t Accuracy 0.8963\n",
      "Epoch [39][40]\t Batch [200][550]\t Training Loss 0.3960\t Accuracy 0.8973\n",
      "Epoch [39][40]\t Batch [250][550]\t Training Loss 0.3953\t Accuracy 0.8976\n",
      "Epoch [39][40]\t Batch [300][550]\t Training Loss 0.3958\t Accuracy 0.8970\n",
      "Epoch [39][40]\t Batch [350][550]\t Training Loss 0.3999\t Accuracy 0.8956\n",
      "Epoch [39][40]\t Batch [400][550]\t Training Loss 0.3996\t Accuracy 0.8956\n",
      "Epoch [39][40]\t Batch [450][550]\t Training Loss 0.4007\t Accuracy 0.8953\n",
      "Epoch [39][40]\t Batch [500][550]\t Training Loss 0.4018\t Accuracy 0.8948\n",
      "\n",
      "Epoch [39]\t Average training loss 0.4020\t Average training accuracy 0.8946\n",
      "Epoch [39]\t Average validation loss 0.3258\t Average validation accuracy 0.9214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sigmoidMLP, sigmoid_loss, sigmoid_acc = train(sigmoidMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9048.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(sigmoidMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 MLP with Softmax Cross-Entropy Loss and ReLU Activation Function\n",
    "Build and train a MLP contraining one hidden layer with 128 units using ReLU activation function and Softmax cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 40\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 4e-3\n",
    "weight_decay = 1e-3\n",
    "\n",
    "disp_freq = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reluMLP = Network()\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][40]\t Batch [0][550]\t Training Loss 2.6442\t Accuracy 0.1000\n",
      "Epoch [0][40]\t Batch [50][550]\t Training Loss 2.3083\t Accuracy 0.1614\n",
      "Epoch [0][40]\t Batch [100][550]\t Training Loss 2.1158\t Accuracy 0.2598\n",
      "Epoch [0][40]\t Batch [150][550]\t Training Loss 1.9603\t Accuracy 0.3483\n",
      "Epoch [0][40]\t Batch [200][550]\t Training Loss 1.8406\t Accuracy 0.4153\n",
      "Epoch [0][40]\t Batch [250][550]\t Training Loss 1.7335\t Accuracy 0.4680\n",
      "Epoch [0][40]\t Batch [300][550]\t Training Loss 1.6401\t Accuracy 0.5113\n",
      "Epoch [0][40]\t Batch [350][550]\t Training Loss 1.5653\t Accuracy 0.5443\n",
      "Epoch [0][40]\t Batch [400][550]\t Training Loss 1.4938\t Accuracy 0.5718\n",
      "Epoch [0][40]\t Batch [450][550]\t Training Loss 1.4324\t Accuracy 0.5947\n",
      "Epoch [0][40]\t Batch [500][550]\t Training Loss 1.3768\t Accuracy 0.6139\n",
      "\n",
      "Epoch [0]\t Average training loss 1.3257\t Average training accuracy 0.6316\n",
      "Epoch [0]\t Average validation loss 0.7080\t Average validation accuracy 0.8522\n",
      "\n",
      "Epoch [1][40]\t Batch [0][550]\t Training Loss 0.7485\t Accuracy 0.8500\n",
      "Epoch [1][40]\t Batch [50][550]\t Training Loss 0.7560\t Accuracy 0.8200\n",
      "Epoch [1][40]\t Batch [100][550]\t Training Loss 0.7476\t Accuracy 0.8231\n",
      "Epoch [1][40]\t Batch [150][550]\t Training Loss 0.7360\t Accuracy 0.8224\n",
      "Epoch [1][40]\t Batch [200][550]\t Training Loss 0.7246\t Accuracy 0.8240\n",
      "Epoch [1][40]\t Batch [250][550]\t Training Loss 0.7096\t Accuracy 0.8266\n",
      "Epoch [1][40]\t Batch [300][550]\t Training Loss 0.6956\t Accuracy 0.8296\n",
      "Epoch [1][40]\t Batch [350][550]\t Training Loss 0.6894\t Accuracy 0.8307\n",
      "Epoch [1][40]\t Batch [400][550]\t Training Loss 0.6787\t Accuracy 0.8328\n",
      "Epoch [1][40]\t Batch [450][550]\t Training Loss 0.6701\t Accuracy 0.8343\n",
      "Epoch [1][40]\t Batch [500][550]\t Training Loss 0.6619\t Accuracy 0.8355\n",
      "\n",
      "Epoch [1]\t Average training loss 0.6521\t Average training accuracy 0.8376\n",
      "Epoch [1]\t Average validation loss 0.4674\t Average validation accuracy 0.8930\n",
      "\n",
      "Epoch [2][40]\t Batch [0][550]\t Training Loss 0.5312\t Accuracy 0.8800\n",
      "Epoch [2][40]\t Batch [50][550]\t Training Loss 0.5301\t Accuracy 0.8635\n",
      "Epoch [2][40]\t Batch [100][550]\t Training Loss 0.5347\t Accuracy 0.8627\n",
      "Epoch [2][40]\t Batch [150][550]\t Training Loss 0.5362\t Accuracy 0.8599\n",
      "Epoch [2][40]\t Batch [200][550]\t Training Loss 0.5331\t Accuracy 0.8610\n",
      "Epoch [2][40]\t Batch [250][550]\t Training Loss 0.5272\t Accuracy 0.8622\n",
      "Epoch [2][40]\t Batch [300][550]\t Training Loss 0.5219\t Accuracy 0.8639\n",
      "Epoch [2][40]\t Batch [350][550]\t Training Loss 0.5224\t Accuracy 0.8639\n",
      "Epoch [2][40]\t Batch [400][550]\t Training Loss 0.5185\t Accuracy 0.8651\n",
      "Epoch [2][40]\t Batch [450][550]\t Training Loss 0.5159\t Accuracy 0.8654\n",
      "Epoch [2][40]\t Batch [500][550]\t Training Loss 0.5135\t Accuracy 0.8653\n",
      "\n",
      "Epoch [2]\t Average training loss 0.5093\t Average training accuracy 0.8662\n",
      "Epoch [2]\t Average validation loss 0.3837\t Average validation accuracy 0.9066\n",
      "\n",
      "Epoch [3][40]\t Batch [0][550]\t Training Loss 0.4491\t Accuracy 0.9100\n",
      "Epoch [3][40]\t Batch [50][550]\t Training Loss 0.4445\t Accuracy 0.8812\n",
      "Epoch [3][40]\t Batch [100][550]\t Training Loss 0.4525\t Accuracy 0.8804\n",
      "Epoch [3][40]\t Batch [150][550]\t Training Loss 0.4579\t Accuracy 0.8769\n",
      "Epoch [3][40]\t Batch [200][550]\t Training Loss 0.4562\t Accuracy 0.8769\n",
      "Epoch [3][40]\t Batch [250][550]\t Training Loss 0.4528\t Accuracy 0.8778\n",
      "Epoch [3][40]\t Batch [300][550]\t Training Loss 0.4499\t Accuracy 0.8792\n",
      "Epoch [3][40]\t Batch [350][550]\t Training Loss 0.4520\t Accuracy 0.8784\n",
      "Epoch [3][40]\t Batch [400][550]\t Training Loss 0.4500\t Accuracy 0.8787\n",
      "Epoch [3][40]\t Batch [450][550]\t Training Loss 0.4490\t Accuracy 0.8789\n",
      "Epoch [3][40]\t Batch [500][550]\t Training Loss 0.4484\t Accuracy 0.8785\n",
      "\n",
      "Epoch [3]\t Average training loss 0.4460\t Average training accuracy 0.8795\n",
      "Epoch [3]\t Average validation loss 0.3408\t Average validation accuracy 0.9150\n",
      "\n",
      "Epoch [4][40]\t Batch [0][550]\t Training Loss 0.4039\t Accuracy 0.9200\n",
      "Epoch [4][40]\t Batch [50][550]\t Training Loss 0.3980\t Accuracy 0.8955\n",
      "Epoch [4][40]\t Batch [100][550]\t Training Loss 0.4075\t Accuracy 0.8932\n",
      "Epoch [4][40]\t Batch [150][550]\t Training Loss 0.4144\t Accuracy 0.8884\n",
      "Epoch [4][40]\t Batch [200][550]\t Training Loss 0.4130\t Accuracy 0.8887\n",
      "Epoch [4][40]\t Batch [250][550]\t Training Loss 0.4107\t Accuracy 0.8892\n",
      "Epoch [4][40]\t Batch [300][550]\t Training Loss 0.4089\t Accuracy 0.8898\n",
      "Epoch [4][40]\t Batch [350][550]\t Training Loss 0.4115\t Accuracy 0.8889\n",
      "Epoch [4][40]\t Batch [400][550]\t Training Loss 0.4103\t Accuracy 0.8888\n",
      "Epoch [4][40]\t Batch [450][550]\t Training Loss 0.4100\t Accuracy 0.8887\n",
      "Epoch [4][40]\t Batch [500][550]\t Training Loss 0.4102\t Accuracy 0.8881\n",
      "\n",
      "Epoch [4]\t Average training loss 0.4086\t Average training accuracy 0.8887\n",
      "Epoch [4]\t Average validation loss 0.3140\t Average validation accuracy 0.9212\n",
      "\n",
      "Epoch [5][40]\t Batch [0][550]\t Training Loss 0.3742\t Accuracy 0.9300\n",
      "Epoch [5][40]\t Batch [50][550]\t Training Loss 0.3678\t Accuracy 0.9022\n",
      "Epoch [5][40]\t Batch [100][550]\t Training Loss 0.3780\t Accuracy 0.8993\n",
      "Epoch [5][40]\t Batch [150][550]\t Training Loss 0.3858\t Accuracy 0.8947\n",
      "Epoch [5][40]\t Batch [200][550]\t Training Loss 0.3843\t Accuracy 0.8951\n",
      "Epoch [5][40]\t Batch [250][550]\t Training Loss 0.3827\t Accuracy 0.8957\n",
      "Epoch [5][40]\t Batch [300][550]\t Training Loss 0.3815\t Accuracy 0.8957\n",
      "Epoch [5][40]\t Batch [350][550]\t Training Loss 0.3843\t Accuracy 0.8950\n",
      "Epoch [5][40]\t Batch [400][550]\t Training Loss 0.3835\t Accuracy 0.8948\n",
      "Epoch [5][40]\t Batch [450][550]\t Training Loss 0.3836\t Accuracy 0.8947\n",
      "Epoch [5][40]\t Batch [500][550]\t Training Loss 0.3841\t Accuracy 0.8940\n",
      "\n",
      "Epoch [5]\t Average training loss 0.3831\t Average training accuracy 0.8944\n",
      "Epoch [5]\t Average validation loss 0.2954\t Average validation accuracy 0.9254\n",
      "\n",
      "Epoch [6][40]\t Batch [0][550]\t Training Loss 0.3524\t Accuracy 0.9400\n",
      "Epoch [6][40]\t Batch [50][550]\t Training Loss 0.3460\t Accuracy 0.9069\n",
      "Epoch [6][40]\t Batch [100][550]\t Training Loss 0.3568\t Accuracy 0.9044\n",
      "Epoch [6][40]\t Batch [150][550]\t Training Loss 0.3651\t Accuracy 0.9001\n",
      "Epoch [6][40]\t Batch [200][550]\t Training Loss 0.3634\t Accuracy 0.9004\n",
      "Epoch [6][40]\t Batch [250][550]\t Training Loss 0.3622\t Accuracy 0.9008\n",
      "Epoch [6][40]\t Batch [300][550]\t Training Loss 0.3614\t Accuracy 0.9007\n",
      "Epoch [6][40]\t Batch [350][550]\t Training Loss 0.3643\t Accuracy 0.8998\n",
      "Epoch [6][40]\t Batch [400][550]\t Training Loss 0.3637\t Accuracy 0.8995\n",
      "Epoch [6][40]\t Batch [450][550]\t Training Loss 0.3640\t Accuracy 0.8995\n",
      "Epoch [6][40]\t Batch [500][550]\t Training Loss 0.3648\t Accuracy 0.8988\n",
      "\n",
      "Epoch [6]\t Average training loss 0.3641\t Average training accuracy 0.8989\n",
      "Epoch [6]\t Average validation loss 0.2814\t Average validation accuracy 0.9276\n",
      "\n",
      "Epoch [7][40]\t Batch [0][550]\t Training Loss 0.3352\t Accuracy 0.9400\n",
      "Epoch [7][40]\t Batch [50][550]\t Training Loss 0.3293\t Accuracy 0.9122\n",
      "Epoch [7][40]\t Batch [100][550]\t Training Loss 0.3403\t Accuracy 0.9086\n",
      "Epoch [7][40]\t Batch [150][550]\t Training Loss 0.3490\t Accuracy 0.9045\n",
      "Epoch [7][40]\t Batch [200][550]\t Training Loss 0.3471\t Accuracy 0.9048\n",
      "Epoch [7][40]\t Batch [250][550]\t Training Loss 0.3461\t Accuracy 0.9050\n",
      "Epoch [7][40]\t Batch [300][550]\t Training Loss 0.3456\t Accuracy 0.9049\n",
      "Epoch [7][40]\t Batch [350][550]\t Training Loss 0.3485\t Accuracy 0.9039\n",
      "Epoch [7][40]\t Batch [400][550]\t Training Loss 0.3482\t Accuracy 0.9033\n",
      "Epoch [7][40]\t Batch [450][550]\t Training Loss 0.3486\t Accuracy 0.9034\n",
      "Epoch [7][40]\t Batch [500][550]\t Training Loss 0.3495\t Accuracy 0.9027\n",
      "\n",
      "Epoch [7]\t Average training loss 0.3490\t Average training accuracy 0.9027\n",
      "Epoch [7]\t Average validation loss 0.2703\t Average validation accuracy 0.9308\n",
      "\n",
      "Epoch [8][40]\t Batch [0][550]\t Training Loss 0.3211\t Accuracy 0.9400\n",
      "Epoch [8][40]\t Batch [50][550]\t Training Loss 0.3158\t Accuracy 0.9167\n",
      "Epoch [8][40]\t Batch [100][550]\t Training Loss 0.3269\t Accuracy 0.9117\n",
      "Epoch [8][40]\t Batch [150][550]\t Training Loss 0.3357\t Accuracy 0.9077\n",
      "Epoch [8][40]\t Batch [200][550]\t Training Loss 0.3338\t Accuracy 0.9082\n",
      "Epoch [8][40]\t Batch [250][550]\t Training Loss 0.3330\t Accuracy 0.9082\n",
      "Epoch [8][40]\t Batch [300][550]\t Training Loss 0.3327\t Accuracy 0.9082\n",
      "Epoch [8][40]\t Batch [350][550]\t Training Loss 0.3356\t Accuracy 0.9072\n",
      "Epoch [8][40]\t Batch [400][550]\t Training Loss 0.3354\t Accuracy 0.9067\n",
      "Epoch [8][40]\t Batch [450][550]\t Training Loss 0.3359\t Accuracy 0.9069\n",
      "Epoch [8][40]\t Batch [500][550]\t Training Loss 0.3369\t Accuracy 0.9061\n",
      "\n",
      "Epoch [8]\t Average training loss 0.3366\t Average training accuracy 0.9062\n",
      "Epoch [8]\t Average validation loss 0.2611\t Average validation accuracy 0.9324\n",
      "\n",
      "Epoch [9][40]\t Batch [0][550]\t Training Loss 0.3089\t Accuracy 0.9400\n",
      "Epoch [9][40]\t Batch [50][550]\t Training Loss 0.3045\t Accuracy 0.9188\n",
      "Epoch [9][40]\t Batch [100][550]\t Training Loss 0.3156\t Accuracy 0.9142\n",
      "Epoch [9][40]\t Batch [150][550]\t Training Loss 0.3246\t Accuracy 0.9099\n",
      "Epoch [9][40]\t Batch [200][550]\t Training Loss 0.3225\t Accuracy 0.9108\n",
      "Epoch [9][40]\t Batch [250][550]\t Training Loss 0.3219\t Accuracy 0.9111\n",
      "Epoch [9][40]\t Batch [300][550]\t Training Loss 0.3218\t Accuracy 0.9112\n",
      "Epoch [9][40]\t Batch [350][550]\t Training Loss 0.3246\t Accuracy 0.9103\n",
      "Epoch [9][40]\t Batch [400][550]\t Training Loss 0.3245\t Accuracy 0.9097\n",
      "Epoch [9][40]\t Batch [450][550]\t Training Loss 0.3251\t Accuracy 0.9097\n",
      "Epoch [9][40]\t Batch [500][550]\t Training Loss 0.3261\t Accuracy 0.9090\n",
      "\n",
      "Epoch [9]\t Average training loss 0.3260\t Average training accuracy 0.9089\n",
      "Epoch [9]\t Average validation loss 0.2532\t Average validation accuracy 0.9350\n",
      "\n",
      "Epoch [10][40]\t Batch [0][550]\t Training Loss 0.2982\t Accuracy 0.9400\n",
      "Epoch [10][40]\t Batch [50][550]\t Training Loss 0.2948\t Accuracy 0.9220\n",
      "Epoch [10][40]\t Batch [100][550]\t Training Loss 0.3059\t Accuracy 0.9169\n",
      "Epoch [10][40]\t Batch [150][550]\t Training Loss 0.3149\t Accuracy 0.9124\n",
      "Epoch [10][40]\t Batch [200][550]\t Training Loss 0.3127\t Accuracy 0.9134\n",
      "Epoch [10][40]\t Batch [250][550]\t Training Loss 0.3123\t Accuracy 0.9135\n",
      "Epoch [10][40]\t Batch [300][550]\t Training Loss 0.3123\t Accuracy 0.9136\n",
      "Epoch [10][40]\t Batch [350][550]\t Training Loss 0.3151\t Accuracy 0.9126\n",
      "Epoch [10][40]\t Batch [400][550]\t Training Loss 0.3150\t Accuracy 0.9121\n",
      "Epoch [10][40]\t Batch [450][550]\t Training Loss 0.3157\t Accuracy 0.9121\n",
      "Epoch [10][40]\t Batch [500][550]\t Training Loss 0.3168\t Accuracy 0.9113\n",
      "\n",
      "Epoch [10]\t Average training loss 0.3167\t Average training accuracy 0.9112\n",
      "Epoch [10]\t Average validation loss 0.2464\t Average validation accuracy 0.9364\n",
      "\n",
      "Epoch [11][40]\t Batch [0][550]\t Training Loss 0.2887\t Accuracy 0.9400\n",
      "Epoch [11][40]\t Batch [50][550]\t Training Loss 0.2862\t Accuracy 0.9245\n",
      "Epoch [11][40]\t Batch [100][550]\t Training Loss 0.2974\t Accuracy 0.9198\n",
      "Epoch [11][40]\t Batch [150][550]\t Training Loss 0.3064\t Accuracy 0.9148\n",
      "Epoch [11][40]\t Batch [200][550]\t Training Loss 0.3041\t Accuracy 0.9160\n",
      "Epoch [11][40]\t Batch [250][550]\t Training Loss 0.3039\t Accuracy 0.9160\n",
      "Epoch [11][40]\t Batch [300][550]\t Training Loss 0.3039\t Accuracy 0.9162\n",
      "Epoch [11][40]\t Batch [350][550]\t Training Loss 0.3067\t Accuracy 0.9152\n",
      "Epoch [11][40]\t Batch [400][550]\t Training Loss 0.3067\t Accuracy 0.9148\n",
      "Epoch [11][40]\t Batch [450][550]\t Training Loss 0.3074\t Accuracy 0.9147\n",
      "Epoch [11][40]\t Batch [500][550]\t Training Loss 0.3085\t Accuracy 0.9138\n",
      "\n",
      "Epoch [11]\t Average training loss 0.3085\t Average training accuracy 0.9135\n",
      "Epoch [11]\t Average validation loss 0.2404\t Average validation accuracy 0.9384\n",
      "\n",
      "Epoch [12][40]\t Batch [0][550]\t Training Loss 0.2801\t Accuracy 0.9400\n",
      "Epoch [12][40]\t Batch [50][550]\t Training Loss 0.2787\t Accuracy 0.9265\n",
      "Epoch [12][40]\t Batch [100][550]\t Training Loss 0.2898\t Accuracy 0.9220\n",
      "Epoch [12][40]\t Batch [150][550]\t Training Loss 0.2988\t Accuracy 0.9169\n",
      "Epoch [12][40]\t Batch [200][550]\t Training Loss 0.2965\t Accuracy 0.9184\n",
      "Epoch [12][40]\t Batch [250][550]\t Training Loss 0.2964\t Accuracy 0.9185\n",
      "Epoch [12][40]\t Batch [300][550]\t Training Loss 0.2965\t Accuracy 0.9184\n",
      "Epoch [12][40]\t Batch [350][550]\t Training Loss 0.2991\t Accuracy 0.9175\n",
      "Epoch [12][40]\t Batch [400][550]\t Training Loss 0.2992\t Accuracy 0.9171\n",
      "Epoch [12][40]\t Batch [450][550]\t Training Loss 0.2999\t Accuracy 0.9170\n",
      "Epoch [12][40]\t Batch [500][550]\t Training Loss 0.3011\t Accuracy 0.9161\n",
      "\n",
      "Epoch [12]\t Average training loss 0.3012\t Average training accuracy 0.9159\n",
      "Epoch [12]\t Average validation loss 0.2350\t Average validation accuracy 0.9394\n",
      "\n",
      "Epoch [13][40]\t Batch [0][550]\t Training Loss 0.2721\t Accuracy 0.9500\n",
      "Epoch [13][40]\t Batch [50][550]\t Training Loss 0.2718\t Accuracy 0.9276\n",
      "Epoch [13][40]\t Batch [100][550]\t Training Loss 0.2830\t Accuracy 0.9237\n",
      "Epoch [13][40]\t Batch [150][550]\t Training Loss 0.2919\t Accuracy 0.9186\n",
      "Epoch [13][40]\t Batch [200][550]\t Training Loss 0.2895\t Accuracy 0.9202\n",
      "Epoch [13][40]\t Batch [250][550]\t Training Loss 0.2895\t Accuracy 0.9204\n",
      "Epoch [13][40]\t Batch [300][550]\t Training Loss 0.2897\t Accuracy 0.9203\n",
      "Epoch [13][40]\t Batch [350][550]\t Training Loss 0.2923\t Accuracy 0.9192\n",
      "Epoch [13][40]\t Batch [400][550]\t Training Loss 0.2924\t Accuracy 0.9190\n",
      "Epoch [13][40]\t Batch [450][550]\t Training Loss 0.2932\t Accuracy 0.9189\n",
      "Epoch [13][40]\t Batch [500][550]\t Training Loss 0.2944\t Accuracy 0.9179\n",
      "\n",
      "Epoch [13]\t Average training loss 0.2945\t Average training accuracy 0.9178\n",
      "Epoch [13]\t Average validation loss 0.2301\t Average validation accuracy 0.9406\n",
      "\n",
      "Epoch [14][40]\t Batch [0][550]\t Training Loss 0.2646\t Accuracy 0.9500\n",
      "Epoch [14][40]\t Batch [50][550]\t Training Loss 0.2657\t Accuracy 0.9296\n",
      "Epoch [14][40]\t Batch [100][550]\t Training Loss 0.2768\t Accuracy 0.9252\n",
      "Epoch [14][40]\t Batch [150][550]\t Training Loss 0.2857\t Accuracy 0.9204\n",
      "Epoch [14][40]\t Batch [200][550]\t Training Loss 0.2832\t Accuracy 0.9217\n",
      "Epoch [14][40]\t Batch [250][550]\t Training Loss 0.2833\t Accuracy 0.9220\n",
      "Epoch [14][40]\t Batch [300][550]\t Training Loss 0.2836\t Accuracy 0.9219\n",
      "Epoch [14][40]\t Batch [350][550]\t Training Loss 0.2861\t Accuracy 0.9210\n",
      "Epoch [14][40]\t Batch [400][550]\t Training Loss 0.2862\t Accuracy 0.9208\n",
      "Epoch [14][40]\t Batch [450][550]\t Training Loss 0.2870\t Accuracy 0.9206\n",
      "Epoch [14][40]\t Batch [500][550]\t Training Loss 0.2883\t Accuracy 0.9197\n",
      "\n",
      "Epoch [14]\t Average training loss 0.2884\t Average training accuracy 0.9197\n",
      "Epoch [14]\t Average validation loss 0.2257\t Average validation accuracy 0.9420\n",
      "\n",
      "Epoch [15][40]\t Batch [0][550]\t Training Loss 0.2577\t Accuracy 0.9500\n",
      "Epoch [15][40]\t Batch [50][550]\t Training Loss 0.2600\t Accuracy 0.9312\n",
      "Epoch [15][40]\t Batch [100][550]\t Training Loss 0.2711\t Accuracy 0.9266\n",
      "Epoch [15][40]\t Batch [150][550]\t Training Loss 0.2799\t Accuracy 0.9221\n",
      "Epoch [15][40]\t Batch [200][550]\t Training Loss 0.2775\t Accuracy 0.9233\n",
      "Epoch [15][40]\t Batch [250][550]\t Training Loss 0.2776\t Accuracy 0.9233\n",
      "Epoch [15][40]\t Batch [300][550]\t Training Loss 0.2779\t Accuracy 0.9234\n",
      "Epoch [15][40]\t Batch [350][550]\t Training Loss 0.2804\t Accuracy 0.9226\n",
      "Epoch [15][40]\t Batch [400][550]\t Training Loss 0.2805\t Accuracy 0.9225\n",
      "Epoch [15][40]\t Batch [450][550]\t Training Loss 0.2813\t Accuracy 0.9221\n",
      "Epoch [15][40]\t Batch [500][550]\t Training Loss 0.2826\t Accuracy 0.9213\n",
      "\n",
      "Epoch [15]\t Average training loss 0.2828\t Average training accuracy 0.9213\n",
      "Epoch [15]\t Average validation loss 0.2216\t Average validation accuracy 0.9434\n",
      "\n",
      "Epoch [16][40]\t Batch [0][550]\t Training Loss 0.2517\t Accuracy 0.9500\n",
      "Epoch [16][40]\t Batch [50][550]\t Training Loss 0.2548\t Accuracy 0.9325\n",
      "Epoch [16][40]\t Batch [100][550]\t Training Loss 0.2658\t Accuracy 0.9278\n",
      "Epoch [16][40]\t Batch [150][550]\t Training Loss 0.2746\t Accuracy 0.9239\n",
      "Epoch [16][40]\t Batch [200][550]\t Training Loss 0.2721\t Accuracy 0.9251\n",
      "Epoch [16][40]\t Batch [250][550]\t Training Loss 0.2724\t Accuracy 0.9251\n",
      "Epoch [16][40]\t Batch [300][550]\t Training Loss 0.2727\t Accuracy 0.9252\n",
      "Epoch [16][40]\t Batch [350][550]\t Training Loss 0.2751\t Accuracy 0.9242\n",
      "Epoch [16][40]\t Batch [400][550]\t Training Loss 0.2753\t Accuracy 0.9241\n",
      "Epoch [16][40]\t Batch [450][550]\t Training Loss 0.2761\t Accuracy 0.9238\n",
      "Epoch [16][40]\t Batch [500][550]\t Training Loss 0.2774\t Accuracy 0.9230\n",
      "\n",
      "Epoch [16]\t Average training loss 0.2776\t Average training accuracy 0.9229\n",
      "Epoch [16]\t Average validation loss 0.2178\t Average validation accuracy 0.9442\n",
      "\n",
      "Epoch [17][40]\t Batch [0][550]\t Training Loss 0.2462\t Accuracy 0.9500\n",
      "Epoch [17][40]\t Batch [50][550]\t Training Loss 0.2501\t Accuracy 0.9349\n",
      "Epoch [17][40]\t Batch [100][550]\t Training Loss 0.2610\t Accuracy 0.9292\n",
      "Epoch [17][40]\t Batch [150][550]\t Training Loss 0.2696\t Accuracy 0.9254\n",
      "Epoch [17][40]\t Batch [200][550]\t Training Loss 0.2672\t Accuracy 0.9267\n",
      "Epoch [17][40]\t Batch [250][550]\t Training Loss 0.2675\t Accuracy 0.9267\n",
      "Epoch [17][40]\t Batch [300][550]\t Training Loss 0.2679\t Accuracy 0.9268\n",
      "Epoch [17][40]\t Batch [350][550]\t Training Loss 0.2702\t Accuracy 0.9258\n",
      "Epoch [17][40]\t Batch [400][550]\t Training Loss 0.2704\t Accuracy 0.9257\n",
      "Epoch [17][40]\t Batch [450][550]\t Training Loss 0.2712\t Accuracy 0.9254\n",
      "Epoch [17][40]\t Batch [500][550]\t Training Loss 0.2726\t Accuracy 0.9245\n",
      "\n",
      "Epoch [17]\t Average training loss 0.2728\t Average training accuracy 0.9245\n",
      "Epoch [17]\t Average validation loss 0.2143\t Average validation accuracy 0.9458\n",
      "\n",
      "Epoch [18][40]\t Batch [0][550]\t Training Loss 0.2412\t Accuracy 0.9500\n",
      "Epoch [18][40]\t Batch [50][550]\t Training Loss 0.2456\t Accuracy 0.9363\n",
      "Epoch [18][40]\t Batch [100][550]\t Training Loss 0.2565\t Accuracy 0.9311\n",
      "Epoch [18][40]\t Batch [150][550]\t Training Loss 0.2650\t Accuracy 0.9270\n",
      "Epoch [18][40]\t Batch [200][550]\t Training Loss 0.2626\t Accuracy 0.9282\n",
      "Epoch [18][40]\t Batch [250][550]\t Training Loss 0.2630\t Accuracy 0.9280\n",
      "Epoch [18][40]\t Batch [300][550]\t Training Loss 0.2634\t Accuracy 0.9282\n",
      "Epoch [18][40]\t Batch [350][550]\t Training Loss 0.2656\t Accuracy 0.9272\n",
      "Epoch [18][40]\t Batch [400][550]\t Training Loss 0.2658\t Accuracy 0.9271\n",
      "Epoch [18][40]\t Batch [450][550]\t Training Loss 0.2667\t Accuracy 0.9268\n",
      "Epoch [18][40]\t Batch [500][550]\t Training Loss 0.2681\t Accuracy 0.9259\n",
      "\n",
      "Epoch [18]\t Average training loss 0.2683\t Average training accuracy 0.9260\n",
      "Epoch [18]\t Average validation loss 0.2109\t Average validation accuracy 0.9466\n",
      "\n",
      "Epoch [19][40]\t Batch [0][550]\t Training Loss 0.2366\t Accuracy 0.9500\n",
      "Epoch [19][40]\t Batch [50][550]\t Training Loss 0.2414\t Accuracy 0.9373\n",
      "Epoch [19][40]\t Batch [100][550]\t Training Loss 0.2523\t Accuracy 0.9320\n",
      "Epoch [19][40]\t Batch [150][550]\t Training Loss 0.2607\t Accuracy 0.9286\n",
      "Epoch [19][40]\t Batch [200][550]\t Training Loss 0.2583\t Accuracy 0.9297\n",
      "Epoch [19][40]\t Batch [250][550]\t Training Loss 0.2587\t Accuracy 0.9295\n",
      "Epoch [19][40]\t Batch [300][550]\t Training Loss 0.2591\t Accuracy 0.9296\n",
      "Epoch [19][40]\t Batch [350][550]\t Training Loss 0.2614\t Accuracy 0.9286\n",
      "Epoch [19][40]\t Batch [400][550]\t Training Loss 0.2616\t Accuracy 0.9286\n",
      "Epoch [19][40]\t Batch [450][550]\t Training Loss 0.2624\t Accuracy 0.9282\n",
      "Epoch [19][40]\t Batch [500][550]\t Training Loss 0.2638\t Accuracy 0.9274\n",
      "\n",
      "Epoch [19]\t Average training loss 0.2641\t Average training accuracy 0.9273\n",
      "Epoch [19]\t Average validation loss 0.2078\t Average validation accuracy 0.9464\n",
      "\n",
      "Epoch [20][40]\t Batch [0][550]\t Training Loss 0.2324\t Accuracy 0.9500\n",
      "Epoch [20][40]\t Batch [50][550]\t Training Loss 0.2375\t Accuracy 0.9382\n",
      "Epoch [20][40]\t Batch [100][550]\t Training Loss 0.2483\t Accuracy 0.9332\n",
      "Epoch [20][40]\t Batch [150][550]\t Training Loss 0.2567\t Accuracy 0.9298\n",
      "Epoch [20][40]\t Batch [200][550]\t Training Loss 0.2542\t Accuracy 0.9308\n",
      "Epoch [20][40]\t Batch [250][550]\t Training Loss 0.2547\t Accuracy 0.9307\n",
      "Epoch [20][40]\t Batch [300][550]\t Training Loss 0.2552\t Accuracy 0.9307\n",
      "Epoch [20][40]\t Batch [350][550]\t Training Loss 0.2573\t Accuracy 0.9297\n",
      "Epoch [20][40]\t Batch [400][550]\t Training Loss 0.2575\t Accuracy 0.9297\n",
      "Epoch [20][40]\t Batch [450][550]\t Training Loss 0.2584\t Accuracy 0.9294\n",
      "Epoch [20][40]\t Batch [500][550]\t Training Loss 0.2598\t Accuracy 0.9286\n",
      "\n",
      "Epoch [20]\t Average training loss 0.2601\t Average training accuracy 0.9285\n",
      "Epoch [20]\t Average validation loss 0.2049\t Average validation accuracy 0.9472\n",
      "\n",
      "Epoch [21][40]\t Batch [0][550]\t Training Loss 0.2284\t Accuracy 0.9500\n",
      "Epoch [21][40]\t Batch [50][550]\t Training Loss 0.2338\t Accuracy 0.9398\n",
      "Epoch [21][40]\t Batch [100][550]\t Training Loss 0.2445\t Accuracy 0.9348\n",
      "Epoch [21][40]\t Batch [150][550]\t Training Loss 0.2528\t Accuracy 0.9315\n",
      "Epoch [21][40]\t Batch [200][550]\t Training Loss 0.2504\t Accuracy 0.9323\n",
      "Epoch [21][40]\t Batch [250][550]\t Training Loss 0.2509\t Accuracy 0.9320\n",
      "Epoch [21][40]\t Batch [300][550]\t Training Loss 0.2514\t Accuracy 0.9319\n",
      "Epoch [21][40]\t Batch [350][550]\t Training Loss 0.2535\t Accuracy 0.9309\n",
      "Epoch [21][40]\t Batch [400][550]\t Training Loss 0.2537\t Accuracy 0.9308\n",
      "Epoch [21][40]\t Batch [450][550]\t Training Loss 0.2546\t Accuracy 0.9305\n",
      "Epoch [21][40]\t Batch [500][550]\t Training Loss 0.2560\t Accuracy 0.9296\n",
      "\n",
      "Epoch [21]\t Average training loss 0.2563\t Average training accuracy 0.9296\n",
      "Epoch [21]\t Average validation loss 0.2021\t Average validation accuracy 0.9480\n",
      "\n",
      "Epoch [22][40]\t Batch [0][550]\t Training Loss 0.2246\t Accuracy 0.9500\n",
      "Epoch [22][40]\t Batch [50][550]\t Training Loss 0.2303\t Accuracy 0.9406\n",
      "Epoch [22][40]\t Batch [100][550]\t Training Loss 0.2410\t Accuracy 0.9357\n",
      "Epoch [22][40]\t Batch [150][550]\t Training Loss 0.2491\t Accuracy 0.9327\n",
      "Epoch [22][40]\t Batch [200][550]\t Training Loss 0.2467\t Accuracy 0.9335\n",
      "Epoch [22][40]\t Batch [250][550]\t Training Loss 0.2473\t Accuracy 0.9330\n",
      "Epoch [22][40]\t Batch [300][550]\t Training Loss 0.2478\t Accuracy 0.9330\n",
      "Epoch [22][40]\t Batch [350][550]\t Training Loss 0.2499\t Accuracy 0.9320\n",
      "Epoch [22][40]\t Batch [400][550]\t Training Loss 0.2501\t Accuracy 0.9320\n",
      "Epoch [22][40]\t Batch [450][550]\t Training Loss 0.2510\t Accuracy 0.9318\n",
      "Epoch [22][40]\t Batch [500][550]\t Training Loss 0.2524\t Accuracy 0.9309\n",
      "\n",
      "Epoch [22]\t Average training loss 0.2527\t Average training accuracy 0.9309\n",
      "Epoch [22]\t Average validation loss 0.1995\t Average validation accuracy 0.9490\n",
      "\n",
      "Epoch [23][40]\t Batch [0][550]\t Training Loss 0.2210\t Accuracy 0.9500\n",
      "Epoch [23][40]\t Batch [50][550]\t Training Loss 0.2270\t Accuracy 0.9412\n",
      "Epoch [23][40]\t Batch [100][550]\t Training Loss 0.2376\t Accuracy 0.9363\n",
      "Epoch [23][40]\t Batch [150][550]\t Training Loss 0.2457\t Accuracy 0.9335\n",
      "Epoch [23][40]\t Batch [200][550]\t Training Loss 0.2433\t Accuracy 0.9343\n",
      "Epoch [23][40]\t Batch [250][550]\t Training Loss 0.2439\t Accuracy 0.9340\n",
      "Epoch [23][40]\t Batch [300][550]\t Training Loss 0.2444\t Accuracy 0.9340\n",
      "Epoch [23][40]\t Batch [350][550]\t Training Loss 0.2464\t Accuracy 0.9330\n",
      "Epoch [23][40]\t Batch [400][550]\t Training Loss 0.2467\t Accuracy 0.9330\n",
      "Epoch [23][40]\t Batch [450][550]\t Training Loss 0.2476\t Accuracy 0.9329\n",
      "Epoch [23][40]\t Batch [500][550]\t Training Loss 0.2490\t Accuracy 0.9320\n",
      "\n",
      "Epoch [23]\t Average training loss 0.2493\t Average training accuracy 0.9320\n",
      "Epoch [23]\t Average validation loss 0.1970\t Average validation accuracy 0.9502\n",
      "\n",
      "Epoch [24][40]\t Batch [0][550]\t Training Loss 0.2176\t Accuracy 0.9500\n",
      "Epoch [24][40]\t Batch [50][550]\t Training Loss 0.2238\t Accuracy 0.9416\n",
      "Epoch [24][40]\t Batch [100][550]\t Training Loss 0.2344\t Accuracy 0.9365\n",
      "Epoch [24][40]\t Batch [150][550]\t Training Loss 0.2424\t Accuracy 0.9338\n",
      "Epoch [24][40]\t Batch [200][550]\t Training Loss 0.2400\t Accuracy 0.9349\n",
      "Epoch [24][40]\t Batch [250][550]\t Training Loss 0.2406\t Accuracy 0.9346\n",
      "Epoch [24][40]\t Batch [300][550]\t Training Loss 0.2412\t Accuracy 0.9347\n",
      "Epoch [24][40]\t Batch [350][550]\t Training Loss 0.2431\t Accuracy 0.9338\n",
      "Epoch [24][40]\t Batch [400][550]\t Training Loss 0.2434\t Accuracy 0.9338\n",
      "Epoch [24][40]\t Batch [450][550]\t Training Loss 0.2443\t Accuracy 0.9336\n",
      "Epoch [24][40]\t Batch [500][550]\t Training Loss 0.2458\t Accuracy 0.9328\n",
      "\n",
      "Epoch [24]\t Average training loss 0.2460\t Average training accuracy 0.9328\n",
      "Epoch [24]\t Average validation loss 0.1946\t Average validation accuracy 0.9506\n",
      "\n",
      "Epoch [25][40]\t Batch [0][550]\t Training Loss 0.2144\t Accuracy 0.9500\n",
      "Epoch [25][40]\t Batch [50][550]\t Training Loss 0.2208\t Accuracy 0.9427\n",
      "Epoch [25][40]\t Batch [100][550]\t Training Loss 0.2313\t Accuracy 0.9377\n",
      "Epoch [25][40]\t Batch [150][550]\t Training Loss 0.2392\t Accuracy 0.9350\n",
      "Epoch [25][40]\t Batch [200][550]\t Training Loss 0.2368\t Accuracy 0.9359\n",
      "Epoch [25][40]\t Batch [250][550]\t Training Loss 0.2375\t Accuracy 0.9355\n",
      "Epoch [25][40]\t Batch [300][550]\t Training Loss 0.2381\t Accuracy 0.9355\n",
      "Epoch [25][40]\t Batch [350][550]\t Training Loss 0.2400\t Accuracy 0.9346\n",
      "Epoch [25][40]\t Batch [400][550]\t Training Loss 0.2402\t Accuracy 0.9346\n",
      "Epoch [25][40]\t Batch [450][550]\t Training Loss 0.2412\t Accuracy 0.9343\n",
      "Epoch [25][40]\t Batch [500][550]\t Training Loss 0.2426\t Accuracy 0.9336\n",
      "\n",
      "Epoch [25]\t Average training loss 0.2429\t Average training accuracy 0.9336\n",
      "Epoch [25]\t Average validation loss 0.1924\t Average validation accuracy 0.9512\n",
      "\n",
      "Epoch [26][40]\t Batch [0][550]\t Training Loss 0.2112\t Accuracy 0.9500\n",
      "Epoch [26][40]\t Batch [50][550]\t Training Loss 0.2180\t Accuracy 0.9441\n",
      "Epoch [26][40]\t Batch [100][550]\t Training Loss 0.2284\t Accuracy 0.9385\n",
      "Epoch [26][40]\t Batch [150][550]\t Training Loss 0.2362\t Accuracy 0.9360\n",
      "Epoch [26][40]\t Batch [200][550]\t Training Loss 0.2338\t Accuracy 0.9368\n",
      "Epoch [26][40]\t Batch [250][550]\t Training Loss 0.2345\t Accuracy 0.9363\n",
      "Epoch [26][40]\t Batch [300][550]\t Training Loss 0.2351\t Accuracy 0.9363\n",
      "Epoch [26][40]\t Batch [350][550]\t Training Loss 0.2370\t Accuracy 0.9355\n",
      "Epoch [26][40]\t Batch [400][550]\t Training Loss 0.2373\t Accuracy 0.9354\n",
      "Epoch [26][40]\t Batch [450][550]\t Training Loss 0.2382\t Accuracy 0.9353\n",
      "Epoch [26][40]\t Batch [500][550]\t Training Loss 0.2397\t Accuracy 0.9346\n",
      "\n",
      "Epoch [26]\t Average training loss 0.2399\t Average training accuracy 0.9345\n",
      "Epoch [26]\t Average validation loss 0.1902\t Average validation accuracy 0.9524\n",
      "\n",
      "Epoch [27][40]\t Batch [0][550]\t Training Loss 0.2083\t Accuracy 0.9500\n",
      "Epoch [27][40]\t Batch [50][550]\t Training Loss 0.2152\t Accuracy 0.9453\n",
      "Epoch [27][40]\t Batch [100][550]\t Training Loss 0.2256\t Accuracy 0.9395\n",
      "Epoch [27][40]\t Batch [150][550]\t Training Loss 0.2333\t Accuracy 0.9369\n",
      "Epoch [27][40]\t Batch [200][550]\t Training Loss 0.2310\t Accuracy 0.9376\n",
      "Epoch [27][40]\t Batch [250][550]\t Training Loss 0.2317\t Accuracy 0.9371\n",
      "Epoch [27][40]\t Batch [300][550]\t Training Loss 0.2323\t Accuracy 0.9371\n",
      "Epoch [27][40]\t Batch [350][550]\t Training Loss 0.2341\t Accuracy 0.9363\n",
      "Epoch [27][40]\t Batch [400][550]\t Training Loss 0.2344\t Accuracy 0.9363\n",
      "Epoch [27][40]\t Batch [450][550]\t Training Loss 0.2353\t Accuracy 0.9361\n",
      "Epoch [27][40]\t Batch [500][550]\t Training Loss 0.2368\t Accuracy 0.9354\n",
      "\n",
      "Epoch [27]\t Average training loss 0.2371\t Average training accuracy 0.9354\n",
      "Epoch [27]\t Average validation loss 0.1881\t Average validation accuracy 0.9534\n",
      "\n",
      "Epoch [28][40]\t Batch [0][550]\t Training Loss 0.2055\t Accuracy 0.9500\n",
      "Epoch [28][40]\t Batch [50][550]\t Training Loss 0.2126\t Accuracy 0.9459\n",
      "Epoch [28][40]\t Batch [100][550]\t Training Loss 0.2230\t Accuracy 0.9403\n",
      "Epoch [28][40]\t Batch [150][550]\t Training Loss 0.2306\t Accuracy 0.9377\n",
      "Epoch [28][40]\t Batch [200][550]\t Training Loss 0.2283\t Accuracy 0.9383\n",
      "Epoch [28][40]\t Batch [250][550]\t Training Loss 0.2290\t Accuracy 0.9379\n",
      "Epoch [28][40]\t Batch [300][550]\t Training Loss 0.2296\t Accuracy 0.9378\n",
      "Epoch [28][40]\t Batch [350][550]\t Training Loss 0.2314\t Accuracy 0.9370\n",
      "Epoch [28][40]\t Batch [400][550]\t Training Loss 0.2317\t Accuracy 0.9370\n",
      "Epoch [28][40]\t Batch [450][550]\t Training Loss 0.2326\t Accuracy 0.9369\n",
      "Epoch [28][40]\t Batch [500][550]\t Training Loss 0.2341\t Accuracy 0.9362\n",
      "\n",
      "Epoch [28]\t Average training loss 0.2344\t Average training accuracy 0.9362\n",
      "Epoch [28]\t Average validation loss 0.1862\t Average validation accuracy 0.9536\n",
      "\n",
      "Epoch [29][40]\t Batch [0][550]\t Training Loss 0.2027\t Accuracy 0.9500\n",
      "Epoch [29][40]\t Batch [50][550]\t Training Loss 0.2101\t Accuracy 0.9459\n",
      "Epoch [29][40]\t Batch [100][550]\t Training Loss 0.2204\t Accuracy 0.9405\n",
      "Epoch [29][40]\t Batch [150][550]\t Training Loss 0.2279\t Accuracy 0.9380\n",
      "Epoch [29][40]\t Batch [200][550]\t Training Loss 0.2256\t Accuracy 0.9388\n",
      "Epoch [29][40]\t Batch [250][550]\t Training Loss 0.2263\t Accuracy 0.9385\n",
      "Epoch [29][40]\t Batch [300][550]\t Training Loss 0.2270\t Accuracy 0.9385\n",
      "Epoch [29][40]\t Batch [350][550]\t Training Loss 0.2288\t Accuracy 0.9377\n",
      "Epoch [29][40]\t Batch [400][550]\t Training Loss 0.2290\t Accuracy 0.9377\n",
      "Epoch [29][40]\t Batch [450][550]\t Training Loss 0.2299\t Accuracy 0.9375\n",
      "Epoch [29][40]\t Batch [500][550]\t Training Loss 0.2315\t Accuracy 0.9369\n",
      "\n",
      "Epoch [29]\t Average training loss 0.2318\t Average training accuracy 0.9368\n",
      "Epoch [29]\t Average validation loss 0.1843\t Average validation accuracy 0.9542\n",
      "\n",
      "Epoch [30][40]\t Batch [0][550]\t Training Loss 0.2001\t Accuracy 0.9500\n",
      "Epoch [30][40]\t Batch [50][550]\t Training Loss 0.2077\t Accuracy 0.9463\n",
      "Epoch [30][40]\t Batch [100][550]\t Training Loss 0.2180\t Accuracy 0.9412\n",
      "Epoch [30][40]\t Batch [150][550]\t Training Loss 0.2254\t Accuracy 0.9389\n",
      "Epoch [30][40]\t Batch [200][550]\t Training Loss 0.2231\t Accuracy 0.9396\n",
      "Epoch [30][40]\t Batch [250][550]\t Training Loss 0.2238\t Accuracy 0.9392\n",
      "Epoch [30][40]\t Batch [300][550]\t Training Loss 0.2245\t Accuracy 0.9393\n",
      "Epoch [30][40]\t Batch [350][550]\t Training Loss 0.2262\t Accuracy 0.9384\n",
      "Epoch [30][40]\t Batch [400][550]\t Training Loss 0.2265\t Accuracy 0.9385\n",
      "Epoch [30][40]\t Batch [450][550]\t Training Loss 0.2274\t Accuracy 0.9383\n",
      "Epoch [30][40]\t Batch [500][550]\t Training Loss 0.2289\t Accuracy 0.9377\n",
      "\n",
      "Epoch [30]\t Average training loss 0.2292\t Average training accuracy 0.9376\n",
      "Epoch [30]\t Average validation loss 0.1825\t Average validation accuracy 0.9546\n",
      "\n",
      "Epoch [31][40]\t Batch [0][550]\t Training Loss 0.1977\t Accuracy 0.9500\n",
      "Epoch [31][40]\t Batch [50][550]\t Training Loss 0.2054\t Accuracy 0.9467\n",
      "Epoch [31][40]\t Batch [100][550]\t Training Loss 0.2157\t Accuracy 0.9420\n",
      "Epoch [31][40]\t Batch [150][550]\t Training Loss 0.2229\t Accuracy 0.9395\n",
      "Epoch [31][40]\t Batch [200][550]\t Training Loss 0.2207\t Accuracy 0.9403\n",
      "Epoch [31][40]\t Batch [250][550]\t Training Loss 0.2214\t Accuracy 0.9398\n",
      "Epoch [31][40]\t Batch [300][550]\t Training Loss 0.2221\t Accuracy 0.9397\n",
      "Epoch [31][40]\t Batch [350][550]\t Training Loss 0.2238\t Accuracy 0.9389\n",
      "Epoch [31][40]\t Batch [400][550]\t Training Loss 0.2241\t Accuracy 0.9390\n",
      "Epoch [31][40]\t Batch [450][550]\t Training Loss 0.2250\t Accuracy 0.9390\n",
      "Epoch [31][40]\t Batch [500][550]\t Training Loss 0.2265\t Accuracy 0.9384\n",
      "\n",
      "Epoch [31]\t Average training loss 0.2268\t Average training accuracy 0.9382\n",
      "Epoch [31]\t Average validation loss 0.1807\t Average validation accuracy 0.9552\n",
      "\n",
      "Epoch [32][40]\t Batch [0][550]\t Training Loss 0.1953\t Accuracy 0.9500\n",
      "Epoch [32][40]\t Batch [50][550]\t Training Loss 0.2032\t Accuracy 0.9473\n",
      "Epoch [32][40]\t Batch [100][550]\t Training Loss 0.2134\t Accuracy 0.9430\n",
      "Epoch [32][40]\t Batch [150][550]\t Training Loss 0.2206\t Accuracy 0.9403\n",
      "Epoch [32][40]\t Batch [200][550]\t Training Loss 0.2183\t Accuracy 0.9410\n",
      "Epoch [32][40]\t Batch [250][550]\t Training Loss 0.2191\t Accuracy 0.9405\n",
      "Epoch [32][40]\t Batch [300][550]\t Training Loss 0.2198\t Accuracy 0.9403\n",
      "Epoch [32][40]\t Batch [350][550]\t Training Loss 0.2215\t Accuracy 0.9394\n",
      "Epoch [32][40]\t Batch [400][550]\t Training Loss 0.2217\t Accuracy 0.9396\n",
      "Epoch [32][40]\t Batch [450][550]\t Training Loss 0.2226\t Accuracy 0.9395\n",
      "Epoch [32][40]\t Batch [500][550]\t Training Loss 0.2242\t Accuracy 0.9390\n",
      "\n",
      "Epoch [32]\t Average training loss 0.2244\t Average training accuracy 0.9388\n",
      "Epoch [32]\t Average validation loss 0.1791\t Average validation accuracy 0.9556\n",
      "\n",
      "Epoch [33][40]\t Batch [0][550]\t Training Loss 0.1930\t Accuracy 0.9500\n",
      "Epoch [33][40]\t Batch [50][550]\t Training Loss 0.2010\t Accuracy 0.9478\n",
      "Epoch [33][40]\t Batch [100][550]\t Training Loss 0.2112\t Accuracy 0.9436\n",
      "Epoch [33][40]\t Batch [150][550]\t Training Loss 0.2183\t Accuracy 0.9409\n",
      "Epoch [33][40]\t Batch [200][550]\t Training Loss 0.2161\t Accuracy 0.9416\n",
      "Epoch [33][40]\t Batch [250][550]\t Training Loss 0.2168\t Accuracy 0.9410\n",
      "Epoch [33][40]\t Batch [300][550]\t Training Loss 0.2176\t Accuracy 0.9408\n",
      "Epoch [33][40]\t Batch [350][550]\t Training Loss 0.2192\t Accuracy 0.9401\n",
      "Epoch [33][40]\t Batch [400][550]\t Training Loss 0.2194\t Accuracy 0.9401\n",
      "Epoch [33][40]\t Batch [450][550]\t Training Loss 0.2203\t Accuracy 0.9401\n",
      "Epoch [33][40]\t Batch [500][550]\t Training Loss 0.2219\t Accuracy 0.9395\n",
      "\n",
      "Epoch [33]\t Average training loss 0.2222\t Average training accuracy 0.9394\n",
      "Epoch [33]\t Average validation loss 0.1775\t Average validation accuracy 0.9560\n",
      "\n",
      "Epoch [34][40]\t Batch [0][550]\t Training Loss 0.1907\t Accuracy 0.9500\n",
      "Epoch [34][40]\t Batch [50][550]\t Training Loss 0.1989\t Accuracy 0.9480\n",
      "Epoch [34][40]\t Batch [100][550]\t Training Loss 0.2091\t Accuracy 0.9440\n",
      "Epoch [34][40]\t Batch [150][550]\t Training Loss 0.2161\t Accuracy 0.9411\n",
      "Epoch [34][40]\t Batch [200][550]\t Training Loss 0.2139\t Accuracy 0.9419\n",
      "Epoch [34][40]\t Batch [250][550]\t Training Loss 0.2147\t Accuracy 0.9413\n",
      "Epoch [34][40]\t Batch [300][550]\t Training Loss 0.2154\t Accuracy 0.9412\n",
      "Epoch [34][40]\t Batch [350][550]\t Training Loss 0.2170\t Accuracy 0.9405\n",
      "Epoch [34][40]\t Batch [400][550]\t Training Loss 0.2172\t Accuracy 0.9406\n",
      "Epoch [34][40]\t Batch [450][550]\t Training Loss 0.2181\t Accuracy 0.9406\n",
      "Epoch [34][40]\t Batch [500][550]\t Training Loss 0.2197\t Accuracy 0.9400\n",
      "\n",
      "Epoch [34]\t Average training loss 0.2200\t Average training accuracy 0.9399\n",
      "Epoch [34]\t Average validation loss 0.1759\t Average validation accuracy 0.9574\n",
      "\n",
      "Epoch [35][40]\t Batch [0][550]\t Training Loss 0.1884\t Accuracy 0.9600\n",
      "Epoch [35][40]\t Batch [50][550]\t Training Loss 0.1969\t Accuracy 0.9486\n",
      "Epoch [35][40]\t Batch [100][550]\t Training Loss 0.2071\t Accuracy 0.9448\n",
      "Epoch [35][40]\t Batch [150][550]\t Training Loss 0.2140\t Accuracy 0.9419\n",
      "Epoch [35][40]\t Batch [200][550]\t Training Loss 0.2118\t Accuracy 0.9427\n",
      "Epoch [35][40]\t Batch [250][550]\t Training Loss 0.2126\t Accuracy 0.9422\n",
      "Epoch [35][40]\t Batch [300][550]\t Training Loss 0.2133\t Accuracy 0.9420\n",
      "Epoch [35][40]\t Batch [350][550]\t Training Loss 0.2149\t Accuracy 0.9412\n",
      "Epoch [35][40]\t Batch [400][550]\t Training Loss 0.2151\t Accuracy 0.9413\n",
      "Epoch [35][40]\t Batch [450][550]\t Training Loss 0.2160\t Accuracy 0.9414\n",
      "Epoch [35][40]\t Batch [500][550]\t Training Loss 0.2176\t Accuracy 0.9408\n",
      "\n",
      "Epoch [35]\t Average training loss 0.2179\t Average training accuracy 0.9407\n",
      "Epoch [35]\t Average validation loss 0.1744\t Average validation accuracy 0.9582\n",
      "\n",
      "Epoch [36][40]\t Batch [0][550]\t Training Loss 0.1862\t Accuracy 0.9600\n",
      "Epoch [36][40]\t Batch [50][550]\t Training Loss 0.1950\t Accuracy 0.9492\n",
      "Epoch [36][40]\t Batch [100][550]\t Training Loss 0.2051\t Accuracy 0.9454\n",
      "Epoch [36][40]\t Batch [150][550]\t Training Loss 0.2119\t Accuracy 0.9425\n",
      "Epoch [36][40]\t Batch [200][550]\t Training Loss 0.2098\t Accuracy 0.9431\n",
      "Epoch [36][40]\t Batch [250][550]\t Training Loss 0.2105\t Accuracy 0.9426\n",
      "Epoch [36][40]\t Batch [300][550]\t Training Loss 0.2113\t Accuracy 0.9424\n",
      "Epoch [36][40]\t Batch [350][550]\t Training Loss 0.2128\t Accuracy 0.9417\n",
      "Epoch [36][40]\t Batch [400][550]\t Training Loss 0.2131\t Accuracy 0.9418\n",
      "Epoch [36][40]\t Batch [450][550]\t Training Loss 0.2140\t Accuracy 0.9419\n",
      "Epoch [36][40]\t Batch [500][550]\t Training Loss 0.2155\t Accuracy 0.9413\n",
      "\n",
      "Epoch [36]\t Average training loss 0.2158\t Average training accuracy 0.9412\n",
      "Epoch [36]\t Average validation loss 0.1729\t Average validation accuracy 0.9586\n",
      "\n",
      "Epoch [37][40]\t Batch [0][550]\t Training Loss 0.1841\t Accuracy 0.9600\n",
      "Epoch [37][40]\t Batch [50][550]\t Training Loss 0.1931\t Accuracy 0.9502\n",
      "Epoch [37][40]\t Batch [100][550]\t Training Loss 0.2032\t Accuracy 0.9459\n",
      "Epoch [37][40]\t Batch [150][550]\t Training Loss 0.2099\t Accuracy 0.9430\n",
      "Epoch [37][40]\t Batch [200][550]\t Training Loss 0.2078\t Accuracy 0.9438\n",
      "Epoch [37][40]\t Batch [250][550]\t Training Loss 0.2085\t Accuracy 0.9433\n",
      "Epoch [37][40]\t Batch [300][550]\t Training Loss 0.2093\t Accuracy 0.9430\n",
      "Epoch [37][40]\t Batch [350][550]\t Training Loss 0.2108\t Accuracy 0.9423\n",
      "Epoch [37][40]\t Batch [400][550]\t Training Loss 0.2111\t Accuracy 0.9425\n",
      "Epoch [37][40]\t Batch [450][550]\t Training Loss 0.2120\t Accuracy 0.9425\n",
      "Epoch [37][40]\t Batch [500][550]\t Training Loss 0.2135\t Accuracy 0.9419\n",
      "\n",
      "Epoch [37]\t Average training loss 0.2138\t Average training accuracy 0.9418\n",
      "Epoch [37]\t Average validation loss 0.1715\t Average validation accuracy 0.9598\n",
      "\n",
      "Epoch [38][40]\t Batch [0][550]\t Training Loss 0.1821\t Accuracy 0.9600\n",
      "Epoch [38][40]\t Batch [50][550]\t Training Loss 0.1913\t Accuracy 0.9506\n",
      "Epoch [38][40]\t Batch [100][550]\t Training Loss 0.2013\t Accuracy 0.9464\n",
      "Epoch [38][40]\t Batch [150][550]\t Training Loss 0.2080\t Accuracy 0.9434\n",
      "Epoch [38][40]\t Batch [200][550]\t Training Loss 0.2059\t Accuracy 0.9441\n",
      "Epoch [38][40]\t Batch [250][550]\t Training Loss 0.2066\t Accuracy 0.9435\n",
      "Epoch [38][40]\t Batch [300][550]\t Training Loss 0.2074\t Accuracy 0.9434\n",
      "Epoch [38][40]\t Batch [350][550]\t Training Loss 0.2089\t Accuracy 0.9426\n",
      "Epoch [38][40]\t Batch [400][550]\t Training Loss 0.2091\t Accuracy 0.9429\n",
      "Epoch [38][40]\t Batch [450][550]\t Training Loss 0.2100\t Accuracy 0.9431\n",
      "Epoch [38][40]\t Batch [500][550]\t Training Loss 0.2116\t Accuracy 0.9425\n",
      "\n",
      "Epoch [38]\t Average training loss 0.2119\t Average training accuracy 0.9423\n",
      "Epoch [38]\t Average validation loss 0.1702\t Average validation accuracy 0.9600\n",
      "\n",
      "Epoch [39][40]\t Batch [0][550]\t Training Loss 0.1801\t Accuracy 0.9600\n",
      "Epoch [39][40]\t Batch [50][550]\t Training Loss 0.1896\t Accuracy 0.9518\n",
      "Epoch [39][40]\t Batch [100][550]\t Training Loss 0.1995\t Accuracy 0.9472\n",
      "Epoch [39][40]\t Batch [150][550]\t Training Loss 0.2061\t Accuracy 0.9442\n",
      "Epoch [39][40]\t Batch [200][550]\t Training Loss 0.2040\t Accuracy 0.9449\n",
      "Epoch [39][40]\t Batch [250][550]\t Training Loss 0.2048\t Accuracy 0.9444\n",
      "Epoch [39][40]\t Batch [300][550]\t Training Loss 0.2056\t Accuracy 0.9442\n",
      "Epoch [39][40]\t Batch [350][550]\t Training Loss 0.2070\t Accuracy 0.9434\n",
      "Epoch [39][40]\t Batch [400][550]\t Training Loss 0.2072\t Accuracy 0.9437\n",
      "Epoch [39][40]\t Batch [450][550]\t Training Loss 0.2082\t Accuracy 0.9437\n",
      "Epoch [39][40]\t Batch [500][550]\t Training Loss 0.2097\t Accuracy 0.9431\n",
      "\n",
      "Epoch [39]\t Average training loss 0.2100\t Average training accuracy 0.9430\n",
      "Epoch [39]\t Average validation loss 0.1689\t Average validation accuracy 0.9602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9429.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'Sigmoid': [sigmoid_loss, sigmoid_acc],\n",
    "                   'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~You have finished homework2-mlp, congratulations!~~  \n",
    "\n",
    "**Next, according to the requirements 4) of report:**\n",
    "### **You need to construct a two-hidden-layer MLP, using any activation function and loss function.**\n",
    "\n",
    "**Note: Please insert some new cells blow (using '+' bottom in the toolbar) refer to above codes. Do not modify the former code directly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "max_epoch = 60\n",
    "init_std = 0.01\n",
    "\n",
    "learning_rate_SGD = 3e-3\n",
    "weight_decay = 1e-3\n",
    "\n",
    "disp_freq = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][60]\t Batch [0][550]\t Training Loss 0.2714\t Accuracy 0.1500\n",
      "Epoch [0][60]\t Batch [60][550]\t Training Loss 0.1126\t Accuracy 0.4272\n",
      "Epoch [0][60]\t Batch [120][550]\t Training Loss 0.0982\t Accuracy 0.5376\n",
      "Epoch [0][60]\t Batch [180][550]\t Training Loss 0.0915\t Accuracy 0.5943\n",
      "Epoch [0][60]\t Batch [240][550]\t Training Loss 0.0865\t Accuracy 0.6383\n",
      "Epoch [0][60]\t Batch [300][550]\t Training Loss 0.0828\t Accuracy 0.6690\n",
      "Epoch [0][60]\t Batch [360][550]\t Training Loss 0.0800\t Accuracy 0.6930\n",
      "Epoch [0][60]\t Batch [420][550]\t Training Loss 0.0776\t Accuracy 0.7129\n",
      "Epoch [0][60]\t Batch [480][550]\t Training Loss 0.0756\t Accuracy 0.7274\n",
      "Epoch [0][60]\t Batch [540][550]\t Training Loss 0.0738\t Accuracy 0.7407\n",
      "\n",
      "Epoch [0]\t Average training loss 0.0735\t Average training accuracy 0.7426\n",
      "Epoch [0]\t Average validation loss 0.0553\t Average validation accuracy 0.8836\n",
      "\n",
      "Epoch [1][60]\t Batch [0][550]\t Training Loss 0.0542\t Accuracy 0.8800\n",
      "Epoch [1][60]\t Batch [60][550]\t Training Loss 0.0556\t Accuracy 0.8800\n",
      "Epoch [1][60]\t Batch [120][550]\t Training Loss 0.0555\t Accuracy 0.8819\n",
      "Epoch [1][60]\t Batch [180][550]\t Training Loss 0.0559\t Accuracy 0.8764\n",
      "Epoch [1][60]\t Batch [240][550]\t Training Loss 0.0550\t Accuracy 0.8816\n",
      "Epoch [1][60]\t Batch [300][550]\t Training Loss 0.0545\t Accuracy 0.8835\n",
      "Epoch [1][60]\t Batch [360][550]\t Training Loss 0.0541\t Accuracy 0.8843\n",
      "Epoch [1][60]\t Batch [420][550]\t Training Loss 0.0536\t Accuracy 0.8866\n",
      "Epoch [1][60]\t Batch [480][550]\t Training Loss 0.0532\t Accuracy 0.8872\n",
      "Epoch [1][60]\t Batch [540][550]\t Training Loss 0.0528\t Accuracy 0.8888\n",
      "\n",
      "Epoch [1]\t Average training loss 0.0527\t Average training accuracy 0.8892\n",
      "Epoch [1]\t Average validation loss 0.0455\t Average validation accuracy 0.9262\n",
      "\n",
      "Epoch [2][60]\t Batch [0][550]\t Training Loss 0.0447\t Accuracy 0.9400\n",
      "Epoch [2][60]\t Batch [60][550]\t Training Loss 0.0462\t Accuracy 0.9207\n",
      "Epoch [2][60]\t Batch [120][550]\t Training Loss 0.0466\t Accuracy 0.9170\n",
      "Epoch [2][60]\t Batch [180][550]\t Training Loss 0.0472\t Accuracy 0.9115\n",
      "Epoch [2][60]\t Batch [240][550]\t Training Loss 0.0467\t Accuracy 0.9150\n",
      "Epoch [2][60]\t Batch [300][550]\t Training Loss 0.0465\t Accuracy 0.9154\n",
      "Epoch [2][60]\t Batch [360][550]\t Training Loss 0.0463\t Accuracy 0.9154\n",
      "Epoch [2][60]\t Batch [420][550]\t Training Loss 0.0461\t Accuracy 0.9167\n",
      "Epoch [2][60]\t Batch [480][550]\t Training Loss 0.0460\t Accuracy 0.9165\n",
      "Epoch [2][60]\t Batch [540][550]\t Training Loss 0.0458\t Accuracy 0.9169\n",
      "\n",
      "Epoch [2]\t Average training loss 0.0457\t Average training accuracy 0.9172\n",
      "Epoch [2]\t Average validation loss 0.0409\t Average validation accuracy 0.9396\n",
      "\n",
      "Epoch [3][60]\t Batch [0][550]\t Training Loss 0.0396\t Accuracy 0.9400\n",
      "Epoch [3][60]\t Batch [60][550]\t Training Loss 0.0416\t Accuracy 0.9366\n",
      "Epoch [3][60]\t Batch [120][550]\t Training Loss 0.0420\t Accuracy 0.9324\n",
      "Epoch [3][60]\t Batch [180][550]\t Training Loss 0.0427\t Accuracy 0.9273\n",
      "Epoch [3][60]\t Batch [240][550]\t Training Loss 0.0424\t Accuracy 0.9300\n",
      "Epoch [3][60]\t Batch [300][550]\t Training Loss 0.0422\t Accuracy 0.9308\n",
      "Epoch [3][60]\t Batch [360][550]\t Training Loss 0.0422\t Accuracy 0.9306\n",
      "Epoch [3][60]\t Batch [420][550]\t Training Loss 0.0421\t Accuracy 0.9311\n",
      "Epoch [3][60]\t Batch [480][550]\t Training Loss 0.0420\t Accuracy 0.9306\n",
      "Epoch [3][60]\t Batch [540][550]\t Training Loss 0.0419\t Accuracy 0.9308\n",
      "\n",
      "Epoch [3]\t Average training loss 0.0419\t Average training accuracy 0.9311\n",
      "Epoch [3]\t Average validation loss 0.0379\t Average validation accuracy 0.9496\n",
      "\n",
      "Epoch [4][60]\t Batch [0][550]\t Training Loss 0.0364\t Accuracy 0.9600\n",
      "Epoch [4][60]\t Batch [60][550]\t Training Loss 0.0386\t Accuracy 0.9472\n",
      "Epoch [4][60]\t Batch [120][550]\t Training Loss 0.0391\t Accuracy 0.9440\n",
      "Epoch [4][60]\t Batch [180][550]\t Training Loss 0.0398\t Accuracy 0.9392\n",
      "Epoch [4][60]\t Batch [240][550]\t Training Loss 0.0395\t Accuracy 0.9409\n",
      "Epoch [4][60]\t Batch [300][550]\t Training Loss 0.0394\t Accuracy 0.9412\n",
      "Epoch [4][60]\t Batch [360][550]\t Training Loss 0.0394\t Accuracy 0.9410\n",
      "Epoch [4][60]\t Batch [420][550]\t Training Loss 0.0393\t Accuracy 0.9413\n",
      "Epoch [4][60]\t Batch [480][550]\t Training Loss 0.0393\t Accuracy 0.9408\n",
      "Epoch [4][60]\t Batch [540][550]\t Training Loss 0.0392\t Accuracy 0.9407\n",
      "\n",
      "Epoch [4]\t Average training loss 0.0392\t Average training accuracy 0.9409\n",
      "Epoch [4]\t Average validation loss 0.0358\t Average validation accuracy 0.9552\n",
      "\n",
      "Epoch [5][60]\t Batch [0][550]\t Training Loss 0.0344\t Accuracy 0.9600\n",
      "Epoch [5][60]\t Batch [60][550]\t Training Loss 0.0364\t Accuracy 0.9546\n",
      "Epoch [5][60]\t Batch [120][550]\t Training Loss 0.0370\t Accuracy 0.9512\n",
      "Epoch [5][60]\t Batch [180][550]\t Training Loss 0.0377\t Accuracy 0.9463\n",
      "Epoch [5][60]\t Batch [240][550]\t Training Loss 0.0373\t Accuracy 0.9476\n",
      "Epoch [5][60]\t Batch [300][550]\t Training Loss 0.0373\t Accuracy 0.9478\n",
      "Epoch [5][60]\t Batch [360][550]\t Training Loss 0.0373\t Accuracy 0.9475\n",
      "Epoch [5][60]\t Batch [420][550]\t Training Loss 0.0373\t Accuracy 0.9480\n",
      "Epoch [5][60]\t Batch [480][550]\t Training Loss 0.0373\t Accuracy 0.9475\n",
      "Epoch [5][60]\t Batch [540][550]\t Training Loss 0.0373\t Accuracy 0.9472\n",
      "\n",
      "Epoch [5]\t Average training loss 0.0372\t Average training accuracy 0.9475\n",
      "Epoch [5]\t Average validation loss 0.0342\t Average validation accuracy 0.9616\n",
      "\n",
      "Epoch [6][60]\t Batch [0][550]\t Training Loss 0.0329\t Accuracy 0.9600\n",
      "Epoch [6][60]\t Batch [60][550]\t Training Loss 0.0348\t Accuracy 0.9598\n",
      "Epoch [6][60]\t Batch [120][550]\t Training Loss 0.0353\t Accuracy 0.9556\n",
      "Epoch [6][60]\t Batch [180][550]\t Training Loss 0.0360\t Accuracy 0.9516\n",
      "Epoch [6][60]\t Batch [240][550]\t Training Loss 0.0357\t Accuracy 0.9531\n",
      "Epoch [6][60]\t Batch [300][550]\t Training Loss 0.0357\t Accuracy 0.9534\n",
      "Epoch [6][60]\t Batch [360][550]\t Training Loss 0.0357\t Accuracy 0.9532\n",
      "Epoch [6][60]\t Batch [420][550]\t Training Loss 0.0357\t Accuracy 0.9532\n",
      "Epoch [6][60]\t Batch [480][550]\t Training Loss 0.0357\t Accuracy 0.9526\n",
      "Epoch [6][60]\t Batch [540][550]\t Training Loss 0.0357\t Accuracy 0.9523\n",
      "\n",
      "Epoch [6]\t Average training loss 0.0356\t Average training accuracy 0.9524\n",
      "Epoch [6]\t Average validation loss 0.0330\t Average validation accuracy 0.9640\n",
      "\n",
      "Epoch [7][60]\t Batch [0][550]\t Training Loss 0.0316\t Accuracy 0.9600\n",
      "Epoch [7][60]\t Batch [60][550]\t Training Loss 0.0334\t Accuracy 0.9630\n",
      "Epoch [7][60]\t Batch [120][550]\t Training Loss 0.0340\t Accuracy 0.9600\n",
      "Epoch [7][60]\t Batch [180][550]\t Training Loss 0.0346\t Accuracy 0.9562\n",
      "Epoch [7][60]\t Batch [240][550]\t Training Loss 0.0343\t Accuracy 0.9571\n",
      "Epoch [7][60]\t Batch [300][550]\t Training Loss 0.0343\t Accuracy 0.9568\n",
      "Epoch [7][60]\t Batch [360][550]\t Training Loss 0.0343\t Accuracy 0.9568\n",
      "Epoch [7][60]\t Batch [420][550]\t Training Loss 0.0343\t Accuracy 0.9570\n",
      "Epoch [7][60]\t Batch [480][550]\t Training Loss 0.0344\t Accuracy 0.9565\n",
      "Epoch [7][60]\t Batch [540][550]\t Training Loss 0.0343\t Accuracy 0.9564\n",
      "\n",
      "Epoch [7]\t Average training loss 0.0343\t Average training accuracy 0.9565\n",
      "Epoch [7]\t Average validation loss 0.0319\t Average validation accuracy 0.9640\n",
      "\n",
      "Epoch [8][60]\t Batch [0][550]\t Training Loss 0.0304\t Accuracy 0.9700\n",
      "Epoch [8][60]\t Batch [60][550]\t Training Loss 0.0322\t Accuracy 0.9662\n",
      "Epoch [8][60]\t Batch [120][550]\t Training Loss 0.0328\t Accuracy 0.9634\n",
      "Epoch [8][60]\t Batch [180][550]\t Training Loss 0.0334\t Accuracy 0.9600\n",
      "Epoch [8][60]\t Batch [240][550]\t Training Loss 0.0331\t Accuracy 0.9609\n",
      "Epoch [8][60]\t Batch [300][550]\t Training Loss 0.0331\t Accuracy 0.9603\n",
      "Epoch [8][60]\t Batch [360][550]\t Training Loss 0.0331\t Accuracy 0.9605\n",
      "Epoch [8][60]\t Batch [420][550]\t Training Loss 0.0331\t Accuracy 0.9608\n",
      "Epoch [8][60]\t Batch [480][550]\t Training Loss 0.0332\t Accuracy 0.9603\n",
      "Epoch [8][60]\t Batch [540][550]\t Training Loss 0.0332\t Accuracy 0.9601\n",
      "\n",
      "Epoch [8]\t Average training loss 0.0331\t Average training accuracy 0.9603\n",
      "Epoch [8]\t Average validation loss 0.0311\t Average validation accuracy 0.9660\n",
      "\n",
      "Epoch [9][60]\t Batch [0][550]\t Training Loss 0.0293\t Accuracy 0.9700\n",
      "Epoch [9][60]\t Batch [60][550]\t Training Loss 0.0312\t Accuracy 0.9687\n",
      "Epoch [9][60]\t Batch [120][550]\t Training Loss 0.0318\t Accuracy 0.9660\n",
      "Epoch [9][60]\t Batch [180][550]\t Training Loss 0.0323\t Accuracy 0.9638\n",
      "Epoch [9][60]\t Batch [240][550]\t Training Loss 0.0320\t Accuracy 0.9646\n",
      "Epoch [9][60]\t Batch [300][550]\t Training Loss 0.0321\t Accuracy 0.9636\n",
      "Epoch [9][60]\t Batch [360][550]\t Training Loss 0.0321\t Accuracy 0.9638\n",
      "Epoch [9][60]\t Batch [420][550]\t Training Loss 0.0321\t Accuracy 0.9640\n",
      "Epoch [9][60]\t Batch [480][550]\t Training Loss 0.0321\t Accuracy 0.9636\n",
      "Epoch [9][60]\t Batch [540][550]\t Training Loss 0.0321\t Accuracy 0.9633\n",
      "\n",
      "Epoch [9]\t Average training loss 0.0321\t Average training accuracy 0.9636\n",
      "Epoch [9]\t Average validation loss 0.0304\t Average validation accuracy 0.9676\n",
      "\n",
      "Epoch [10][60]\t Batch [0][550]\t Training Loss 0.0285\t Accuracy 0.9700\n",
      "Epoch [10][60]\t Batch [60][550]\t Training Loss 0.0304\t Accuracy 0.9708\n",
      "Epoch [10][60]\t Batch [120][550]\t Training Loss 0.0309\t Accuracy 0.9684\n",
      "Epoch [10][60]\t Batch [180][550]\t Training Loss 0.0314\t Accuracy 0.9666\n",
      "Epoch [10][60]\t Batch [240][550]\t Training Loss 0.0311\t Accuracy 0.9668\n",
      "Epoch [10][60]\t Batch [300][550]\t Training Loss 0.0312\t Accuracy 0.9659\n",
      "Epoch [10][60]\t Batch [360][550]\t Training Loss 0.0312\t Accuracy 0.9661\n",
      "Epoch [10][60]\t Batch [420][550]\t Training Loss 0.0312\t Accuracy 0.9661\n",
      "Epoch [10][60]\t Batch [480][550]\t Training Loss 0.0313\t Accuracy 0.9656\n",
      "Epoch [10][60]\t Batch [540][550]\t Training Loss 0.0313\t Accuracy 0.9655\n",
      "\n",
      "Epoch [10]\t Average training loss 0.0312\t Average training accuracy 0.9657\n",
      "Epoch [10]\t Average validation loss 0.0299\t Average validation accuracy 0.9694\n",
      "\n",
      "Epoch [11][60]\t Batch [0][550]\t Training Loss 0.0278\t Accuracy 0.9700\n",
      "Epoch [11][60]\t Batch [60][550]\t Training Loss 0.0296\t Accuracy 0.9716\n",
      "Epoch [11][60]\t Batch [120][550]\t Training Loss 0.0301\t Accuracy 0.9692\n",
      "Epoch [11][60]\t Batch [180][550]\t Training Loss 0.0306\t Accuracy 0.9680\n",
      "Epoch [11][60]\t Batch [240][550]\t Training Loss 0.0303\t Accuracy 0.9687\n",
      "Epoch [11][60]\t Batch [300][550]\t Training Loss 0.0304\t Accuracy 0.9679\n",
      "Epoch [11][60]\t Batch [360][550]\t Training Loss 0.0304\t Accuracy 0.9682\n",
      "Epoch [11][60]\t Batch [420][550]\t Training Loss 0.0304\t Accuracy 0.9680\n",
      "Epoch [11][60]\t Batch [480][550]\t Training Loss 0.0305\t Accuracy 0.9675\n",
      "Epoch [11][60]\t Batch [540][550]\t Training Loss 0.0305\t Accuracy 0.9673\n",
      "\n",
      "Epoch [11]\t Average training loss 0.0304\t Average training accuracy 0.9675\n",
      "Epoch [11]\t Average validation loss 0.0294\t Average validation accuracy 0.9700\n",
      "\n",
      "Epoch [12][60]\t Batch [0][550]\t Training Loss 0.0273\t Accuracy 0.9700\n",
      "Epoch [12][60]\t Batch [60][550]\t Training Loss 0.0289\t Accuracy 0.9723\n",
      "Epoch [12][60]\t Batch [120][550]\t Training Loss 0.0295\t Accuracy 0.9703\n",
      "Epoch [12][60]\t Batch [180][550]\t Training Loss 0.0298\t Accuracy 0.9694\n",
      "Epoch [12][60]\t Batch [240][550]\t Training Loss 0.0296\t Accuracy 0.9703\n",
      "Epoch [12][60]\t Batch [300][550]\t Training Loss 0.0297\t Accuracy 0.9695\n",
      "Epoch [12][60]\t Batch [360][550]\t Training Loss 0.0297\t Accuracy 0.9696\n",
      "Epoch [12][60]\t Batch [420][550]\t Training Loss 0.0297\t Accuracy 0.9695\n",
      "Epoch [12][60]\t Batch [480][550]\t Training Loss 0.0298\t Accuracy 0.9691\n",
      "Epoch [12][60]\t Batch [540][550]\t Training Loss 0.0298\t Accuracy 0.9690\n",
      "\n",
      "Epoch [12]\t Average training loss 0.0298\t Average training accuracy 0.9691\n",
      "Epoch [12]\t Average validation loss 0.0289\t Average validation accuracy 0.9712\n",
      "\n",
      "Epoch [13][60]\t Batch [0][550]\t Training Loss 0.0267\t Accuracy 0.9700\n",
      "Epoch [13][60]\t Batch [60][550]\t Training Loss 0.0283\t Accuracy 0.9730\n",
      "Epoch [13][60]\t Batch [120][550]\t Training Loss 0.0288\t Accuracy 0.9713\n",
      "Epoch [13][60]\t Batch [180][550]\t Training Loss 0.0292\t Accuracy 0.9710\n",
      "Epoch [13][60]\t Batch [240][550]\t Training Loss 0.0289\t Accuracy 0.9718\n",
      "Epoch [13][60]\t Batch [300][550]\t Training Loss 0.0291\t Accuracy 0.9712\n",
      "Epoch [13][60]\t Batch [360][550]\t Training Loss 0.0290\t Accuracy 0.9714\n",
      "Epoch [13][60]\t Batch [420][550]\t Training Loss 0.0291\t Accuracy 0.9712\n",
      "Epoch [13][60]\t Batch [480][550]\t Training Loss 0.0291\t Accuracy 0.9709\n",
      "Epoch [13][60]\t Batch [540][550]\t Training Loss 0.0292\t Accuracy 0.9706\n",
      "\n",
      "Epoch [13]\t Average training loss 0.0291\t Average training accuracy 0.9708\n",
      "Epoch [13]\t Average validation loss 0.0286\t Average validation accuracy 0.9724\n",
      "\n",
      "Epoch [14][60]\t Batch [0][550]\t Training Loss 0.0263\t Accuracy 0.9700\n",
      "Epoch [14][60]\t Batch [60][550]\t Training Loss 0.0277\t Accuracy 0.9736\n",
      "Epoch [14][60]\t Batch [120][550]\t Training Loss 0.0283\t Accuracy 0.9727\n",
      "Epoch [14][60]\t Batch [180][550]\t Training Loss 0.0286\t Accuracy 0.9726\n",
      "Epoch [14][60]\t Batch [240][550]\t Training Loss 0.0283\t Accuracy 0.9732\n",
      "Epoch [14][60]\t Batch [300][550]\t Training Loss 0.0285\t Accuracy 0.9726\n",
      "Epoch [14][60]\t Batch [360][550]\t Training Loss 0.0285\t Accuracy 0.9729\n",
      "Epoch [14][60]\t Batch [420][550]\t Training Loss 0.0285\t Accuracy 0.9727\n",
      "Epoch [14][60]\t Batch [480][550]\t Training Loss 0.0286\t Accuracy 0.9723\n",
      "Epoch [14][60]\t Batch [540][550]\t Training Loss 0.0286\t Accuracy 0.9721\n",
      "\n",
      "Epoch [14]\t Average training loss 0.0285\t Average training accuracy 0.9722\n",
      "Epoch [14]\t Average validation loss 0.0282\t Average validation accuracy 0.9730\n",
      "\n",
      "Epoch [15][60]\t Batch [0][550]\t Training Loss 0.0256\t Accuracy 0.9700\n",
      "Epoch [15][60]\t Batch [60][550]\t Training Loss 0.0271\t Accuracy 0.9752\n",
      "Epoch [15][60]\t Batch [120][550]\t Training Loss 0.0277\t Accuracy 0.9740\n",
      "Epoch [15][60]\t Batch [180][550]\t Training Loss 0.0280\t Accuracy 0.9741\n",
      "Epoch [15][60]\t Batch [240][550]\t Training Loss 0.0278\t Accuracy 0.9746\n",
      "Epoch [15][60]\t Batch [300][550]\t Training Loss 0.0279\t Accuracy 0.9740\n",
      "Epoch [15][60]\t Batch [360][550]\t Training Loss 0.0279\t Accuracy 0.9742\n",
      "Epoch [15][60]\t Batch [420][550]\t Training Loss 0.0279\t Accuracy 0.9740\n",
      "Epoch [15][60]\t Batch [480][550]\t Training Loss 0.0280\t Accuracy 0.9738\n",
      "Epoch [15][60]\t Batch [540][550]\t Training Loss 0.0280\t Accuracy 0.9736\n",
      "\n",
      "Epoch [15]\t Average training loss 0.0280\t Average training accuracy 0.9738\n",
      "Epoch [15]\t Average validation loss 0.0279\t Average validation accuracy 0.9728\n",
      "\n",
      "Epoch [16][60]\t Batch [0][550]\t Training Loss 0.0251\t Accuracy 0.9700\n",
      "Epoch [16][60]\t Batch [60][550]\t Training Loss 0.0266\t Accuracy 0.9762\n",
      "Epoch [16][60]\t Batch [120][550]\t Training Loss 0.0272\t Accuracy 0.9750\n",
      "Epoch [16][60]\t Batch [180][550]\t Training Loss 0.0275\t Accuracy 0.9753\n",
      "Epoch [16][60]\t Batch [240][550]\t Training Loss 0.0273\t Accuracy 0.9758\n",
      "Epoch [16][60]\t Batch [300][550]\t Training Loss 0.0274\t Accuracy 0.9751\n",
      "Epoch [16][60]\t Batch [360][550]\t Training Loss 0.0274\t Accuracy 0.9753\n",
      "Epoch [16][60]\t Batch [420][550]\t Training Loss 0.0274\t Accuracy 0.9751\n",
      "Epoch [16][60]\t Batch [480][550]\t Training Loss 0.0275\t Accuracy 0.9749\n",
      "Epoch [16][60]\t Batch [540][550]\t Training Loss 0.0275\t Accuracy 0.9747\n",
      "\n",
      "Epoch [16]\t Average training loss 0.0275\t Average training accuracy 0.9749\n",
      "Epoch [16]\t Average validation loss 0.0275\t Average validation accuracy 0.9740\n",
      "\n",
      "Epoch [17][60]\t Batch [0][550]\t Training Loss 0.0247\t Accuracy 0.9700\n",
      "Epoch [17][60]\t Batch [60][550]\t Training Loss 0.0262\t Accuracy 0.9774\n",
      "Epoch [17][60]\t Batch [120][550]\t Training Loss 0.0268\t Accuracy 0.9760\n",
      "Epoch [17][60]\t Batch [180][550]\t Training Loss 0.0270\t Accuracy 0.9762\n",
      "Epoch [17][60]\t Batch [240][550]\t Training Loss 0.0268\t Accuracy 0.9767\n",
      "Epoch [17][60]\t Batch [300][550]\t Training Loss 0.0270\t Accuracy 0.9762\n",
      "Epoch [17][60]\t Batch [360][550]\t Training Loss 0.0269\t Accuracy 0.9765\n",
      "Epoch [17][60]\t Batch [420][550]\t Training Loss 0.0270\t Accuracy 0.9762\n",
      "Epoch [17][60]\t Batch [480][550]\t Training Loss 0.0270\t Accuracy 0.9760\n",
      "Epoch [17][60]\t Batch [540][550]\t Training Loss 0.0271\t Accuracy 0.9758\n",
      "\n",
      "Epoch [17]\t Average training loss 0.0270\t Average training accuracy 0.9759\n",
      "Epoch [17]\t Average validation loss 0.0271\t Average validation accuracy 0.9746\n",
      "\n",
      "Epoch [18][60]\t Batch [0][550]\t Training Loss 0.0243\t Accuracy 0.9800\n",
      "Epoch [18][60]\t Batch [60][550]\t Training Loss 0.0258\t Accuracy 0.9777\n",
      "Epoch [18][60]\t Batch [120][550]\t Training Loss 0.0264\t Accuracy 0.9765\n",
      "Epoch [18][60]\t Batch [180][550]\t Training Loss 0.0266\t Accuracy 0.9769\n",
      "Epoch [18][60]\t Batch [240][550]\t Training Loss 0.0264\t Accuracy 0.9776\n",
      "Epoch [18][60]\t Batch [300][550]\t Training Loss 0.0265\t Accuracy 0.9772\n",
      "Epoch [18][60]\t Batch [360][550]\t Training Loss 0.0265\t Accuracy 0.9775\n",
      "Epoch [18][60]\t Batch [420][550]\t Training Loss 0.0265\t Accuracy 0.9772\n",
      "Epoch [18][60]\t Batch [480][550]\t Training Loss 0.0266\t Accuracy 0.9770\n",
      "Epoch [18][60]\t Batch [540][550]\t Training Loss 0.0267\t Accuracy 0.9768\n",
      "\n",
      "Epoch [18]\t Average training loss 0.0266\t Average training accuracy 0.9769\n",
      "Epoch [18]\t Average validation loss 0.0268\t Average validation accuracy 0.9746\n",
      "\n",
      "Epoch [19][60]\t Batch [0][550]\t Training Loss 0.0239\t Accuracy 0.9800\n",
      "Epoch [19][60]\t Batch [60][550]\t Training Loss 0.0254\t Accuracy 0.9785\n",
      "Epoch [19][60]\t Batch [120][550]\t Training Loss 0.0260\t Accuracy 0.9776\n",
      "Epoch [19][60]\t Batch [180][550]\t Training Loss 0.0261\t Accuracy 0.9779\n",
      "Epoch [19][60]\t Batch [240][550]\t Training Loss 0.0259\t Accuracy 0.9784\n",
      "Epoch [19][60]\t Batch [300][550]\t Training Loss 0.0261\t Accuracy 0.9781\n",
      "Epoch [19][60]\t Batch [360][550]\t Training Loss 0.0261\t Accuracy 0.9783\n",
      "Epoch [19][60]\t Batch [420][550]\t Training Loss 0.0261\t Accuracy 0.9781\n",
      "Epoch [19][60]\t Batch [480][550]\t Training Loss 0.0262\t Accuracy 0.9779\n",
      "Epoch [19][60]\t Batch [540][550]\t Training Loss 0.0262\t Accuracy 0.9777\n",
      "\n",
      "Epoch [19]\t Average training loss 0.0262\t Average training accuracy 0.9778\n",
      "Epoch [19]\t Average validation loss 0.0266\t Average validation accuracy 0.9752\n",
      "\n",
      "Epoch [20][60]\t Batch [0][550]\t Training Loss 0.0237\t Accuracy 0.9800\n",
      "Epoch [20][60]\t Batch [60][550]\t Training Loss 0.0251\t Accuracy 0.9797\n",
      "Epoch [20][60]\t Batch [120][550]\t Training Loss 0.0256\t Accuracy 0.9788\n",
      "Epoch [20][60]\t Batch [180][550]\t Training Loss 0.0257\t Accuracy 0.9790\n",
      "Epoch [20][60]\t Batch [240][550]\t Training Loss 0.0256\t Accuracy 0.9794\n",
      "Epoch [20][60]\t Batch [300][550]\t Training Loss 0.0257\t Accuracy 0.9792\n",
      "Epoch [20][60]\t Batch [360][550]\t Training Loss 0.0257\t Accuracy 0.9793\n",
      "Epoch [20][60]\t Batch [420][550]\t Training Loss 0.0257\t Accuracy 0.9790\n",
      "Epoch [20][60]\t Batch [480][550]\t Training Loss 0.0258\t Accuracy 0.9788\n",
      "Epoch [20][60]\t Batch [540][550]\t Training Loss 0.0259\t Accuracy 0.9785\n",
      "\n",
      "Epoch [20]\t Average training loss 0.0258\t Average training accuracy 0.9787\n",
      "Epoch [20]\t Average validation loss 0.0264\t Average validation accuracy 0.9750\n",
      "\n",
      "Epoch [21][60]\t Batch [0][550]\t Training Loss 0.0234\t Accuracy 0.9800\n",
      "Epoch [21][60]\t Batch [60][550]\t Training Loss 0.0247\t Accuracy 0.9807\n",
      "Epoch [21][60]\t Batch [120][550]\t Training Loss 0.0252\t Accuracy 0.9796\n",
      "Epoch [21][60]\t Batch [180][550]\t Training Loss 0.0254\t Accuracy 0.9799\n",
      "Epoch [21][60]\t Batch [240][550]\t Training Loss 0.0252\t Accuracy 0.9802\n",
      "Epoch [21][60]\t Batch [300][550]\t Training Loss 0.0254\t Accuracy 0.9799\n",
      "Epoch [21][60]\t Batch [360][550]\t Training Loss 0.0253\t Accuracy 0.9801\n",
      "Epoch [21][60]\t Batch [420][550]\t Training Loss 0.0254\t Accuracy 0.9797\n",
      "Epoch [21][60]\t Batch [480][550]\t Training Loss 0.0254\t Accuracy 0.9795\n",
      "Epoch [21][60]\t Batch [540][550]\t Training Loss 0.0255\t Accuracy 0.9792\n",
      "\n",
      "Epoch [21]\t Average training loss 0.0254\t Average training accuracy 0.9793\n",
      "Epoch [21]\t Average validation loss 0.0261\t Average validation accuracy 0.9756\n",
      "\n",
      "Epoch [22][60]\t Batch [0][550]\t Training Loss 0.0232\t Accuracy 0.9800\n",
      "Epoch [22][60]\t Batch [60][550]\t Training Loss 0.0244\t Accuracy 0.9810\n",
      "Epoch [22][60]\t Batch [120][550]\t Training Loss 0.0249\t Accuracy 0.9798\n",
      "Epoch [22][60]\t Batch [180][550]\t Training Loss 0.0250\t Accuracy 0.9803\n",
      "Epoch [22][60]\t Batch [240][550]\t Training Loss 0.0248\t Accuracy 0.9807\n",
      "Epoch [22][60]\t Batch [300][550]\t Training Loss 0.0250\t Accuracy 0.9805\n",
      "Epoch [22][60]\t Batch [360][550]\t Training Loss 0.0250\t Accuracy 0.9806\n",
      "Epoch [22][60]\t Batch [420][550]\t Training Loss 0.0250\t Accuracy 0.9804\n",
      "Epoch [22][60]\t Batch [480][550]\t Training Loss 0.0251\t Accuracy 0.9801\n",
      "Epoch [22][60]\t Batch [540][550]\t Training Loss 0.0251\t Accuracy 0.9798\n",
      "\n",
      "Epoch [22]\t Average training loss 0.0251\t Average training accuracy 0.9800\n",
      "Epoch [22]\t Average validation loss 0.0259\t Average validation accuracy 0.9762\n",
      "\n",
      "Epoch [23][60]\t Batch [0][550]\t Training Loss 0.0230\t Accuracy 0.9800\n",
      "Epoch [23][60]\t Batch [60][550]\t Training Loss 0.0241\t Accuracy 0.9816\n",
      "Epoch [23][60]\t Batch [120][550]\t Training Loss 0.0246\t Accuracy 0.9807\n",
      "Epoch [23][60]\t Batch [180][550]\t Training Loss 0.0247\t Accuracy 0.9810\n",
      "Epoch [23][60]\t Batch [240][550]\t Training Loss 0.0245\t Accuracy 0.9814\n",
      "Epoch [23][60]\t Batch [300][550]\t Training Loss 0.0247\t Accuracy 0.9813\n",
      "Epoch [23][60]\t Batch [360][550]\t Training Loss 0.0247\t Accuracy 0.9814\n",
      "Epoch [23][60]\t Batch [420][550]\t Training Loss 0.0247\t Accuracy 0.9811\n",
      "Epoch [23][60]\t Batch [480][550]\t Training Loss 0.0248\t Accuracy 0.9809\n",
      "Epoch [23][60]\t Batch [540][550]\t Training Loss 0.0248\t Accuracy 0.9806\n",
      "\n",
      "Epoch [23]\t Average training loss 0.0248\t Average training accuracy 0.9807\n",
      "Epoch [23]\t Average validation loss 0.0257\t Average validation accuracy 0.9764\n",
      "\n",
      "Epoch [24][60]\t Batch [0][550]\t Training Loss 0.0227\t Accuracy 0.9800\n",
      "Epoch [24][60]\t Batch [60][550]\t Training Loss 0.0238\t Accuracy 0.9820\n",
      "Epoch [24][60]\t Batch [120][550]\t Training Loss 0.0243\t Accuracy 0.9815\n",
      "Epoch [24][60]\t Batch [180][550]\t Training Loss 0.0243\t Accuracy 0.9819\n",
      "Epoch [24][60]\t Batch [240][550]\t Training Loss 0.0242\t Accuracy 0.9823\n",
      "Epoch [24][60]\t Batch [300][550]\t Training Loss 0.0243\t Accuracy 0.9822\n",
      "Epoch [24][60]\t Batch [360][550]\t Training Loss 0.0243\t Accuracy 0.9824\n",
      "Epoch [24][60]\t Batch [420][550]\t Training Loss 0.0244\t Accuracy 0.9820\n",
      "Epoch [24][60]\t Batch [480][550]\t Training Loss 0.0244\t Accuracy 0.9817\n",
      "Epoch [24][60]\t Batch [540][550]\t Training Loss 0.0245\t Accuracy 0.9813\n",
      "\n",
      "Epoch [24]\t Average training loss 0.0245\t Average training accuracy 0.9814\n",
      "Epoch [24]\t Average validation loss 0.0256\t Average validation accuracy 0.9760\n",
      "\n",
      "Epoch [25][60]\t Batch [0][550]\t Training Loss 0.0226\t Accuracy 0.9800\n",
      "Epoch [25][60]\t Batch [60][550]\t Training Loss 0.0234\t Accuracy 0.9820\n",
      "Epoch [25][60]\t Batch [120][550]\t Training Loss 0.0239\t Accuracy 0.9818\n",
      "Epoch [25][60]\t Batch [180][550]\t Training Loss 0.0240\t Accuracy 0.9825\n",
      "Epoch [25][60]\t Batch [240][550]\t Training Loss 0.0239\t Accuracy 0.9829\n",
      "Epoch [25][60]\t Batch [300][550]\t Training Loss 0.0240\t Accuracy 0.9827\n",
      "Epoch [25][60]\t Batch [360][550]\t Training Loss 0.0240\t Accuracy 0.9829\n",
      "Epoch [25][60]\t Batch [420][550]\t Training Loss 0.0241\t Accuracy 0.9826\n",
      "Epoch [25][60]\t Batch [480][550]\t Training Loss 0.0241\t Accuracy 0.9824\n",
      "Epoch [25][60]\t Batch [540][550]\t Training Loss 0.0242\t Accuracy 0.9821\n",
      "\n",
      "Epoch [25]\t Average training loss 0.0241\t Average training accuracy 0.9822\n",
      "Epoch [25]\t Average validation loss 0.0254\t Average validation accuracy 0.9764\n",
      "\n",
      "Epoch [26][60]\t Batch [0][550]\t Training Loss 0.0223\t Accuracy 0.9800\n",
      "Epoch [26][60]\t Batch [60][550]\t Training Loss 0.0232\t Accuracy 0.9825\n",
      "Epoch [26][60]\t Batch [120][550]\t Training Loss 0.0237\t Accuracy 0.9826\n",
      "Epoch [26][60]\t Batch [180][550]\t Training Loss 0.0237\t Accuracy 0.9833\n",
      "Epoch [26][60]\t Batch [240][550]\t Training Loss 0.0236\t Accuracy 0.9837\n",
      "Epoch [26][60]\t Batch [300][550]\t Training Loss 0.0237\t Accuracy 0.9835\n",
      "Epoch [26][60]\t Batch [360][550]\t Training Loss 0.0237\t Accuracy 0.9837\n",
      "Epoch [26][60]\t Batch [420][550]\t Training Loss 0.0238\t Accuracy 0.9834\n",
      "Epoch [26][60]\t Batch [480][550]\t Training Loss 0.0238\t Accuracy 0.9831\n",
      "Epoch [26][60]\t Batch [540][550]\t Training Loss 0.0239\t Accuracy 0.9829\n",
      "\n",
      "Epoch [26]\t Average training loss 0.0239\t Average training accuracy 0.9830\n",
      "Epoch [26]\t Average validation loss 0.0253\t Average validation accuracy 0.9764\n",
      "\n",
      "Epoch [27][60]\t Batch [0][550]\t Training Loss 0.0221\t Accuracy 0.9800\n",
      "Epoch [27][60]\t Batch [60][550]\t Training Loss 0.0229\t Accuracy 0.9834\n",
      "Epoch [27][60]\t Batch [120][550]\t Training Loss 0.0234\t Accuracy 0.9837\n",
      "Epoch [27][60]\t Batch [180][550]\t Training Loss 0.0234\t Accuracy 0.9842\n",
      "Epoch [27][60]\t Batch [240][550]\t Training Loss 0.0233\t Accuracy 0.9845\n",
      "Epoch [27][60]\t Batch [300][550]\t Training Loss 0.0235\t Accuracy 0.9843\n",
      "Epoch [27][60]\t Batch [360][550]\t Training Loss 0.0235\t Accuracy 0.9844\n",
      "Epoch [27][60]\t Batch [420][550]\t Training Loss 0.0235\t Accuracy 0.9840\n",
      "Epoch [27][60]\t Batch [480][550]\t Training Loss 0.0236\t Accuracy 0.9837\n",
      "Epoch [27][60]\t Batch [540][550]\t Training Loss 0.0236\t Accuracy 0.9835\n",
      "\n",
      "Epoch [27]\t Average training loss 0.0236\t Average training accuracy 0.9836\n",
      "Epoch [27]\t Average validation loss 0.0252\t Average validation accuracy 0.9760\n",
      "\n",
      "Epoch [28][60]\t Batch [0][550]\t Training Loss 0.0219\t Accuracy 0.9800\n",
      "Epoch [28][60]\t Batch [60][550]\t Training Loss 0.0227\t Accuracy 0.9836\n",
      "Epoch [28][60]\t Batch [120][550]\t Training Loss 0.0231\t Accuracy 0.9839\n",
      "Epoch [28][60]\t Batch [180][550]\t Training Loss 0.0232\t Accuracy 0.9845\n",
      "Epoch [28][60]\t Batch [240][550]\t Training Loss 0.0230\t Accuracy 0.9848\n",
      "Epoch [28][60]\t Batch [300][550]\t Training Loss 0.0232\t Accuracy 0.9847\n",
      "Epoch [28][60]\t Batch [360][550]\t Training Loss 0.0232\t Accuracy 0.9848\n",
      "Epoch [28][60]\t Batch [420][550]\t Training Loss 0.0232\t Accuracy 0.9843\n",
      "Epoch [28][60]\t Batch [480][550]\t Training Loss 0.0233\t Accuracy 0.9841\n",
      "Epoch [28][60]\t Batch [540][550]\t Training Loss 0.0234\t Accuracy 0.9839\n",
      "\n",
      "Epoch [28]\t Average training loss 0.0233\t Average training accuracy 0.9841\n",
      "Epoch [28]\t Average validation loss 0.0251\t Average validation accuracy 0.9760\n",
      "\n",
      "Epoch [29][60]\t Batch [0][550]\t Training Loss 0.0218\t Accuracy 0.9800\n",
      "Epoch [29][60]\t Batch [60][550]\t Training Loss 0.0224\t Accuracy 0.9839\n",
      "Epoch [29][60]\t Batch [120][550]\t Training Loss 0.0229\t Accuracy 0.9840\n",
      "Epoch [29][60]\t Batch [180][550]\t Training Loss 0.0229\t Accuracy 0.9848\n",
      "Epoch [29][60]\t Batch [240][550]\t Training Loss 0.0228\t Accuracy 0.9852\n",
      "Epoch [29][60]\t Batch [300][550]\t Training Loss 0.0229\t Accuracy 0.9851\n",
      "Epoch [29][60]\t Batch [360][550]\t Training Loss 0.0229\t Accuracy 0.9852\n",
      "Epoch [29][60]\t Batch [420][550]\t Training Loss 0.0230\t Accuracy 0.9848\n",
      "Epoch [29][60]\t Batch [480][550]\t Training Loss 0.0231\t Accuracy 0.9847\n",
      "Epoch [29][60]\t Batch [540][550]\t Training Loss 0.0231\t Accuracy 0.9844\n",
      "\n",
      "Epoch [29]\t Average training loss 0.0231\t Average training accuracy 0.9845\n",
      "Epoch [29]\t Average validation loss 0.0249\t Average validation accuracy 0.9760\n",
      "\n",
      "Epoch [30][60]\t Batch [0][550]\t Training Loss 0.0216\t Accuracy 0.9800\n",
      "Epoch [30][60]\t Batch [60][550]\t Training Loss 0.0222\t Accuracy 0.9851\n",
      "Epoch [30][60]\t Batch [120][550]\t Training Loss 0.0226\t Accuracy 0.9848\n",
      "Epoch [30][60]\t Batch [180][550]\t Training Loss 0.0227\t Accuracy 0.9855\n",
      "Epoch [30][60]\t Batch [240][550]\t Training Loss 0.0225\t Accuracy 0.9858\n",
      "Epoch [30][60]\t Batch [300][550]\t Training Loss 0.0227\t Accuracy 0.9856\n",
      "Epoch [30][60]\t Batch [360][550]\t Training Loss 0.0227\t Accuracy 0.9856\n",
      "Epoch [30][60]\t Batch [420][550]\t Training Loss 0.0227\t Accuracy 0.9852\n",
      "Epoch [30][60]\t Batch [480][550]\t Training Loss 0.0228\t Accuracy 0.9851\n",
      "Epoch [30][60]\t Batch [540][550]\t Training Loss 0.0229\t Accuracy 0.9849\n",
      "\n",
      "Epoch [30]\t Average training loss 0.0228\t Average training accuracy 0.9850\n",
      "Epoch [30]\t Average validation loss 0.0247\t Average validation accuracy 0.9760\n",
      "\n",
      "Epoch [31][60]\t Batch [0][550]\t Training Loss 0.0215\t Accuracy 0.9800\n",
      "Epoch [31][60]\t Batch [60][550]\t Training Loss 0.0220\t Accuracy 0.9851\n",
      "Epoch [31][60]\t Batch [120][550]\t Training Loss 0.0224\t Accuracy 0.9850\n",
      "Epoch [31][60]\t Batch [180][550]\t Training Loss 0.0224\t Accuracy 0.9857\n",
      "Epoch [31][60]\t Batch [240][550]\t Training Loss 0.0223\t Accuracy 0.9860\n",
      "Epoch [31][60]\t Batch [300][550]\t Training Loss 0.0224\t Accuracy 0.9859\n",
      "Epoch [31][60]\t Batch [360][550]\t Training Loss 0.0225\t Accuracy 0.9858\n",
      "Epoch [31][60]\t Batch [420][550]\t Training Loss 0.0225\t Accuracy 0.9855\n",
      "Epoch [31][60]\t Batch [480][550]\t Training Loss 0.0226\t Accuracy 0.9854\n",
      "Epoch [31][60]\t Batch [540][550]\t Training Loss 0.0226\t Accuracy 0.9852\n",
      "\n",
      "Epoch [31]\t Average training loss 0.0226\t Average training accuracy 0.9853\n",
      "Epoch [31]\t Average validation loss 0.0246\t Average validation accuracy 0.9764\n",
      "\n",
      "Epoch [32][60]\t Batch [0][550]\t Training Loss 0.0214\t Accuracy 0.9800\n",
      "Epoch [32][60]\t Batch [60][550]\t Training Loss 0.0218\t Accuracy 0.9852\n",
      "Epoch [32][60]\t Batch [120][550]\t Training Loss 0.0222\t Accuracy 0.9851\n",
      "Epoch [32][60]\t Batch [180][550]\t Training Loss 0.0222\t Accuracy 0.9859\n",
      "Epoch [32][60]\t Batch [240][550]\t Training Loss 0.0221\t Accuracy 0.9863\n",
      "Epoch [32][60]\t Batch [300][550]\t Training Loss 0.0222\t Accuracy 0.9862\n",
      "Epoch [32][60]\t Batch [360][550]\t Training Loss 0.0222\t Accuracy 0.9861\n",
      "Epoch [32][60]\t Batch [420][550]\t Training Loss 0.0223\t Accuracy 0.9859\n",
      "Epoch [32][60]\t Batch [480][550]\t Training Loss 0.0224\t Accuracy 0.9858\n",
      "Epoch [32][60]\t Batch [540][550]\t Training Loss 0.0224\t Accuracy 0.9856\n",
      "\n",
      "Epoch [32]\t Average training loss 0.0224\t Average training accuracy 0.9857\n",
      "Epoch [32]\t Average validation loss 0.0245\t Average validation accuracy 0.9764\n",
      "\n",
      "Epoch [33][60]\t Batch [0][550]\t Training Loss 0.0213\t Accuracy 0.9800\n",
      "Epoch [33][60]\t Batch [60][550]\t Training Loss 0.0216\t Accuracy 0.9856\n",
      "Epoch [33][60]\t Batch [120][550]\t Training Loss 0.0219\t Accuracy 0.9855\n",
      "Epoch [33][60]\t Batch [180][550]\t Training Loss 0.0220\t Accuracy 0.9862\n",
      "Epoch [33][60]\t Batch [240][550]\t Training Loss 0.0218\t Accuracy 0.9866\n",
      "Epoch [33][60]\t Batch [300][550]\t Training Loss 0.0220\t Accuracy 0.9865\n",
      "Epoch [33][60]\t Batch [360][550]\t Training Loss 0.0220\t Accuracy 0.9865\n",
      "Epoch [33][60]\t Batch [420][550]\t Training Loss 0.0220\t Accuracy 0.9862\n",
      "Epoch [33][60]\t Batch [480][550]\t Training Loss 0.0221\t Accuracy 0.9861\n",
      "Epoch [33][60]\t Batch [540][550]\t Training Loss 0.0222\t Accuracy 0.9859\n",
      "\n",
      "Epoch [33]\t Average training loss 0.0221\t Average training accuracy 0.9860\n",
      "Epoch [33]\t Average validation loss 0.0244\t Average validation accuracy 0.9766\n",
      "\n",
      "Epoch [34][60]\t Batch [0][550]\t Training Loss 0.0212\t Accuracy 0.9800\n",
      "Epoch [34][60]\t Batch [60][550]\t Training Loss 0.0213\t Accuracy 0.9861\n",
      "Epoch [34][60]\t Batch [120][550]\t Training Loss 0.0217\t Accuracy 0.9859\n",
      "Epoch [34][60]\t Batch [180][550]\t Training Loss 0.0217\t Accuracy 0.9867\n",
      "Epoch [34][60]\t Batch [240][550]\t Training Loss 0.0216\t Accuracy 0.9870\n",
      "Epoch [34][60]\t Batch [300][550]\t Training Loss 0.0218\t Accuracy 0.9869\n",
      "Epoch [34][60]\t Batch [360][550]\t Training Loss 0.0218\t Accuracy 0.9868\n",
      "Epoch [34][60]\t Batch [420][550]\t Training Loss 0.0218\t Accuracy 0.9866\n",
      "Epoch [34][60]\t Batch [480][550]\t Training Loss 0.0219\t Accuracy 0.9864\n",
      "Epoch [34][60]\t Batch [540][550]\t Training Loss 0.0220\t Accuracy 0.9863\n",
      "\n",
      "Epoch [34]\t Average training loss 0.0219\t Average training accuracy 0.9864\n",
      "Epoch [34]\t Average validation loss 0.0244\t Average validation accuracy 0.9776\n",
      "\n",
      "Epoch [35][60]\t Batch [0][550]\t Training Loss 0.0212\t Accuracy 0.9800\n",
      "Epoch [35][60]\t Batch [60][550]\t Training Loss 0.0211\t Accuracy 0.9867\n",
      "Epoch [35][60]\t Batch [120][550]\t Training Loss 0.0215\t Accuracy 0.9864\n",
      "Epoch [35][60]\t Batch [180][550]\t Training Loss 0.0215\t Accuracy 0.9871\n",
      "Epoch [35][60]\t Batch [240][550]\t Training Loss 0.0214\t Accuracy 0.9874\n",
      "Epoch [35][60]\t Batch [300][550]\t Training Loss 0.0216\t Accuracy 0.9873\n",
      "Epoch [35][60]\t Batch [360][550]\t Training Loss 0.0216\t Accuracy 0.9871\n",
      "Epoch [35][60]\t Batch [420][550]\t Training Loss 0.0216\t Accuracy 0.9869\n",
      "Epoch [35][60]\t Batch [480][550]\t Training Loss 0.0217\t Accuracy 0.9867\n",
      "Epoch [35][60]\t Batch [540][550]\t Training Loss 0.0218\t Accuracy 0.9866\n",
      "\n",
      "Epoch [35]\t Average training loss 0.0217\t Average training accuracy 0.9867\n",
      "Epoch [35]\t Average validation loss 0.0243\t Average validation accuracy 0.9780\n",
      "\n",
      "Epoch [36][60]\t Batch [0][550]\t Training Loss 0.0211\t Accuracy 0.9800\n",
      "Epoch [36][60]\t Batch [60][550]\t Training Loss 0.0209\t Accuracy 0.9874\n",
      "Epoch [36][60]\t Batch [120][550]\t Training Loss 0.0213\t Accuracy 0.9870\n",
      "Epoch [36][60]\t Batch [180][550]\t Training Loss 0.0213\t Accuracy 0.9876\n",
      "Epoch [36][60]\t Batch [240][550]\t Training Loss 0.0212\t Accuracy 0.9879\n",
      "Epoch [36][60]\t Batch [300][550]\t Training Loss 0.0214\t Accuracy 0.9878\n",
      "Epoch [36][60]\t Batch [360][550]\t Training Loss 0.0214\t Accuracy 0.9876\n",
      "Epoch [36][60]\t Batch [420][550]\t Training Loss 0.0214\t Accuracy 0.9873\n",
      "Epoch [36][60]\t Batch [480][550]\t Training Loss 0.0215\t Accuracy 0.9872\n",
      "Epoch [36][60]\t Batch [540][550]\t Training Loss 0.0216\t Accuracy 0.9871\n",
      "\n",
      "Epoch [36]\t Average training loss 0.0215\t Average training accuracy 0.9872\n",
      "Epoch [36]\t Average validation loss 0.0242\t Average validation accuracy 0.9790\n",
      "\n",
      "Epoch [37][60]\t Batch [0][550]\t Training Loss 0.0210\t Accuracy 0.9800\n",
      "Epoch [37][60]\t Batch [60][550]\t Training Loss 0.0208\t Accuracy 0.9875\n",
      "Epoch [37][60]\t Batch [120][550]\t Training Loss 0.0211\t Accuracy 0.9872\n",
      "Epoch [37][60]\t Batch [180][550]\t Training Loss 0.0211\t Accuracy 0.9880\n",
      "Epoch [37][60]\t Batch [240][550]\t Training Loss 0.0210\t Accuracy 0.9883\n",
      "Epoch [37][60]\t Batch [300][550]\t Training Loss 0.0212\t Accuracy 0.9880\n",
      "Epoch [37][60]\t Batch [360][550]\t Training Loss 0.0212\t Accuracy 0.9878\n",
      "Epoch [37][60]\t Batch [420][550]\t Training Loss 0.0212\t Accuracy 0.9876\n",
      "Epoch [37][60]\t Batch [480][550]\t Training Loss 0.0213\t Accuracy 0.9875\n",
      "Epoch [37][60]\t Batch [540][550]\t Training Loss 0.0214\t Accuracy 0.9873\n",
      "\n",
      "Epoch [37]\t Average training loss 0.0213\t Average training accuracy 0.9874\n",
      "Epoch [37]\t Average validation loss 0.0243\t Average validation accuracy 0.9792\n",
      "\n",
      "Epoch [38][60]\t Batch [0][550]\t Training Loss 0.0210\t Accuracy 0.9800\n",
      "Epoch [38][60]\t Batch [60][550]\t Training Loss 0.0206\t Accuracy 0.9882\n",
      "Epoch [38][60]\t Batch [120][550]\t Training Loss 0.0209\t Accuracy 0.9877\n",
      "Epoch [38][60]\t Batch [180][550]\t Training Loss 0.0209\t Accuracy 0.9885\n",
      "Epoch [38][60]\t Batch [240][550]\t Training Loss 0.0208\t Accuracy 0.9887\n",
      "Epoch [38][60]\t Batch [300][550]\t Training Loss 0.0210\t Accuracy 0.9884\n",
      "Epoch [38][60]\t Batch [360][550]\t Training Loss 0.0210\t Accuracy 0.9882\n",
      "Epoch [38][60]\t Batch [420][550]\t Training Loss 0.0210\t Accuracy 0.9880\n",
      "Epoch [38][60]\t Batch [480][550]\t Training Loss 0.0211\t Accuracy 0.9878\n",
      "Epoch [38][60]\t Batch [540][550]\t Training Loss 0.0212\t Accuracy 0.9876\n",
      "\n",
      "Epoch [38]\t Average training loss 0.0211\t Average training accuracy 0.9877\n",
      "Epoch [38]\t Average validation loss 0.0242\t Average validation accuracy 0.9796\n",
      "\n",
      "Epoch [39][60]\t Batch [0][550]\t Training Loss 0.0209\t Accuracy 0.9800\n",
      "Epoch [39][60]\t Batch [60][550]\t Training Loss 0.0204\t Accuracy 0.9885\n",
      "Epoch [39][60]\t Batch [120][550]\t Training Loss 0.0207\t Accuracy 0.9881\n",
      "Epoch [39][60]\t Batch [180][550]\t Training Loss 0.0207\t Accuracy 0.9890\n",
      "Epoch [39][60]\t Batch [240][550]\t Training Loss 0.0206\t Accuracy 0.9891\n",
      "Epoch [39][60]\t Batch [300][550]\t Training Loss 0.0208\t Accuracy 0.9887\n",
      "Epoch [39][60]\t Batch [360][550]\t Training Loss 0.0208\t Accuracy 0.9884\n",
      "Epoch [39][60]\t Batch [420][550]\t Training Loss 0.0208\t Accuracy 0.9882\n",
      "Epoch [39][60]\t Batch [480][550]\t Training Loss 0.0209\t Accuracy 0.9881\n",
      "Epoch [39][60]\t Batch [540][550]\t Training Loss 0.0210\t Accuracy 0.9879\n",
      "\n",
      "Epoch [39]\t Average training loss 0.0209\t Average training accuracy 0.9880\n",
      "Epoch [39]\t Average validation loss 0.0241\t Average validation accuracy 0.9794\n",
      "\n",
      "Epoch [40][60]\t Batch [0][550]\t Training Loss 0.0208\t Accuracy 0.9800\n",
      "Epoch [40][60]\t Batch [60][550]\t Training Loss 0.0202\t Accuracy 0.9887\n",
      "Epoch [40][60]\t Batch [120][550]\t Training Loss 0.0205\t Accuracy 0.9883\n",
      "Epoch [40][60]\t Batch [180][550]\t Training Loss 0.0205\t Accuracy 0.9892\n",
      "Epoch [40][60]\t Batch [240][550]\t Training Loss 0.0204\t Accuracy 0.9893\n",
      "Epoch [40][60]\t Batch [300][550]\t Training Loss 0.0206\t Accuracy 0.9890\n",
      "Epoch [40][60]\t Batch [360][550]\t Training Loss 0.0206\t Accuracy 0.9888\n",
      "Epoch [40][60]\t Batch [420][550]\t Training Loss 0.0207\t Accuracy 0.9885\n",
      "Epoch [40][60]\t Batch [480][550]\t Training Loss 0.0208\t Accuracy 0.9885\n",
      "Epoch [40][60]\t Batch [540][550]\t Training Loss 0.0208\t Accuracy 0.9882\n",
      "\n",
      "Epoch [40]\t Average training loss 0.0208\t Average training accuracy 0.9883\n",
      "Epoch [40]\t Average validation loss 0.0240\t Average validation accuracy 0.9798\n",
      "\n",
      "Epoch [41][60]\t Batch [0][550]\t Training Loss 0.0208\t Accuracy 0.9800\n",
      "Epoch [41][60]\t Batch [60][550]\t Training Loss 0.0201\t Accuracy 0.9889\n",
      "Epoch [41][60]\t Batch [120][550]\t Training Loss 0.0203\t Accuracy 0.9885\n",
      "Epoch [41][60]\t Batch [180][550]\t Training Loss 0.0203\t Accuracy 0.9895\n",
      "Epoch [41][60]\t Batch [240][550]\t Training Loss 0.0203\t Accuracy 0.9896\n",
      "Epoch [41][60]\t Batch [300][550]\t Training Loss 0.0204\t Accuracy 0.9893\n",
      "Epoch [41][60]\t Batch [360][550]\t Training Loss 0.0204\t Accuracy 0.9892\n",
      "Epoch [41][60]\t Batch [420][550]\t Training Loss 0.0205\t Accuracy 0.9890\n",
      "Epoch [41][60]\t Batch [480][550]\t Training Loss 0.0206\t Accuracy 0.9888\n",
      "Epoch [41][60]\t Batch [540][550]\t Training Loss 0.0206\t Accuracy 0.9886\n",
      "\n",
      "Epoch [41]\t Average training loss 0.0206\t Average training accuracy 0.9886\n",
      "Epoch [41]\t Average validation loss 0.0240\t Average validation accuracy 0.9800\n",
      "\n",
      "Epoch [42][60]\t Batch [0][550]\t Training Loss 0.0208\t Accuracy 0.9800\n",
      "Epoch [42][60]\t Batch [60][550]\t Training Loss 0.0199\t Accuracy 0.9890\n",
      "Epoch [42][60]\t Batch [120][550]\t Training Loss 0.0202\t Accuracy 0.9888\n",
      "Epoch [42][60]\t Batch [180][550]\t Training Loss 0.0201\t Accuracy 0.9898\n",
      "Epoch [42][60]\t Batch [240][550]\t Training Loss 0.0201\t Accuracy 0.9898\n",
      "Epoch [42][60]\t Batch [300][550]\t Training Loss 0.0202\t Accuracy 0.9895\n",
      "Epoch [42][60]\t Batch [360][550]\t Training Loss 0.0202\t Accuracy 0.9894\n",
      "Epoch [42][60]\t Batch [420][550]\t Training Loss 0.0203\t Accuracy 0.9891\n",
      "Epoch [42][60]\t Batch [480][550]\t Training Loss 0.0204\t Accuracy 0.9890\n",
      "Epoch [42][60]\t Batch [540][550]\t Training Loss 0.0205\t Accuracy 0.9888\n",
      "\n",
      "Epoch [42]\t Average training loss 0.0204\t Average training accuracy 0.9889\n",
      "Epoch [42]\t Average validation loss 0.0239\t Average validation accuracy 0.9802\n",
      "\n",
      "Epoch [43][60]\t Batch [0][550]\t Training Loss 0.0207\t Accuracy 0.9800\n",
      "Epoch [43][60]\t Batch [60][550]\t Training Loss 0.0197\t Accuracy 0.9890\n",
      "Epoch [43][60]\t Batch [120][550]\t Training Loss 0.0200\t Accuracy 0.9888\n",
      "Epoch [43][60]\t Batch [180][550]\t Training Loss 0.0199\t Accuracy 0.9898\n",
      "Epoch [43][60]\t Batch [240][550]\t Training Loss 0.0199\t Accuracy 0.9898\n",
      "Epoch [43][60]\t Batch [300][550]\t Training Loss 0.0201\t Accuracy 0.9896\n",
      "Epoch [43][60]\t Batch [360][550]\t Training Loss 0.0201\t Accuracy 0.9895\n",
      "Epoch [43][60]\t Batch [420][550]\t Training Loss 0.0202\t Accuracy 0.9892\n",
      "Epoch [43][60]\t Batch [480][550]\t Training Loss 0.0202\t Accuracy 0.9891\n",
      "Epoch [43][60]\t Batch [540][550]\t Training Loss 0.0203\t Accuracy 0.9890\n",
      "\n",
      "Epoch [43]\t Average training loss 0.0203\t Average training accuracy 0.9891\n",
      "Epoch [43]\t Average validation loss 0.0239\t Average validation accuracy 0.9800\n",
      "\n",
      "Epoch [44][60]\t Batch [0][550]\t Training Loss 0.0207\t Accuracy 0.9800\n",
      "Epoch [44][60]\t Batch [60][550]\t Training Loss 0.0195\t Accuracy 0.9898\n",
      "Epoch [44][60]\t Batch [120][550]\t Training Loss 0.0198\t Accuracy 0.9893\n",
      "Epoch [44][60]\t Batch [180][550]\t Training Loss 0.0198\t Accuracy 0.9903\n",
      "Epoch [44][60]\t Batch [240][550]\t Training Loss 0.0198\t Accuracy 0.9902\n",
      "Epoch [44][60]\t Batch [300][550]\t Training Loss 0.0199\t Accuracy 0.9900\n",
      "Epoch [44][60]\t Batch [360][550]\t Training Loss 0.0199\t Accuracy 0.9899\n",
      "Epoch [44][60]\t Batch [420][550]\t Training Loss 0.0200\t Accuracy 0.9896\n",
      "Epoch [44][60]\t Batch [480][550]\t Training Loss 0.0201\t Accuracy 0.9894\n",
      "Epoch [44][60]\t Batch [540][550]\t Training Loss 0.0201\t Accuracy 0.9893\n",
      "\n",
      "Epoch [44]\t Average training loss 0.0201\t Average training accuracy 0.9894\n",
      "Epoch [44]\t Average validation loss 0.0238\t Average validation accuracy 0.9806\n",
      "\n",
      "Epoch [45][60]\t Batch [0][550]\t Training Loss 0.0206\t Accuracy 0.9800\n",
      "Epoch [45][60]\t Batch [60][550]\t Training Loss 0.0194\t Accuracy 0.9898\n",
      "Epoch [45][60]\t Batch [120][550]\t Training Loss 0.0196\t Accuracy 0.9894\n",
      "Epoch [45][60]\t Batch [180][550]\t Training Loss 0.0196\t Accuracy 0.9904\n",
      "Epoch [45][60]\t Batch [240][550]\t Training Loss 0.0196\t Accuracy 0.9904\n",
      "Epoch [45][60]\t Batch [300][550]\t Training Loss 0.0197\t Accuracy 0.9901\n",
      "Epoch [45][60]\t Batch [360][550]\t Training Loss 0.0197\t Accuracy 0.9900\n",
      "Epoch [45][60]\t Batch [420][550]\t Training Loss 0.0198\t Accuracy 0.9898\n",
      "Epoch [45][60]\t Batch [480][550]\t Training Loss 0.0199\t Accuracy 0.9897\n",
      "Epoch [45][60]\t Batch [540][550]\t Training Loss 0.0200\t Accuracy 0.9895\n",
      "\n",
      "Epoch [45]\t Average training loss 0.0199\t Average training accuracy 0.9895\n",
      "Epoch [45]\t Average validation loss 0.0236\t Average validation accuracy 0.9808\n",
      "\n",
      "Epoch [46][60]\t Batch [0][550]\t Training Loss 0.0203\t Accuracy 0.9800\n",
      "Epoch [46][60]\t Batch [60][550]\t Training Loss 0.0192\t Accuracy 0.9905\n",
      "Epoch [46][60]\t Batch [120][550]\t Training Loss 0.0195\t Accuracy 0.9898\n",
      "Epoch [46][60]\t Batch [180][550]\t Training Loss 0.0194\t Accuracy 0.9908\n",
      "Epoch [46][60]\t Batch [240][550]\t Training Loss 0.0195\t Accuracy 0.9907\n",
      "Epoch [46][60]\t Batch [300][550]\t Training Loss 0.0196\t Accuracy 0.9904\n",
      "Epoch [46][60]\t Batch [360][550]\t Training Loss 0.0196\t Accuracy 0.9903\n",
      "Epoch [46][60]\t Batch [420][550]\t Training Loss 0.0197\t Accuracy 0.9900\n",
      "Epoch [46][60]\t Batch [480][550]\t Training Loss 0.0197\t Accuracy 0.9900\n",
      "Epoch [46][60]\t Batch [540][550]\t Training Loss 0.0198\t Accuracy 0.9899\n",
      "\n",
      "Epoch [46]\t Average training loss 0.0198\t Average training accuracy 0.9899\n",
      "Epoch [46]\t Average validation loss 0.0235\t Average validation accuracy 0.9808\n",
      "\n",
      "Epoch [47][60]\t Batch [0][550]\t Training Loss 0.0201\t Accuracy 0.9800\n",
      "Epoch [47][60]\t Batch [60][550]\t Training Loss 0.0190\t Accuracy 0.9907\n",
      "Epoch [47][60]\t Batch [120][550]\t Training Loss 0.0193\t Accuracy 0.9900\n",
      "Epoch [47][60]\t Batch [180][550]\t Training Loss 0.0193\t Accuracy 0.9910\n",
      "Epoch [47][60]\t Batch [240][550]\t Training Loss 0.0193\t Accuracy 0.9908\n",
      "Epoch [47][60]\t Batch [300][550]\t Training Loss 0.0194\t Accuracy 0.9905\n",
      "Epoch [47][60]\t Batch [360][550]\t Training Loss 0.0194\t Accuracy 0.9905\n",
      "Epoch [47][60]\t Batch [420][550]\t Training Loss 0.0195\t Accuracy 0.9902\n",
      "Epoch [47][60]\t Batch [480][550]\t Training Loss 0.0196\t Accuracy 0.9901\n",
      "Epoch [47][60]\t Batch [540][550]\t Training Loss 0.0197\t Accuracy 0.9900\n",
      "\n",
      "Epoch [47]\t Average training loss 0.0196\t Average training accuracy 0.9901\n",
      "Epoch [47]\t Average validation loss 0.0234\t Average validation accuracy 0.9808\n",
      "\n",
      "Epoch [48][60]\t Batch [0][550]\t Training Loss 0.0201\t Accuracy 0.9800\n",
      "Epoch [48][60]\t Batch [60][550]\t Training Loss 0.0189\t Accuracy 0.9908\n",
      "Epoch [48][60]\t Batch [120][550]\t Training Loss 0.0191\t Accuracy 0.9904\n",
      "Epoch [48][60]\t Batch [180][550]\t Training Loss 0.0191\t Accuracy 0.9914\n",
      "Epoch [48][60]\t Batch [240][550]\t Training Loss 0.0191\t Accuracy 0.9914\n",
      "Epoch [48][60]\t Batch [300][550]\t Training Loss 0.0193\t Accuracy 0.9910\n",
      "Epoch [48][60]\t Batch [360][550]\t Training Loss 0.0193\t Accuracy 0.9909\n",
      "Epoch [48][60]\t Batch [420][550]\t Training Loss 0.0194\t Accuracy 0.9906\n",
      "Epoch [48][60]\t Batch [480][550]\t Training Loss 0.0194\t Accuracy 0.9905\n",
      "Epoch [48][60]\t Batch [540][550]\t Training Loss 0.0195\t Accuracy 0.9904\n",
      "\n",
      "Epoch [48]\t Average training loss 0.0195\t Average training accuracy 0.9905\n",
      "Epoch [48]\t Average validation loss 0.0233\t Average validation accuracy 0.9808\n",
      "\n",
      "Epoch [49][60]\t Batch [0][550]\t Training Loss 0.0199\t Accuracy 0.9800\n",
      "Epoch [49][60]\t Batch [60][550]\t Training Loss 0.0188\t Accuracy 0.9911\n",
      "Epoch [49][60]\t Batch [120][550]\t Training Loss 0.0190\t Accuracy 0.9906\n",
      "Epoch [49][60]\t Batch [180][550]\t Training Loss 0.0189\t Accuracy 0.9915\n",
      "Epoch [49][60]\t Batch [240][550]\t Training Loss 0.0190\t Accuracy 0.9915\n",
      "Epoch [49][60]\t Batch [300][550]\t Training Loss 0.0191\t Accuracy 0.9913\n",
      "Epoch [49][60]\t Batch [360][550]\t Training Loss 0.0191\t Accuracy 0.9911\n",
      "Epoch [49][60]\t Batch [420][550]\t Training Loss 0.0192\t Accuracy 0.9909\n",
      "Epoch [49][60]\t Batch [480][550]\t Training Loss 0.0193\t Accuracy 0.9908\n",
      "Epoch [49][60]\t Batch [540][550]\t Training Loss 0.0194\t Accuracy 0.9906\n",
      "\n",
      "Epoch [49]\t Average training loss 0.0193\t Average training accuracy 0.9907\n",
      "Epoch [49]\t Average validation loss 0.0232\t Average validation accuracy 0.9810\n",
      "\n",
      "Epoch [50][60]\t Batch [0][550]\t Training Loss 0.0199\t Accuracy 0.9800\n",
      "Epoch [50][60]\t Batch [60][550]\t Training Loss 0.0187\t Accuracy 0.9910\n",
      "Epoch [50][60]\t Batch [120][550]\t Training Loss 0.0189\t Accuracy 0.9907\n",
      "Epoch [50][60]\t Batch [180][550]\t Training Loss 0.0188\t Accuracy 0.9918\n",
      "Epoch [50][60]\t Batch [240][550]\t Training Loss 0.0188\t Accuracy 0.9917\n",
      "Epoch [50][60]\t Batch [300][550]\t Training Loss 0.0190\t Accuracy 0.9915\n",
      "Epoch [50][60]\t Batch [360][550]\t Training Loss 0.0190\t Accuracy 0.9913\n",
      "Epoch [50][60]\t Batch [420][550]\t Training Loss 0.0191\t Accuracy 0.9911\n",
      "Epoch [50][60]\t Batch [480][550]\t Training Loss 0.0192\t Accuracy 0.9910\n",
      "Epoch [50][60]\t Batch [540][550]\t Training Loss 0.0192\t Accuracy 0.9908\n",
      "\n",
      "Epoch [50]\t Average training loss 0.0192\t Average training accuracy 0.9909\n",
      "Epoch [50]\t Average validation loss 0.0232\t Average validation accuracy 0.9806\n",
      "\n",
      "Epoch [51][60]\t Batch [0][550]\t Training Loss 0.0198\t Accuracy 0.9800\n",
      "Epoch [51][60]\t Batch [60][550]\t Training Loss 0.0186\t Accuracy 0.9916\n",
      "Epoch [51][60]\t Batch [120][550]\t Training Loss 0.0187\t Accuracy 0.9912\n",
      "Epoch [51][60]\t Batch [180][550]\t Training Loss 0.0186\t Accuracy 0.9922\n",
      "Epoch [51][60]\t Batch [240][550]\t Training Loss 0.0187\t Accuracy 0.9920\n",
      "Epoch [51][60]\t Batch [300][550]\t Training Loss 0.0188\t Accuracy 0.9918\n",
      "Epoch [51][60]\t Batch [360][550]\t Training Loss 0.0188\t Accuracy 0.9916\n",
      "Epoch [51][60]\t Batch [420][550]\t Training Loss 0.0189\t Accuracy 0.9913\n",
      "Epoch [51][60]\t Batch [480][550]\t Training Loss 0.0190\t Accuracy 0.9912\n",
      "Epoch [51][60]\t Batch [540][550]\t Training Loss 0.0191\t Accuracy 0.9910\n",
      "\n",
      "Epoch [51]\t Average training loss 0.0190\t Average training accuracy 0.9911\n",
      "Epoch [51]\t Average validation loss 0.0232\t Average validation accuracy 0.9808\n",
      "\n",
      "Epoch [52][60]\t Batch [0][550]\t Training Loss 0.0199\t Accuracy 0.9800\n",
      "Epoch [52][60]\t Batch [60][550]\t Training Loss 0.0185\t Accuracy 0.9918\n",
      "Epoch [52][60]\t Batch [120][550]\t Training Loss 0.0186\t Accuracy 0.9914\n",
      "Epoch [52][60]\t Batch [180][550]\t Training Loss 0.0185\t Accuracy 0.9923\n",
      "Epoch [52][60]\t Batch [240][550]\t Training Loss 0.0186\t Accuracy 0.9922\n",
      "Epoch [52][60]\t Batch [300][550]\t Training Loss 0.0187\t Accuracy 0.9919\n",
      "Epoch [52][60]\t Batch [360][550]\t Training Loss 0.0187\t Accuracy 0.9917\n",
      "Epoch [52][60]\t Batch [420][550]\t Training Loss 0.0188\t Accuracy 0.9914\n",
      "Epoch [52][60]\t Batch [480][550]\t Training Loss 0.0189\t Accuracy 0.9913\n",
      "Epoch [52][60]\t Batch [540][550]\t Training Loss 0.0189\t Accuracy 0.9912\n",
      "\n",
      "Epoch [52]\t Average training loss 0.0189\t Average training accuracy 0.9913\n",
      "Epoch [52]\t Average validation loss 0.0232\t Average validation accuracy 0.9808\n",
      "\n",
      "Epoch [53][60]\t Batch [0][550]\t Training Loss 0.0198\t Accuracy 0.9800\n",
      "Epoch [53][60]\t Batch [60][550]\t Training Loss 0.0184\t Accuracy 0.9920\n",
      "Epoch [53][60]\t Batch [120][550]\t Training Loss 0.0185\t Accuracy 0.9918\n",
      "Epoch [53][60]\t Batch [180][550]\t Training Loss 0.0184\t Accuracy 0.9927\n",
      "Epoch [53][60]\t Batch [240][550]\t Training Loss 0.0184\t Accuracy 0.9924\n",
      "Epoch [53][60]\t Batch [300][550]\t Training Loss 0.0185\t Accuracy 0.9921\n",
      "Epoch [53][60]\t Batch [360][550]\t Training Loss 0.0186\t Accuracy 0.9919\n",
      "Epoch [53][60]\t Batch [420][550]\t Training Loss 0.0187\t Accuracy 0.9916\n",
      "Epoch [53][60]\t Batch [480][550]\t Training Loss 0.0187\t Accuracy 0.9914\n",
      "Epoch [53][60]\t Batch [540][550]\t Training Loss 0.0188\t Accuracy 0.9913\n",
      "\n",
      "Epoch [53]\t Average training loss 0.0188\t Average training accuracy 0.9914\n",
      "Epoch [53]\t Average validation loss 0.0231\t Average validation accuracy 0.9810\n",
      "\n",
      "Epoch [54][60]\t Batch [0][550]\t Training Loss 0.0196\t Accuracy 0.9800\n",
      "Epoch [54][60]\t Batch [60][550]\t Training Loss 0.0182\t Accuracy 0.9918\n",
      "Epoch [54][60]\t Batch [120][550]\t Training Loss 0.0183\t Accuracy 0.9918\n",
      "Epoch [54][60]\t Batch [180][550]\t Training Loss 0.0182\t Accuracy 0.9927\n",
      "Epoch [54][60]\t Batch [240][550]\t Training Loss 0.0183\t Accuracy 0.9925\n",
      "Epoch [54][60]\t Batch [300][550]\t Training Loss 0.0184\t Accuracy 0.9923\n",
      "Epoch [54][60]\t Batch [360][550]\t Training Loss 0.0185\t Accuracy 0.9922\n",
      "Epoch [54][60]\t Batch [420][550]\t Training Loss 0.0185\t Accuracy 0.9919\n",
      "Epoch [54][60]\t Batch [480][550]\t Training Loss 0.0186\t Accuracy 0.9916\n",
      "Epoch [54][60]\t Batch [540][550]\t Training Loss 0.0187\t Accuracy 0.9916\n",
      "\n",
      "Epoch [54]\t Average training loss 0.0186\t Average training accuracy 0.9916\n",
      "Epoch [54]\t Average validation loss 0.0230\t Average validation accuracy 0.9810\n",
      "\n",
      "Epoch [55][60]\t Batch [0][550]\t Training Loss 0.0196\t Accuracy 0.9800\n",
      "Epoch [55][60]\t Batch [60][550]\t Training Loss 0.0181\t Accuracy 0.9921\n",
      "Epoch [55][60]\t Batch [120][550]\t Training Loss 0.0182\t Accuracy 0.9919\n",
      "Epoch [55][60]\t Batch [180][550]\t Training Loss 0.0181\t Accuracy 0.9928\n",
      "Epoch [55][60]\t Batch [240][550]\t Training Loss 0.0182\t Accuracy 0.9926\n",
      "Epoch [55][60]\t Batch [300][550]\t Training Loss 0.0183\t Accuracy 0.9923\n",
      "Epoch [55][60]\t Batch [360][550]\t Training Loss 0.0183\t Accuracy 0.9922\n",
      "Epoch [55][60]\t Batch [420][550]\t Training Loss 0.0184\t Accuracy 0.9919\n",
      "Epoch [55][60]\t Batch [480][550]\t Training Loss 0.0185\t Accuracy 0.9917\n",
      "Epoch [55][60]\t Batch [540][550]\t Training Loss 0.0185\t Accuracy 0.9916\n",
      "\n",
      "Epoch [55]\t Average training loss 0.0185\t Average training accuracy 0.9917\n",
      "Epoch [55]\t Average validation loss 0.0230\t Average validation accuracy 0.9812\n",
      "\n",
      "Epoch [56][60]\t Batch [0][550]\t Training Loss 0.0195\t Accuracy 0.9800\n",
      "Epoch [56][60]\t Batch [60][550]\t Training Loss 0.0180\t Accuracy 0.9921\n",
      "Epoch [56][60]\t Batch [120][550]\t Training Loss 0.0181\t Accuracy 0.9919\n",
      "Epoch [56][60]\t Batch [180][550]\t Training Loss 0.0180\t Accuracy 0.9928\n",
      "Epoch [56][60]\t Batch [240][550]\t Training Loss 0.0181\t Accuracy 0.9926\n",
      "Epoch [56][60]\t Batch [300][550]\t Training Loss 0.0182\t Accuracy 0.9924\n",
      "Epoch [56][60]\t Batch [360][550]\t Training Loss 0.0182\t Accuracy 0.9922\n",
      "Epoch [56][60]\t Batch [420][550]\t Training Loss 0.0183\t Accuracy 0.9920\n",
      "Epoch [56][60]\t Batch [480][550]\t Training Loss 0.0184\t Accuracy 0.9917\n",
      "Epoch [56][60]\t Batch [540][550]\t Training Loss 0.0184\t Accuracy 0.9917\n",
      "\n",
      "Epoch [56]\t Average training loss 0.0184\t Average training accuracy 0.9918\n",
      "Epoch [56]\t Average validation loss 0.0231\t Average validation accuracy 0.9814\n",
      "\n",
      "Epoch [57][60]\t Batch [0][550]\t Training Loss 0.0196\t Accuracy 0.9800\n",
      "Epoch [57][60]\t Batch [60][550]\t Training Loss 0.0178\t Accuracy 0.9921\n",
      "Epoch [57][60]\t Batch [120][550]\t Training Loss 0.0179\t Accuracy 0.9921\n",
      "Epoch [57][60]\t Batch [180][550]\t Training Loss 0.0179\t Accuracy 0.9929\n",
      "Epoch [57][60]\t Batch [240][550]\t Training Loss 0.0179\t Accuracy 0.9927\n",
      "Epoch [57][60]\t Batch [300][550]\t Training Loss 0.0180\t Accuracy 0.9926\n",
      "Epoch [57][60]\t Batch [360][550]\t Training Loss 0.0181\t Accuracy 0.9924\n",
      "Epoch [57][60]\t Batch [420][550]\t Training Loss 0.0182\t Accuracy 0.9922\n",
      "Epoch [57][60]\t Batch [480][550]\t Training Loss 0.0182\t Accuracy 0.9919\n",
      "Epoch [57][60]\t Batch [540][550]\t Training Loss 0.0183\t Accuracy 0.9918\n",
      "\n",
      "Epoch [57]\t Average training loss 0.0182\t Average training accuracy 0.9919\n",
      "Epoch [57]\t Average validation loss 0.0231\t Average validation accuracy 0.9818\n",
      "\n",
      "Epoch [58][60]\t Batch [0][550]\t Training Loss 0.0195\t Accuracy 0.9800\n",
      "Epoch [58][60]\t Batch [60][550]\t Training Loss 0.0177\t Accuracy 0.9925\n",
      "Epoch [58][60]\t Batch [120][550]\t Training Loss 0.0180\t Accuracy 0.9921\n",
      "Epoch [58][60]\t Batch [180][550]\t Training Loss 0.0180\t Accuracy 0.9929\n",
      "Epoch [58][60]\t Batch [240][550]\t Training Loss 0.0179\t Accuracy 0.9929\n",
      "Epoch [58][60]\t Batch [300][550]\t Training Loss 0.0180\t Accuracy 0.9927\n",
      "Epoch [58][60]\t Batch [360][550]\t Training Loss 0.0181\t Accuracy 0.9926\n",
      "Epoch [58][60]\t Batch [420][550]\t Training Loss 0.0181\t Accuracy 0.9923\n",
      "Epoch [58][60]\t Batch [480][550]\t Training Loss 0.0182\t Accuracy 0.9921\n",
      "Epoch [58][60]\t Batch [540][550]\t Training Loss 0.0182\t Accuracy 0.9920\n",
      "\n",
      "Epoch [58]\t Average training loss 0.0182\t Average training accuracy 0.9921\n",
      "Epoch [58]\t Average validation loss 0.0229\t Average validation accuracy 0.9816\n",
      "\n",
      "Epoch [59][60]\t Batch [0][550]\t Training Loss 0.0194\t Accuracy 0.9800\n",
      "Epoch [59][60]\t Batch [60][550]\t Training Loss 0.0177\t Accuracy 0.9926\n",
      "Epoch [59][60]\t Batch [120][550]\t Training Loss 0.0178\t Accuracy 0.9925\n",
      "Epoch [59][60]\t Batch [180][550]\t Training Loss 0.0178\t Accuracy 0.9931\n",
      "Epoch [59][60]\t Batch [240][550]\t Training Loss 0.0178\t Accuracy 0.9930\n",
      "Epoch [59][60]\t Batch [300][550]\t Training Loss 0.0179\t Accuracy 0.9929\n",
      "Epoch [59][60]\t Batch [360][550]\t Training Loss 0.0179\t Accuracy 0.9927\n",
      "Epoch [59][60]\t Batch [420][550]\t Training Loss 0.0180\t Accuracy 0.9924\n",
      "Epoch [59][60]\t Batch [480][550]\t Training Loss 0.0181\t Accuracy 0.9922\n",
      "Epoch [59][60]\t Batch [540][550]\t Training Loss 0.0181\t Accuracy 0.9921\n",
      "\n",
      "Epoch [59]\t Average training loss 0.0180\t Average training accuracy 0.9922\n",
      "Epoch [59]\t Average validation loss 0.0229\t Average validation accuracy 0.9822\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from criterion import SoftmaxCrossEntropyLossLayer, EuclideanLossLayer\n",
    "from layers import FCLayer, SigmoidLayer, ReLULayer\n",
    "from optimizer import SGD\n",
    "\n",
    "# criterion = SoftmaxCrossEntropyLossLayer()\n",
    "criterion = EuclideanLossLayer()\n",
    "\n",
    "\n",
    "sgd = SGD(learning_rate_SGD, weight_decay)\n",
    "\n",
    "reluMLP = Network()\n",
    "# Build ReLUMLP with FCLayer and ReLULayer\n",
    "# 128 is the number of hidden units, you can change by your own\n",
    "reluMLP.add(FCLayer(784, 128))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(128, 64))\n",
    "reluMLP.add(ReLULayer())\n",
    "reluMLP.add(FCLayer(64, 10))\n",
    "\n",
    "reluMLP, relu_loss, relu_acc = train(reluMLP, criterion, sgd, data_train, max_epoch, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "The test accuracy is 0.9758.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test(reluMLP, criterion, data_test, batch_size, disp_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_and_acc({'relu': [relu_loss, relu_acc]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
